<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>JackZhang's Blog | JackZhang's Blog</title><meta name="author" content="JackZhang"><meta name="copyright" content="JackZhang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="贝尔曼方程在一个马尔可夫奖励过程中，从第t时刻状态开始，直到终止状态T时，所有奖励的衰减之和称为回报（Return），公式如下：$$G_{t}&#x3D;r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\gamma^{3} r_{t+4}+\ldots+\gamma^{T-t-1} r_{T}$$其中，$T$是最终时刻，$\gamma$ 是折扣因子，越往后得到的奖励，折扣"><meta property="og:type" content="article"><meta property="og:title" content="JackZhang&#39;s Blog"><meta property="og:url" content="https://www.jkzhang.ml/2025/02/22/#%20%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/index.html"><meta property="og:site_name" content="JackZhang&#39;s Blog"><meta property="og:description" content="贝尔曼方程在一个马尔可夫奖励过程中，从第t时刻状态开始，直到终止状态T时，所有奖励的衰减之和称为回报（Return），公式如下：$$G_{t}&#x3D;r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\gamma^{3} r_{t+4}+\ldots+\gamma^{T-t-1} r_{T}$$其中，$T$是最终时刻，$\gamma$ 是折扣因子，越往后得到的奖励，折扣"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2025-02-22T08:34:25.475Z"><meta property="article:modified_time" content="2025-01-15T13:36:32.350Z"><meta property="article:author" content="JackZhang"><meta property="article:tag" content="�?"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.jkzhang.ml/2025/02/22/#%20%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,noticeOutdate:void 0,highlight:void 0,copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"JackZhang's Blog",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2025-01-15 21:36:32"}</script><script>(e=>{e.saveToLocal={set:(e,t,a)=>{0!==a&&(a={value:t,expiry:Date.now()+864e5*a},localStorage.setItem(e,JSON.stringify(a)))},get:e=>{var t=localStorage.getItem(e);if(t){t=JSON.parse(t);if(!(Date.now()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=(o,n={})=>new Promise((t,e)=>{const a=document.createElement("script");a.src=o,a.async=!0,a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},Object.keys(n).forEach(e=>{a.setAttribute(e,n[e])}),document.head.appendChild(a)}),e.getCSS=(o,n=!1)=>new Promise((t,e)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};e=saveToLocal.get("theme");"dark"===e?activateDarkMode():"light"===e&&activateLightMode();e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="JackZhang's Blog" type="application/atom+xml"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s3.bmp.ovh/imgs/2024/06/07/50ea7f3f41ccf524.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">63</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)"><nav id="nav"><span id="blog-info"><a href="/" title="JackZhang's Blog"><img class="site-icon" src="/img/favicon.ico"><span class="site-name">JackZhang's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">无题</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-22T08:34:25.475Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-15T13:36:32.350Z" title="更新于 2025-01-15 21:36:32">2025-01-15</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h1><p>在一个马尔可夫奖励过程中，从第t时刻状态开始，直到终止状态T时，所有奖励的衰减之和称为<strong>回报</strong>（Return），公式如下：<br>$$<br>G_{t}=r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\gamma^{3} r_{t+4}+\ldots+\gamma^{T-t-1} r_{T}<br>$$<br>其中，$T$是最终时刻，$\gamma$ 是折扣因子，越往后得到的奖励，折扣越多。关于折扣因子的必要性，以前的笔记已经介绍过。</p><p>如果提取一个$\gamma$，还可以得到一个递推式：<br>$$<br>G_{t}=r_{t+1}+\gamma G_{t+1}<br>$$<br>这个意思是，此时的计算只需要关注这一步的奖励，加折扣因子乘以下一步的回报即可。这个式子推导贝尔曼方程时极其有用。</p><p>接下来定义价值函数：<br>$$<br>\begin{aligned}<br>V_{\pi}(s) &amp;=\mathbb{E}\left[G_{t} \mid s_{t}=s\right] \<br>&amp;=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\ldots+\gamma^{T-t-1} r_{T} \mid s_{t}=s\right]<br>\end{aligned}<br>$$<br>其中，$G_t$ 是之前定义的<strong>折扣回报（discounted return）</strong>。我们对$G_t$取了一个期望，期望就是从这个状态开始，我们可能获得多大的价值。所以期望也可以看成未来可能获得奖励的当前价值的表现，就是当我们进入某一个状态后，我们现在有多大的价值。</p><p>可以看到价值函数有一个下标$\pi$，这是因为价值函数除了与s有关，也与策略$\pi$有关，不同的策略价值函数自然也不一样。并且价值函数越大，说明这个策略越好。因此价值函数可以评估策略的优劣。</p><p>我们利用价值函数表示出<strong>贝尔曼方程（Bellman equation）</strong><br>$$<br>V(s)=\underbrace{R(s)}<em>{\text {即时奖励}}+\underbrace{\gamma \sum</em>{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)}_{\text {未来奖励的折扣总和}}<br>$$<br>其中，</p><ul><li>$s’$ 可以看成未来的所有状态，</li><li>$p(s’|s)$ 是指从当前状态转移到未来状态的概率。</li><li>$V(s’)$ 代表的是未来某一个状态的价值。我们从当前状态开始，有一定的概率去到未来的所有状态，所以我们要把 $p\left(s^{\prime} \mid s\right)$ 写上去。我们得到了未来状态后，乘一个 $\gamma$，这样就可以把未来的奖励打折扣。</li><li>$\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)$ 可以看成未来奖励的折扣总和（discounted sum of future reward）。</li></ul><p>贝尔曼方程的推导过程如下：</p><p>由于$V_{\pi}(s) =\mathbb{E}\left[G_{t} \mid s_{t}=s\right]$，再把$G_{t}=r_{t+1}+\gamma G_{t+1}$代入，其中<br>$$<br>\begin{aligned}\mathbb{E}[G_{t+1}|S_{t}=s]&amp;\begin{aligned} =\sum_{s^{\prime}\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_t=s,S_{t+1}=s^{\prime}]p(s^{\prime}|s)\end{aligned}\&amp;\begin{aligned}&amp;=\sum_{s^{\prime}\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_{t+1}=s^{\prime}]p(s^{\prime}|s)&amp;&amp;\text{(due to the Markov property)}\end{aligned}\&amp;\begin{aligned}=\sum_{s^{\prime}\in\mathcal{S}}v_\pi(s^{\prime})p(s^{\prime}|s)\end{aligned}\&amp;\begin{aligned}&amp;=\sum_{s^{\prime}\in\mathcal{S}}v_{\pi}(s^{\prime})\sum_{a\in\mathcal{A}}p(s^{\prime}|s,a)\pi(a|s).\end{aligned}\end{aligned}<br>$$</p><p>接下来我们可以求期望了：</p><p>$$<br>\begin{aligned}<br>V(s)&amp;=\mathbb{E}\left[G_{t} \mid s_{t}=s\right]\<br>&amp;=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\ldots \mid s_{t}=s\right] \</p><pre><code>&amp;=R(s)+\gamma \mathbb&#123;E&#125;[G_&#123;t+1&#125;|s_t=s] \\

&amp;=R(s)+\gamma \sum_&#123;s^&#123;\prime&#125; \in S&#125; p\left(s^&#123;\prime&#125; \mid s\right) V\left(s^&#123;\prime&#125;\right)
\end&#123;aligned&#125;
</code></pre><p>$$<br>再进一步探究：<br>$$<br>\begin{aligned}v_{\pi}(s)&amp;=\mathbb{E}[R_{t+1}|S_{t}=s]+\gamma\mathbb{E}[G_{t+1}|S_{t}=s],\&amp;=\underbrace{\sum_a\pi(a|s)\sum_rp(r|s,a)r}<em>{\text{mean of immediate rewards}}+\underbrace{\gamma\sum_a\pi(a|s)\sum_{s^{\prime}}p(s^{\prime}|s,a)v_\pi(s^{\prime})}_{\text{mean of future rewards}},\&amp;=\sum</em>{a}\underbrace{\pi(a|s)}\left[\sum_{r}\underbrace{p(r|s,a)r}<em>{}+\gamma\sum</em>{s^{\prime}}\underbrace{p(s^{\prime}|s,a)}<em>{}\underbrace{v</em>{\pi}(s^{\prime})}_{}\right],\quad\forall s\in\mathcal{S}.\end{aligned}<br>$$<br>得到了关于$\pi$的表达式，这就说明了策略$\pi$与状态函数的关系，这为policy evaluation提供了工具！</p><p>我们可以把贝尔曼方程写成矩阵的形式：<br>$$<br>\left(\begin{array}{c}<br>V\left(s_{1}\right) \<br>V\left(s_{2}\right) \<br>\vdots \<br>V\left(s_{N}\right)<br>\end{array}\right)=\left(\begin{array}{c}<br>R\left(s_{1}\right) \<br>R\left(s_{2}\right) \<br>\vdots \<br>R\left(s_{N}\right)<br>\end{array}\right)+\gamma\left(\begin{array}{cccc}<br>p\left(s_{1} \mid s_{1}\right) &amp; p\left(s_{2} \mid s_{1}\right) &amp; \ldots &amp; p\left(s_{N} \mid s_{1}\right) \<br>p\left(s_{1} \mid s_{2}\right) &amp; p\left(s_{2} \mid s_{2}\right) &amp; \ldots &amp; p\left(s_{N} \mid s_{2}\right) \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>p\left(s_{1} \mid s_{N}\right) &amp; p\left(s_{2} \mid s_{N}\right) &amp; \ldots &amp; p\left(s_{N} \mid s_{N}\right)<br>\end{array}\right)\left(\begin{array}{c}<br>V\left(s_{1}\right) \<br>V\left(s_{2}\right) \<br>\vdots \<br>V\left(s_{N}\right)<br>\end{array}\right)<br>$$</p><p>我们当前的状态是向量$[V(s_1),V(s_2),\cdots,V(s_N)]^\mathrm{T}$。每一行来看，向量$\boldsymbol{V}$乘状态转移矩阵里面的某一行，再加上它当前可以得到的奖励，就会得到它当前的价值。</p><p>当我们把贝尔曼方程写成矩阵形式后，可以直接求解：<br>$$<br>\begin{aligned}<br>\boldsymbol{V} &amp;= \boldsymbol{\boldsymbol{R}}+ \gamma \boldsymbol{P}\boldsymbol{V} \<br>\boldsymbol{I}\boldsymbol{V} &amp;= \boldsymbol{R}+ \gamma \boldsymbol{P}\boldsymbol{V} \<br>(\boldsymbol{I}-\gamma \boldsymbol{P})\boldsymbol{V}&amp;=\boldsymbol{R} \<br>\boldsymbol{V}&amp;=(\boldsymbol{I}-\gamma \boldsymbol{P})^{-1}\boldsymbol{R}<br>\end{aligned}<br>$$</p><p>我们可以直接得到<strong>解析解（analytic solution）</strong>：<br>$$<br>\boldsymbol{V}=(\boldsymbol{I}-\gamma \boldsymbol{P})^{-1} \boldsymbol{R}<br>$$</p><p>我们可以通过矩阵求逆把 $\boldsymbol{V}$ 的价值直接求出来。但是一个问题是这个矩阵求逆的过程的复杂度是 $O(N^3)$。所以当状态非常多的时候，比如从10个状态到1000个状态，或者到100万个状态，当我们有100万个状态的时候，状态转移矩阵就会是一个100万乘100万的矩阵，对这样一个大矩阵求逆是非常困难的。所以这种通过解析解去求解的方法只适用于很小量的马尔可夫奖励过程。</p><p><img src="C:\Users\24376\AppData\Roaming\Typora\typora-user-images\image-20250115181203944.png" alt="image-20250115181203944"></p><p>这个解法利用了<strong>不动点思想：</strong>假设代数方程</p><p><img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/ad27e68aee3a027d0a5bace0c55dd79250239c47" alt="{\displaystyle f(x)=0}"></p><p>可以等价写成</p><p><img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/c1944e08d22cd6b68dcb809cdc1804d0df3b5458" alt="{\displaystyle x=\varphi (x).}"></p><p>我们称满足<img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/b598d12079d102bda91773a160b7c99540c36d9c" alt="{\displaystyle x^{*}=\varphi (x^{*})}">的<img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/0c3d92911742787872be53f047783d16d9737ea0" alt="{\displaystyle x^{*}}">为<img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/87792d8065e427aa737caaf609bbdf9da8cd6a3e" alt="{\displaystyle \varphi (x)}">的<strong>不动点</strong>，设计如下迭代格式</p><p><img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/1300b775de9924f6d1d0cdda71d12b8d29ec7541" alt="{\displaystyle x_{k+1}=\varphi (x_{k}),\quad k=0,1,\cdots .}"></p><p>如果该格式收敛，那么它可用于求解<img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/ad27e68aee3a027d0a5bace0c55dd79250239c47" alt="{\displaystyle f(x)=0}">的问题，这种方法就是不动点迭代法。 至于这个迭代格式是否收敛，则需要利用不动点定理作进一步分析。</p><p><img src="C:\Users\24376\AppData\Roaming\Typora\typora-user-images\image-20250115181634889.png" alt="image-20250115181634889"></p><p>对于MDP，我们在第一节里已经讲到了它的价值函数$v_\pi(s)$的表达式。但是这个表达式没有考虑到所采用的动作<em>a</em>带来的价值影响，因此我们除了这个$v_\pi(s)$状态价值函数外，还有一个动作价值函数$q_\pi(s,a)$，即：<br>$$<br>q_{\pi}(s, a)=\mathbb{E}<em>{\pi}\left[G</em>{t} \mid s_{t}=s, a_{t}=a\right] \tag{2.4}<br>$$<br>使用$q_\pi(s)$可以表示状态价值函数，即：<br>$$<br>v_{\pi}(s)=\sum_{a \in A} \pi(a \mid s)q_{\pi}(s, a)<br>\tag{2.5}<br>$$<br>另外，我们给出 Q 函数的贝尔曼方程</p><p>$$<br>Q(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V\left(s^{\prime}\right) \tag{2.21}<br>$$<br>根据和的关系，我们可以得到贝尔曼最优方程（Bellman optimality equation）：<br>$$<br>V^*(s)=\max_{a\in\mathcal{A}}{r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)V^*(s^{\prime})}<br>$$</p><p>$$<br>Q^*(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)\max_{a^{\prime}\in\mathcal{A}}Q^*(s^{\prime},a^{\prime})<br>$$</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.jkzhang.ml">JackZhang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.jkzhang.ml/2025/02/22/#%20%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/">https://www.jkzhang.ml/2025/02/22/#%20%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.jkzhang.ml" target="_blank">JackZhang's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2025/02/20/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/" title="Attention is all I need：Transformer的原理和代码详解"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Attention is all I need：Transformer的原理和代码详解</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s3.bmp.ovh/imgs/2024/06/07/50ea7f3f41ccf524.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">JackZhang</div><div class="author-info__description">人，可以生如蚁而美如神</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">63</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhangrongxiang" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="/2437603206@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这个blog已经年久失修，关于我的文章，欢迎来CSDN访问 🥰</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">贝尔曼方程</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E8%84%89%E7%BB%9C/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E8%84%89%E7%BB%9C/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.493Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.491Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/%E6%94%B9%E5%8A%A8/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/%E6%94%B9%E5%8A%A8/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.490Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/reward-to-go/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/reward-to-go/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.484Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.479Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By JackZhang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>