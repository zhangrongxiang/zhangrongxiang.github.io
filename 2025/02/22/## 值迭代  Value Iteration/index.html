<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>JackZhang's Blog | JackZhang's Blog</title><meta name="author" content="JackZhang"><meta name="copyright" content="JackZhang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="值迭代 | Value Iteration值迭代（Value Iteration）直接利用递归性，通过不断迭代逼近最优状态值函数 $v^{*}(s)$。整个算法可以概括为：$$v^{(k+1)}&#x3D;f\left(v^{(k)}\right)&#x3D;\max _\pi\left(r_\pi+\gamma P_\pi v^{(k)}\right), \quad k&#x3D;1,2,3 \ldots$$因此这个算法理论"><meta property="og:type" content="article"><meta property="og:title" content="JackZhang&#39;s Blog"><meta property="og:url" content="https://www.jkzhang.ml/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/index.html"><meta property="og:site_name" content="JackZhang&#39;s Blog"><meta property="og:description" content="值迭代 | Value Iteration值迭代（Value Iteration）直接利用递归性，通过不断迭代逼近最优状态值函数 $v^{*}(s)$。整个算法可以概括为：$$v^{(k+1)}&#x3D;f\left(v^{(k)}\right)&#x3D;\max _\pi\left(r_\pi+\gamma P_\pi v^{(k)}\right), \quad k&#x3D;1,2,3 \ldots$$因此这个算法理论"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2025-02-22T08:34:25.479Z"><meta property="article:modified_time" content="2025-01-15T13:52:43.346Z"><meta property="article:author" content="JackZhang"><meta property="article:tag" content="�?"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.jkzhang.ml/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,noticeOutdate:void 0,highlight:void 0,copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"JackZhang's Blog",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2025-01-15 21:52:43"}</script><script>(e=>{e.saveToLocal={set:(e,t,a)=>{0!==a&&(a={value:t,expiry:Date.now()+864e5*a},localStorage.setItem(e,JSON.stringify(a)))},get:e=>{var t=localStorage.getItem(e);if(t){t=JSON.parse(t);if(!(Date.now()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=(o,n={})=>new Promise((t,e)=>{const a=document.createElement("script");a.src=o,a.async=!0,a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},Object.keys(n).forEach(e=>{a.setAttribute(e,n[e])}),document.head.appendChild(a)}),e.getCSS=(o,n=!1)=>new Promise((t,e)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};e=saveToLocal.get("theme");"dark"===e?activateDarkMode():"light"===e&&activateLightMode();e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="JackZhang's Blog" type="application/atom+xml"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s3.bmp.ovh/imgs/2024/06/07/50ea7f3f41ccf524.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">63</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)"><nav id="nav"><span id="blog-info"><a href="/" title="JackZhang's Blog"><img class="site-icon" src="/img/favicon.ico"><span class="site-name">JackZhang's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">无题</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-22T08:34:25.479Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-15T13:52:43.346Z" title="更新于 2025-01-15 21:52:43">2025-01-15</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="值迭代-Value-Iteration"><a href="#值迭代-Value-Iteration" class="headerlink" title="值迭代 | Value Iteration"></a>值迭代 | Value Iteration</h2><p><strong>值迭代</strong>（Value Iteration）直接利用递归性，通过不断迭代逼近最优状态值函数 $v^{*}(s)$。整个算法可以概括为：<br>$$<br>v^{(k+1)}=f\left(v^{(k)}\right)=\max _\pi\left(r_\pi+\gamma P_\pi v^{(k)}\right), \quad k=1,2,3 \ldots<br>$$<br>因此这个算法理论上需要包含两个主要步骤：</p><ol><li><p><strong>策略更新（PU）</strong>：$\pi^{(k+1)} = \arg\max _\pi\left(r_\pi+\gamma P_\pi v^{(k)}\right)$</p><p>现在我求出了$\pi^{(k+1)}$，我就可以再代入下面的式子，再去求解$v^{(k+1)}$，他们有一个前后顺序！</p></li><li><p><strong>状态更新（VU）</strong>：$v^{(k+1)}=r_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}} v^{(k)}$</p></li></ol><p><strong>注意！此处的$v_k$并不是state value!因为step 2这个方程不是贝尔曼方程，是一个迭代式。对比一下贝尔曼方程的matrix-vector形式，你会看见方程左右应该是同一个$v_k$</strong></p><p>而策略 π 更新的时候由于使用了贪心选择，这两个步骤可以合并简化，<strong>直接使用最优的动作价值去更新状态价值</strong>：<br>$$<br>v^{(k+1)}(s) = \max_a q^{(k+1)}(s,a)<br>$$</p><blockquote><p>值得一提的是，贪心更新策略 π 也带来了一个有趣的现象：<strong>越靠近目标区域的状态越先变好</strong>。直观上就是因为 π 依赖于其他状态，而当其他状态都不好的时候无从更新，只有接近目标区域的状态有明确的优化方向。</p></blockquote><h3 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h3><ol><li><p><strong>初始化值函数</strong>：设定初始值 $v^{(0)}(s)$，通常初始化为全零或任意常数。</p></li><li><p><strong>迭代更新值函数</strong>： 对于每个状态 s，根据贝尔曼最优公式更新<br>$$<br>v^{(k+1)}(s) = \max_a \left[ \sum_r p(r \mid s, a) r + \gamma \sum_{s’} p(s’ \mid s, a) v^{(k)}(s’) \right]<br>$$</p></li><li><p><strong>判断收敛</strong>： 检查值函数更新是否足够小（例如$\lVert v^{(k+1)} - v^{(k)} \rVert_\infty &lt; \epsilon$），若收敛则停止迭代并输出 v(k+1)。</p></li><li><p><strong>推导策略</strong>： 一旦得到收敛的值函数 v∗(s)，通过以下方式获得对应的最优策略 π∗：</p></li></ol><p>$$<br>\pi^*(s) = \arg\max_a \left[ \sum_r p(r \mid s, a) r + \gamma \sum_{s’} p(s’ \mid s, a) v_*(s’) \right]<br>$$<br>假设我们有一个3 x 3的棋盘：</p><ul><li>有一个单元格是超级玛丽，每回合可以往上、下、左、右四个方向移动</li><li>有一个单元格是宝藏，<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=5792339&content_type=Article&match_order=2&q=%E8%B6%85%E7%BA%A7%E7%8E%9B%E4%B8%BD&zhida_source=entity">超级玛丽</a>找到宝藏则游戏结束，目标是让超级玛丽以最快的速度找到宝藏</li><li>假设游戏开始时，宝藏的位置一定是(1, 2)</li></ul><p><img src="https://pic2.zhimg.com/v2-9f91c909194ebcc7a62e0e2d2e2e8837_1440w.jpg"></p><p>这个一个标准的马尔科夫决策过程(MDP)：</p><ul><li><strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=5792339&content_type=Article&match_order=1&q=%E7%8A%B6%E6%80%81%E7%A9%BA%E9%97%B4&zhida_source=entity">状态空间</a>State</strong>：超级玛丽当前的坐标</li><li><strong>决策空间Action</strong>: 上、下、左、右四个动作</li><li>**Action对State的影响和回报 P(State’, Reward | State, Action)**：本文认为该关系是已知的<ul><li>超级玛丽每移动一步，reward = -1</li><li>超级玛丽得到宝箱，reward = 0并且游戏结束</li></ul></li></ul><p>结合上图可以非常简单的理解价值迭代：</p><ul><li><p><strong>初始化</strong>：所有state的价值V(s) = 0</p></li><li><p>第一轮迭代：对于每个state，逐一尝试上、下、左、右四个Action</p><ul><li>记录Action带来的Reward、以及新状态 V(s’)</li><li>选择最优的Action，更新V(s) = Reward + V(s’) = -1 + 0</li><li>第一轮结束后，所有状态都有V(s) = -1，即从当前位置出发走一步获得Reward=-1</li></ul></li><li><p>第二轮迭代：对于每个state，逐一尝试上、下、左、右四个Action</p><ul><li>记录Action带来的Reward、以及新状态 V(s’)</li><li>选择最优的Action，更新V(s) = Reward + V(s’)<ul><li>对于宝箱周围的State，最优的Action是一步到达宝箱，V(s) = Reward + V(s’) = -1 + 0</li><li>对于其他State，所有的Action都是一样的，V(s) = Reward + V(s’) = -1 + -1</li></ul></li><li>第二轮结束后，宝箱周围的State的价值保持不变 V(s) = -1，其他State的价值 V(s) = -2</li></ul></li><li><p>第三轮迭代：对于每个state，逐一尝试上、下、左、右四个Action</p><ul><li>记录Action带来的Reward、以及新状态 V(s’)</li><li>选择最优的Action，更新V(s) = Reward + V(s’)<ul><li>对于宝箱周围的State，最优的Action是一步到达宝箱，V(s) = Reward + V(s’) = -1 + 0</li><li>对于宝箱两步距离的State，最优的Action是先一步到达宝箱周边的State，V(s) = Reward + V(s’) = -1 + -1</li><li>对于宝箱三步距离的State，所有Action都是一样的，V(s) = Reward + V(s’) = -1 + -2</li></ul></li></ul></li><li><p>第四轮迭代：对于每个state，逐一尝试上、下、左、右四个Action</p><ul><li><p>记录Action带来的Reward、以及新状态 V(s’)</p></li><li><p>选择最优的Action，更新V(s) = Reward + V(s’)</p><ul><li>对于宝箱周围的State，最优的Action是一步到达宝箱，V(s) = Reward + V(s’) = -1 + 0</li><li>对于宝箱两步距离的State，最优的Action是先一步到达宝箱周边的State，V(s) = Reward + V(s’) = -1 + -1</li><li>对于宝箱三步距离的State，最优的Action是所有Action都是一样的，V(s) = Reward + V(s’) = -1 + -2</li></ul></li><li><p>在第四轮迭代中，所有V(s)更新前后都没有任何变化，价值迭代已经找到了最优策略。、</p><p><img src="https://picx.zhimg.com/v2-3308c12abd8f521cfd6b30d0eaaf0b81_1440w.jpg"></p></li></ul></li></ul><h2 id="策略迭代-Policy-Iteration"><a href="#策略迭代-Policy-Iteration" class="headerlink" title="策略迭代 | Policy Iteration"></a>策略迭代 | Policy Iteration</h2><p>策略迭代是另一种求解 BOE 的方法，核心思想是交替优化策略和状态值函数，直到收敛到最优策略 π∗。</p><p>与值迭代不同的是，策略迭代先初始化一个<strong>随机策略</strong>，再交替进行以下两个主要步骤：</p><ul><li>策略评估（PE）：$v_{\pi^{(k)}}=r_{\pi^{(k)}}+\gamma P_{\pi^{(k)}} v_{\pi^{(k)}}$</li><li>策略改进（PI）：$\pi^{(k+1)} = \arg\max <em>\pi\left(r_\pi+\gamma P_\pi v</em>{\pi^{(k)}}\right)$</li></ul><p>这个算法按照如下顺序</p><p>$$<br>{\pi_0}\xrightarrow{PE}{v_{\pi_0}}\xrightarrow{PI}\pi_1\xrightarrow{PE}{v_{\pi_1}}\xrightarrow{PI}\pi_2\xrightarrow{PE}v_{\pi_2}\xrightarrow{PI}\ldots<br>$$</p><p>PE=policy evaluation, PI=policy improvement</p><ol><li><p>初始化策略：随机初始化一个初始策略$\pi^{(0)}$。</p></li><li><p>策略评估：在当前策略$\pi^{(k)}$下，通过求解以下线性方程组来获得状态</p></li></ol><p>值函数$v_{\pi^{(k)}}(s):$</p><p>$$<br>v_{\pi^{(k)}}(s)=\sum_a\pi^{(k)}(a\mid s)\left[\sum_rp(r\mid s,a)r+\gamma\sum_{s^{\prime}}p(s^{\prime}\mid s,a)v_{\pi^{(k)}}(s^{\prime})\right]<br>$$</p><p>​ $\circ$当状态空间较小时，可以直接解线性方程组，</p><p>​ $\circ$对于较大的问题，可以使用 Bootstrap 迭代逼近的方法。这个算法过程如下：首先把所有的$V_0(s)$都初始化成0，然后根据V0计算V1，…，一直继续下去直到收敛。根据$V_k(s)$计算$V_{k+1}(s)$的公式如下：<br>$$<br>V_{k+1}(s)=\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)[r+\gamma V_k(s’)]<br>$$<br>我们可以看一下收敛的时候，Vk+1=Vk，Δ=0，那么上面的迭代公式正好就是贝尔曼方程！有了上面的迭代算法，任何给定的策略ππ，我们都可以计算出它的价值函数。</p><ol start="3"><li>策略改进：基于更新后的值函数$v_{\pi^{(k)}}$,贪心更新策略：</li></ol><p>$$<br>\pi^{(k+1)}(s)=\arg\max_a\left[\sum_rp(r\mid s,a)r+\gamma\sum_{s’}p(s’\mid s,a)v_{\pi^{(k)}}(s’)\right]<br>$$</p><ol start="4"><li><p>检查收敛：如果策略不再变化，即$\pi^{(k+1)}=\pi^{(k)}$,则算法结束，$\pi^{(k)}$<br>即为最优策略；否则返回步骤 2。</p><p><img src="C:\Users\24376\AppData\Roaming\Typora\typora-user-images\image-20250115181005916.png" alt="image-20250115181005916"></p></li></ol><p><img src="C:\Users\24376\AppData\Roaming\Typora\typora-user-images\image-20250115181032097.png" alt="image-20250115181032097"></p><h2 id="策略提升定理"><a href="#策略提升定理" class="headerlink" title="策略提升定理"></a>策略提升定理</h2><p>假设$\pi$和$\pi^\prime$是两个确定的策略(一个状态s下只有一个行为a的概率是1，其余是0),如果对于所有的<br>$s\in\mathcal{S}_\text{都有：}$</p><p>$$<br>q_\pi(s,\pi^{\prime}(s))\geq v_\pi(s)<br>$$</p><p>那么策略$\pi^{\prime}$比$\pi$要“好”,也就是对于所有的$s\in\mathcal{S}:$</p><p>$$v_{\pi^{\prime}}(s)\geq v_\pi(s)$$</p><p>并且如果第一个不等式是严格大于，那么第二个不等式也是严格大于，也就是$\pi^{\prime}$一定比$\pi$更好(而不<br>是一样)。</p><p>如果这个定理成立，那就证明了我们前面结论，因为我们新的策略在s的时候采取的满足<br>$q_\pi(s,a)\geq v_\pi(s)$,而其它的状态s’都是一样的，因此新的策略会更好。当然上面的新策略是随便选择一个满足条件的行为a，但更好的办法是从所有可能的a里选择$q_\pi(s,a)$最大的那个a，此外我们可以改变所有s的策略，而不是一个s，这个改进版本就是我们最终用到的策略提升算法。如果所有的状态，当前的a已经是$q_\pi(s,a)$中最大的那个呢？那说明当前策略已经是最优的策略了。下面我们来证明这个定理：<br>$$<br>\begin{aligned}&amp;v_{\pi}(s)\leq q_{\pi}(s,\pi^{\prime}(s))\&amp;\text{t时刻使用策略}\pi^{\prime},\text{t+1时刻之后还是用策略}\pi\&amp;=\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s]\&amp;\text{用前面的假设}\&amp;\leq\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma q_{\pi}(S_{t+1},\pi^{\prime}(S_{t+1}))|S_{t}=s]\&amp;=\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+2}+\gamma v_{\pi}(S_{t+2})]|S_{t}=s]\&amp;=\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma R_{t+2}+\gamma^{2}v_{\pi}(S_{t+2})|S_{t}=s]\&amp;\leq\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\gamma^{3}v_{\pi}(S_{t+3})|S_{t}=s]\&amp;\ldots\&amp;\leq\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\gamma^{3}R_{t+4}\ldots+|S_{t}=s]\&amp;=v_{\pi^{\prime}}(s)\end{aligned}<br>$$</p><p>上面的证明就是不停的应用假设条件，里面有一点就是$E_\pi E_\pi X=E_\pi X$,因为求一次期望之后就和</p><p>$\pi$无关了，可以认为是常量了。</p><p>我们最终使用的策略是“贪心”的策略，对于所有的s，我们都选择qπ(s,a)最大的那个a。</p><table><thead><tr><th><strong>算法</strong></th><th><strong>优点</strong></th><th><strong>缺点</strong></th><th><strong>适用场景</strong></th></tr></thead><tbody><tr><td>值迭代</td><td>通过单个过程即可直接逼近最优值函数，简单、直接，适用于小规模问题</td><td>收敛速度较慢，尤其是当状态空间较大或 γ 接近 1 时，每次迭代的更新量可能变得很小</td><td>状态空间较小、对精度要求较高</td></tr><tr><td>策略迭代</td><td>收敛速度快，通常只需几轮策略更新即可收敛</td><td>每轮策略评估计算量大，尤其是在状态空间很大时，可能成为瓶颈</td><td>中小规模问题、对速度要求较高</td></tr><tr><td>截断策略迭代</td><td>权衡了值迭代和策略迭代的优缺点，减少了策略评估的计算开销，相较于策略迭代更高效</td><td>收敛速度可能略低于完整的策略迭代</td><td>大规模问题、需要减少计算开销</td></tr></tbody></table></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.jkzhang.ml">JackZhang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.jkzhang.ml/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/">https://www.jkzhang.ml/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.jkzhang.ml" target="_blank">JackZhang's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/02/22/reward-to-go/"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2025/02/22/#%20%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info"></div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s3.bmp.ovh/imgs/2024/06/07/50ea7f3f41ccf524.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">JackZhang</div><div class="author-info__description">人，可以生如蚁而美如神</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">63</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhangrongxiang" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="/2437603206@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这个blog已经年久失修，关于我的文章，欢迎来CSDN访问 🥰</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E8%BF%AD%E4%BB%A3-Value-Iteration"><span class="toc-number">1.</span> <span class="toc-text">值迭代 | Value Iteration</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.1.</span> <span class="toc-text">具体步骤</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3-Policy-Iteration"><span class="toc-number">2.</span> <span class="toc-text">策略迭代 | Policy Iteration</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%8F%90%E5%8D%87%E5%AE%9A%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">策略提升定理</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E8%84%89%E7%BB%9C/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E8%84%89%E7%BB%9C/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.493Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.491Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/%E6%94%B9%E5%8A%A8/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/%E6%94%B9%E5%8A%A8/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.490Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/reward-to-go/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/reward-to-go/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.484Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.479Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By JackZhang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>