<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Attention is all I need：Transformer的原理和代码详解 | JackZhang's Blog</title><meta name="author" content="JackZhang"><meta name="copyright" content="JackZhang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="attention机制，attention原理，Transformer原理，Transformer代码"><meta property="og:type" content="article"><meta property="og:title" content="Attention is all I need：Transformer的原理和代码详解"><meta property="og:url" content="https://www.jkzhang.ml/2025/02/20/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/index.html"><meta property="og:site_name" content="JackZhang&#39;s Blog"><meta property="og:description" content="attention机制，attention原理，Transformer原理，Transformer代码"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2025-02-20T09:00:00.000Z"><meta property="article:modified_time" content="2025-02-22T08:38:37.553Z"><meta property="article:author" content="JackZhang"><meta property="article:tag" content="deep learning"><meta property="article:tag" content="transformers"><meta property="article:tag" content="attention"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.jkzhang.ml/2025/02/20/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,noticeOutdate:void 0,highlight:void 0,copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Attention is all I need：Transformer的原理和代码详解",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2025-02-22 16:38:37"}</script><script>(e=>{e.saveToLocal={set:(e,t,a)=>{0!==a&&(a={value:t,expiry:Date.now()+864e5*a},localStorage.setItem(e,JSON.stringify(a)))},get:e=>{var t=localStorage.getItem(e);if(t){t=JSON.parse(t);if(!(Date.now()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=(o,n={})=>new Promise((t,e)=>{const a=document.createElement("script");a.src=o,a.async=!0,a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},Object.keys(n).forEach(e=>{a.setAttribute(e,n[e])}),document.head.appendChild(a)}),e.getCSS=(o,n=!1)=>new Promise((t,e)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};e=saveToLocal.get("theme");"dark"===e?activateDarkMode():"light"===e&&activateLightMode();e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="JackZhang's Blog" type="application/atom+xml"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s3.bmp.ovh/imgs/2024/06/07/50ea7f3f41ccf524.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">63</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)"><nav id="nav"><span id="blog-info"><a href="/" title="JackZhang's Blog"><img class="site-icon" src="/img/favicon.ico"><span class="site-name">JackZhang's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Attention is all I need：Transformer的原理和代码详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-20T09:00:00.000Z" title="发表于 2025-02-20 17:00:00">2025-02-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-02-22T08:38:37.553Z" title="更新于 2025-02-22 16:38:37">2025-02-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Deep-Learning/">Deep Learning</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Deep-Learning/Transformers/">Transformers</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Attention is all I need：Transformer的原理和代码详解"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>Transformer可运行的代码发布在<a target="_blank" rel="noopener" href="https://github.com/JinHanLei/Transformers_tutorial">GitHub</a><br>{: .prompt-tip }</p></blockquote><p>提到ChatGPT的原理，就绕不开Transformer，Transformer中的核心思想之一便是<strong>Attention</strong>，Attention机制彻底击败了在此之前的绝对王者RNN模式，并统治各大NLP任务直到现在。正因如此，Transformer的论文不叫Transformer，而是叫做<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">《Attention is all you need》</a>。本文是以我的理解，阐述Transformer是怎么想出来的，为什么这么设计。</p><h2 id="Attention的思想"><a href="#Attention的思想" class="headerlink" title="Attention的思想"></a>Attention的思想</h2><p>Attention的关键在于理解$$QKV$$，即Query、Key和Value。可以将Attention机制看作一种寻址操作：存储器中存有键Key和值Value，当前产生了一个Query的查询，要查询出Value，那么首先需要匹配Query和Key的相似度。举个也许不恰当，但直观的例子，有以下Key和Value：</p><table><thead><tr><th align="center">Key</th><th align="center">Value</th></tr></thead><tbody><tr><td align="center">段誉的招牌武功</td><td align="center">六脉神剑</td></tr><tr><td align="center">段誉的生父</td><td align="center">段延庆</td></tr><tr><td align="center">段誉的结拜兄弟</td><td align="center">乔峰和虚竹</td></tr><tr><td align="center">乔峰的招牌武功</td><td align="center">降龙十八章</td></tr></tbody></table><p>寻址流程如下：</p><ol><li>发起Query：“段誉的生父是谁？”</li><li>与Key相似度匹配到“段誉的生父”</li><li>返回Value“段延庆”</li></ol><p>这里的关键是相似度计算方法，通常是Query和Key的矩阵乘法，或加个缩放$$\sqrt{d_k}$$，或乘个$$W$$，如下：</p><ul><li>矩阵相乘：$$sim(Q,K)=QK^T$$</li><li>相乘加缩放：$$sim(Q,K)=\frac{QK^T}{\sqrt{d_k}}$$（Transformer使用，缩放使得训练可以收敛）</li><li>权重+激活：$$sim(Q,K)=tanh(WQ+UK)$$</li><li>权重+相乘：$$sim(Q,K)=QWK^T$$</li></ul><p>取出一个或者部分Value的方法叫Hard Attention，如上例只输出“段延庆”。但是，如果我问“段誉结拜兄弟的招牌武功是什么？”，Hard Attention可能匹配到“段誉的结拜兄弟”，输出“乔峰和虚竹”，这就不对了。替代方案就是Soft Attention，提供所有Value和对应的Attention权重，当$$Q=$$“段誉结拜兄弟的招牌武功是什么？”，结果可能如下表：</p><table><thead><tr><th align="center">Key</th><th align="center">Attention权重</th><th align="center">Value</th></tr></thead><tbody><tr><td align="center">段誉的招牌武功</td><td align="center">7</td><td align="center">六脉神剑</td></tr><tr><td align="center">段誉的生父</td><td align="center">1</td><td align="center">段延庆</td></tr><tr><td align="center">段誉的结拜兄弟</td><td align="center">9</td><td align="center">乔峰和虚竹</td></tr><tr><td align="center">乔峰的招牌武功</td><td align="center">5</td><td align="center">降龙十八章</td></tr></tbody></table><p>这样，就可以把高分答案结合，得到正确答案。当问题更宏大，需要的信息就更多，于是干脆每次都输出整张表，虽然有可能冗余，但这样得到的答案是既有重点、又完整的。</p><p>以往设备限制导致计算和输出全部非常困难，但现在设备的发展使得超大规模、超长文本输入的LLM得以出现，而Transformer的self-attention保证了LLM的效率和学习能力。</p><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>Self-Attention说来很简单，就是$$Q=K=V$$。</p><p>为什么要这么做？个人理解是让一句话先找出自己内部的关键词，再去适配下游的任务。例如$$Q=K=V=青花瓷$$，用Pytorch简单计算如下：</p><pre><code class="python">from torch import nn as nn
import torch
# &#123;青:0, 花:1, 瓷:2&#125;
tokens = torch.LongTensor([0, 1, 2])
# 将3个字转换成向量，向量维度为10
token_embedding = nn.Embedding(3, 10)
emb = token_embedding(tokens)
# 相似度计算
QK = torch.mm(emb, emb.T) / torch.sqrt(torch.FloatTensor([10]))
print(QK)
</code></pre><p>由于nn.Embedding随机初始化，所以结果会不一样，我的结果表述如下：</p><p>$$<br>sim(Q,K)=<br>\begin{bmatrix}<br>青 &amp; 花 &amp; 瓷<br>\end{bmatrix}<br>\times<br>\begin{bmatrix}<br>青\<br>花\<br>瓷<br>\end{bmatrix}<br>/10<br>=<br>\begin{bmatrix}<br>3.2897 &amp; 0.7432 &amp; -1.1652 \<br>0.7432 &amp; 1.3647 &amp; -1.1707\<br>-1.1652 &amp; -1.1707 &amp; 5.6380<br>\end{bmatrix}<br>$$</p><p>矩阵对角线表示自身的相似度，比如3.2897就表示“青”和“青”的相似度，就很大。每行代表每个字的权重。由于点积可以产生任意大的数字，这会破坏训练过程的稳定性，因此需要 $$Softmax$$。Attention的公式表示为：</p><p>$$<br>\text{Attention}(Q, K, V) = \text{Softmax} \big( \frac{QK^T}{\sqrt{d_k}} \big)V<br>$$</p><p>代码只须再加上：</p><pre><code class="python">Attention = torch.mm(torch.softmax(QK, dim=-1), emb)
</code></pre><p>这样得到矩阵的每行就表示[青, 花, 瓷]这三个字的Attention。在训练过程中会更新这些参数，从而根据上下文和标签得到更好的向量表示。</p><h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h3><p>为了关注到更多信息，Transformer采用Multi-head Attention机制，也就是重复n次Attention操作然后拼接，得到和原来的Attention维度相同的MultiHead，公式为：</p><p>$$<br>\begin{gather}head_i = \text{Attention}(\boldsymbol{Q}\boldsymbol{W}_i^Q,\boldsymbol{K}\boldsymbol{W}_i^K,\boldsymbol{V}\boldsymbol{W}_i^V)\\text{MultiHead}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \text{Concat}(head_1,…,head_h)\boldsymbol{W}^O\end{gather}<br>$$</p><p>其中 $$\boldsymbol{W}<em>i^Q\in\mathbb{R}^{d</em>{model}\times d_k}, \boldsymbol{W}<em>i^K\in\mathbb{R}^{d</em>{model}\times d_k}, \boldsymbol{W}<em>i^V\in\mathbb{R}^{d</em>{model}\times d_v},\boldsymbol{W}^O\in\mathbb{R}^{hd_{v}\times d_{model}}$$ 。</p><p>原文模型的维度$$d_{model}$$是512，我们设置$$h=8$$个注意力头，那么$$d_k=d_v=d_{model}/h=64$$。每个注意力头负责关注某一方面的语义相似性，多个头就可以让模型同时关注多个方面。不怎么严谨的代码如下，便于理解：</p><pre><code class="python">from torch import nn
import torch.nn.functional as F
from math import sqrt
import torch

class AttentionHead(nn.Module):
    def __init__(self, embed_dim, head_dim):
        super().__init__()
        self.WQ = nn.Linear(embed_dim, head_dim)
        self.WK = nn.Linear(embed_dim, head_dim)
        self.WV = nn.Linear(embed_dim, head_dim)

    def forward(self, query, key, value):
        QK = torch.mm(WQ(query), WK(key).T) / torch.sqrt(query.size(-1))
        Attention = torch.mm(torch.softmax(QK, dim=-1), WV(value))
        return Attention
    
class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config.hidden_size
        num_heads = config.num_attention_heads
        head_dim = embed_dim // num_heads
        self.heads = nn.ModuleList(
            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]
        )
        self.output_linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, query, key, value):
        MultiHead = torch.cat([h(query, key, value) for h in self.heads], dim=-1)
        x = self.output_linear(MultiHead)
        return x
</code></pre><p>更多时候，为了并行效率，多头操作是先乘上$$\boldsymbol{W}\in\mathbb{R}^{d_{model}\times d_{model}}$$的权重矩阵，再将QKV切块相乘，同一个结果但是抛弃了<code>for</code>循环，<a target="_blank" rel="noopener" href="https://github.com/JinHanLei/Transformers_tutorial">我的仓库</a>中就是这种做法。Transformer最核心的就是上文所述的Attention，下面介绍其他部分。</p><h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>之前的博客介绍了Encoder-Decoder结构，Transformer也遵从这种结构。而Transformer“浑身都是宝”，每个部分都被开发出了作用：</p><ul><li><strong>Transformer的Encoder</strong>（如<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT</a>），又称自编码 (auto-encoding) Transformer 模型</li><li><strong>Transformer的Decoder</strong>（如<a target="_blank" rel="noopener" href="https://openai.com/blog/language-unsupervised/">GPT系列</a>），又称自回归 (auto-regressive) Transformer 模型</li><li><strong>完整的Encoder-Decoder</strong>（例如<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.13461">BART</a>、<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.10683.pdf">T5</a>等）</li></ul><p>理解了Transformer，以上模型的上手难度会小很多，我们之后再了解。Transformer整体结构如图：</p><p><img src="/imgs/transformer.jpeg" alt="transformer"></p><p>左边是Encoder，右边是Decoder，可以看到两边都有：</p><ul><li>Positional Encoding</li><li>Multi-head Attention</li><li>Feed Forward</li><li>Add &amp; Norm。</li></ul><p>Multi-head Attention已经在上文介绍了，介绍下其他几位。</p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>在Attention中其实可以看出，并没有任何有关位置的特征，这样”一步两步三步四步望着天“的每一个”步“向量都是一样的，甚至把这句话变成”天着望步四步三步两步”一“，都是一样的，这显然不合理。因为每一步情绪都是递进的，而Attention无法解决前后顺序和句子内的一词多义。</p><p>RNN通过记忆使得每一步不一样，而Transformer采用了<em>Positional Encoding</em>，即位置编码，其公式如下：</p><p>$$<br>\begin{gather}<br>PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\<br>PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})<br>\end{gather}<br>$$</p><p>意思就是向量的偶数位置填$$sin$$，奇数位置填$$cos$$，$$pos$$指相对位置，如”青花瓷“的”青“的$$pos$$就是0。对”青花瓷“这三个10维的向量进行位置编码，简单实现代码如下：</p><pre><code class="python">import math
import torch

seq_len = 3
d_model = 10
pe = torch.zeros(seq_len, d_model)
position = torch.arange(0, seq_len).unsqueeze(1)
div_term = torch.exp(torch.arange(0, d_model, 2) *
                     -math.log(10000.0) / d_model)
pe[:, 0::2] = torch.sin(position * div_term)
pe[:, 1::2] = torch.cos(position * div_term)
print(pe)
</code></pre><p>由于次方比较难算，利用$e^{lnx}=x$的性质，代码中进行了如下转换：</p><p>$$<br>\frac{1}{10000^{2i/d}}<br>=e^{ln\frac{1}{10000^{2i/d}}}<br>=e^{-\frac{2i}{d}ln10000}<br>$$</p><p>得到的PE矩阵用来加上原始的词向量。位置编码相当于根据位置给权重。为什么这么做？我们从0开始思考，让我设计位置编码，怎么设计？</p><p>$$<br>一步两步三步四步望着天:[0,1,2,3,4,5,6,7,8,9,10]<br>$$</p><p>问题在哪？跟embedding向量的数字相比，这个数字太大了，那归一化一下：</p><p>$$<br>一步两步三步四步望着天:[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]<br>$$</p><p>这样的问题在于无法体现位置关系，因为注意力是相乘，下面采取相乘：$0.1 \times 1=0.2 \times 0.5$，不同距离相乘居然一样，反之，相同距离相乘居然不一样。大概有头绪了，位置编码起码需要满足这些条件：SSS</p><ol><li>相同距离相乘一样；</li><li>不同距离相乘不一样；</li><li>数量不限：序列无论多长都能找到相应的位置编码表示。</li></ol><p>Transformer本身是加入额外的位置，词向量加上位置的<strong>绝对位置编码</strong>。另外，还有修改Attention结构的<strong>相对位置编码</strong>。而下面介绍苏神的<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.09864">Rope</a>结合了两者。</p><h4 id="Rope位置编码"><a href="#Rope位置编码" class="headerlink" title="Rope位置编码"></a>Rope位置编码</h4><p>根据Transformer的位置编码公式，$pos+k$位置的编码如下：</p><p>$$<br>\begin{gather}<br>PE_{(pos+k,2i)}=sin((pos+k)/10000^{2i/d_{model}})\<br>PE_{(pos+k,2i+1)}=cos((pos+k)/10000^{2i/d_{model}})<br>\end{gather}<br>$$</p><p>先令$w_i=1/10000^{2i/d_{model}}$，根据：</p><p>$$<br>\begin{gather}<br>sin(\alpha+\beta)=sin\alpha \cdot cos\beta+cos\alpha \cdot sin\beta\<br>cos(\alpha+\beta)=cos\alpha \cdot cos\beta-sin\alpha \cdot sin\beta<br>\end{gather}<br>$$</p><p>推导出：</p><p>$$<br>\begin{gather}<br>PE_{(pos+k,2i)}=sin(w_i(pos+k))=sin(w_ipos) \cdot cos(w_ik)+cos(w_ipos) \cdot sin(w_ik)\<br>PE_{(pos+k,2i+1)}=cos(w_i(pos+k))=cos(w_ipos) \cdot cos(w_ik)-sin(w_ipos) \cdot sin(w_ik)<br>\end{gather}<br>$$</p><p>就是多了$k$的部分，可以表示为矩阵：</p><p>$$<br>\begin{bmatrix}<br>PE_{(pos+k,2i)} \<br>PE_{(pos+k,2i+1)}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>cos(w_ik) &amp; sin(w_ik) \<br>-sin(w_ik) &amp; cos(w_ik)<br>\end{bmatrix}<br>\times<br>\begin{bmatrix}<br>PE_{(pos,2i)} \<br>PE_{(pos,2i+1)}<br>\end{bmatrix}<br>$$</p><p>令:</p><p>$$<br>R_k=<br>\begin{bmatrix}<br>cos(w_ik) &amp; sin(w_ik) \<br>-sin(w_ik) &amp; cos(w_ik)<br>\end{bmatrix}^T<br>$$</p><p>根据：</p><p>$$<br>\begin{gather}<br>-sinx=sin-x \<br>cosx=cos-x<br>\end{gather}<br>$$</p><p>易得：</p><p>$$<br>R_k = R_{-k}^T<br>$$</p><p>并有如下性质，从而可以表示相对位置：</p><p>$$<br>R_{k_2-k_1}=R_{k_1}^TR_{k_2}<br>$$</p><p>对位置为$m$的词向量$A$和位置为$n$的$B$，对他们乘上$R_m$和$R_n$，就给Attention加上了绝对位置信息，并且具有$m-n$的相对位置信息：</p><p>$$<br>AR_m(BR_n)^T=AR_mR_n^TB=AR_{n-m}B<br>$$</p><p>Rope被许多大模型如<a target="_blank" rel="noopener" href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">LLaMA</a>、<a target="_blank" rel="noopener" href="https://huggingface.co/tiiuae/falcon-40b-instruct">falcon</a>等采用。以往较多模型采用直接embedding+学习的方式，但是这样最开始就定死了长度，遇到长文本只能截断，而Rope改良了这一点，使得大模型具有处理超长文本的能力。更多位置编码方式，可以参考苏神的博客<a target="_blank" rel="noopener" href="https://kexue.fm/archives/8130">《让研究人员绞尽脑汁的Transformer位置编码》</a>。</p><h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h3><p>Feed Forward简称FFN，在Encoder和Decoder的压轴处都各有一层，由两个全连接和<a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">ReLU</a>激活函数组成，如下式：</p><p>$$<br>FFN(X)=max(0,xW_1+b_1)W_2+b_2<br>$$</p><p>这个$$max$$就是ReLU。各个激活函数如图：</p><p><img src="/imgs/Activation_Functions.gif" alt="Activation"></p><p>ReLU的导数只有0和1，使得计算成本很低。</p><p>两层全连接把模型维度从512扩展到了2048又回到512。我找遍了原文和各个教程，都没有详细解释这一步的作用。</p><p>个人理解可能是扩展到更高维度以储存更多信息，但之前这么多参数还不够？</p><p>也可能是加个激活函数，但是之前也有Softmax。</p><p>有加入类似层进行调参的工作：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.00751">adapter</a>，但其真实设计意图是什么，也不得而知了。</p><h3 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add &amp; Norm"></a>Add &amp; Norm</h3><p>Add &amp; Norm由Add和Norm两部分组成。</p><p>Add是将箭头指过来的两者相加，包括Attention和原embedding这两个矩阵相加、以及过了FFN和没过之前的矩阵相加，这个很简单，就不细说了。</p><p>Norm指标准化，Transformer中使用<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">LayerNorm</a>，在图像领域常使用BatchNorm，两者都是拿均值方差做标准化处理，都是下式：</p><p>$$<br>y=\frac{x-E[x]}{\sqrt{Var[x]+\epsilon }}<br>$$</p><p>其中$$E[x]$$为均值，$$Var[x]$$为方差，$$\epsilon$$是一个很小的常数，用于避免分母为零。区别就在于两者拿来算均值方差的数不一样，如下：</p><ul><li>LayerNorm的均值方差，是对单个样本的不同特征做操作，即每个词向量内部；</li><li>BatchNorm是对不同样本的同一特征做操作。</li></ul><p>拿“青花瓷”的相似矩阵为例，手推LayerNorm：</p><pre><code class="python">import torch.nn as nn
import math
matrix = torch.Tensor([[3.2897, 0.7432, -1.1652],
                       [0.7432, 1.3647, -1.1707],
                       [-1.1652, -1.1707, 5.6380]])
# torch官方的layer_norm
layer_norm = nn.LayerNorm(matrix.shape[-1])
y_torch = layer_norm(matrix)
print(y_torch)
# 初始化一个与matrix大小相同的全0矩阵
y_ours = torch.zeros_like(matrix)
# 手推均值方差和LayerNorm
for row in range(matrix.shape[0]):
    E = sum(matrix[row]) / len(matrix[row])
    Var = 0
    for col in range(matrix.shape[1]):
        Var += (matrix[row, col] - E) ** 2 / len(matrix[row])
    for col in range(matrix.shape[1]):
        y_ours[row, col] = (matrix[row, col] - E) / math.sqrt(Var + 1e-5)
print(y_ours)
</code></pre><p>求得的结果是一样的。可以看到结果中，有小于0也有大于1的，因此我认为有些教程称之为“归一化”是不合理的，归一化是通过MinMax将所有数据转换至0-1范围内。而LayerNorm，明显是均值0方差1的<strong>标准化</strong>。</p><p>BatchNorm的代码如下：</p><pre><code class="python">import torch.nn as nn
import math
matrix = torch.Tensor([[3.2897, 0.7432, -1.1652],
                       [0.7432, 1.3647, -1.1707],
                       [-1.1652, -1.1707, 5.6380]])
layer_norm = nn.BatchNorm1d(matrix.shape[-1])
y_batch = layer_norm(matrix)
print(y_batch)
y_ours = torch.zeros_like(matrix)
# 手动求均值方差和norm
for col in range(matrix.shape[1]):
    E = sum(matrix[:, col]) / len(matrix[:, col])
    Var = 0
    for row in range(matrix.shape[0]):
        Var += (matrix[row, col] - E) ** 2 / len(matrix[row])
    for row in range(matrix.shape[1]):
        y_ours[row, col] = (matrix[row, col] - E) / math.sqrt(Var + 1e-5)
print(y_ours)
</code></pre><p>差别就在于LayerNorm在行，BatchNorm在列。</p><p>Transformer 为什么使用 Layer？这个问题还没有啥定论，包括LN和BN为啥能work也众说纷纭，感兴趣的话可以参考<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">原文</a>和以下论文：</p><ul><li>PowerNorm: Rethinking Batch Normalization in Transformers <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.07845">[1]</a></li><li>Understanding and Improving Layer Normalization <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.07013">[2]</a></li></ul><p>了解Transformer这些基本组件后，还有值得探讨的是Mask。</p><h3 id="Transformer中的Mask"><a href="#Transformer中的Mask" class="headerlink" title="Transformer中的Mask"></a>Transformer中的Mask</h3><p>此Mask非彼BERT的的那个掩码Mask。这里的Mask指Pad Mask和Attention Mask。</p><p>在<strong>Encoder</strong>中，Pad Mask需要去掉$$<pad>$$的计算。什么是$$<pad>$$呢？为了并行计算提高训练速度，通常把数据打包成batch，一批批训练，例如一个batch：</pad></pad></p><ul><li>[“青花瓷”, “爱在西元前”, “星晴”]</li></ul><p>但是模型只能处理长度相同的句，于是用&lt;pad&gt;填充到相同长度</p><ul><li>[“青花瓷$$<pad><pad>$$”, “爱在西元前”, “星晴$$<pad><pad><pad>$$”]</pad></pad></pad></pad></pad></li></ul><p>计算时把$$<pad>$$的位置设置成负无穷，softmax的值就趋于0，从而忽略。</pad></p><p>在<strong>Decoder</strong>中同样要Pad Mask，除此之外还需要Attention Mask来遮住后面的词。例如训练时文本是“星晴”，标签是“乘着风”，虽然知道标签全句“乘着风”，但是推理时是一个词一个词预测的，Decoder预测出“乘”时，并不知道后面是“着风”。为了在训练时适配推理，预测“着”时需要把“着风”给Mask，也就是去掉“乘”与“着”、“风”的相似度。</p><p>”乘着风$$<pad><pad>$$“的Attention Mask矩阵如图：</pad></pad></p><p>$$<br>\begin{matrix}<br>1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\<br>1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\<br>1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\<br>1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\<br>1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\<br>\end{matrix}<br>$$</p><p>对应相似度矩阵0的位置会被替换成负无穷，softmax后值就趋于0，从而使得Attention矩阵第一行就只有第一个字的权重，第二行有一二两个字的权重，以此类推。</p><h3 id="Transformer全览"><a href="#Transformer全览" class="headerlink" title="Transformer全览"></a>Transformer全览</h3><p>结合上述所有组件，对照着模型图，对Transformer做个全览。</p><p><img src="/imgs/transformer.jpeg" alt="transformer"></p><p>Encoder的流程如下：</p><ol><li><p>输入是<strong>“星晴”</strong>，先根据词表转化为2个向量的矩阵</p></li><li><p>加上位置信息</p></li><li><p>过Self-Attention</p></li><li><p>和没过Self-Attention的矩阵相加，然后LayerNorm标准化</p></li><li><p>过FFN，再和没过FFN的矩阵相加，然后LayerNorm标准化</p></li><li><p>得到跟输入向量维度一样的Encoder矩阵</p></li></ol><p>为了告诉Decoder从哪开始到哪结束，需要添加开始符和结束符，例如$$<sos>$$（start of sentence）和$$<eos>$$（end of sentence）。Decoder训练的流程如下：</eos></sos></p><ol><li>标签是$$\text{trg} = [sos, x_1, x_2, x_3, eos]$$，输入<code>trg[:-1]</code>，如“$$<sos>$$乘着风”，根据词表转化为4个向量的矩阵</sos></li><li>加上位置信息</li><li>过<strong>Attention Mask了</strong>的Self-Attention</li><li>和没过Self-Attention的矩阵相加，然后LayerNorm标准化</li><li><strong>得到的矩阵作为Q，Encoder矩阵作为KV，做Cross Attention</strong></li><li>和没过<strong>Cross Attention</strong>的矩阵相加，然后LayerNorm标准化</li><li>过FFN，再和没过FFN的矩阵相加，然后LayerNorm标准化</li><li>**过一层全连接和Softmax，得到$$\text{output} = [y_0, y_1, y_2, y_3]$$**，例如得到“乘着车$$<eos>$$”</eos></li><li>多分类问题用交叉熵计算<code>trg[1:]</code>和<code>output</code>间的损失、更新参数</li></ol><p>Decoder推理时，这个流程变成：</p><ol><li>向Decoder输入$$<sos>$$，输出$$[y_0]$$</sos></li><li>合并得$$[<sos>, y_0]$$，继续向Decoder输入，得$$[y_0^{‘}, y_1]$$</sos></li><li>合并得$$[<sos>, y_0, y_1]$$，以此类推</sos></li><li>当预测到$$<eos>$$，或者达到设置的最大长度，停止</eos></li></ol><p>通过本章，相信你已经对 Transformer 模型的定义和发展有了大概的了解。幸运的是，<a target="_blank" rel="noopener" href="https://huggingface.co/">Hugging Face</a> 专门为使用 Transformer 模型编写了一个 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/index">Transformers 库</a>，并且在<a target="_blank" rel="noopener" href="https://huggingface.co/models">Hugging Face Hub</a>中提供了训练好的模型以供快速使用。</p><p>在后面的章节中我会手把手地带你编写并训练自己的 Transformer 模型。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.jkzhang.ml">JackZhang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.jkzhang.ml/2025/02/20/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/">https://www.jkzhang.ml/2025/02/20/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.jkzhang.ml" target="_blank">JackZhang's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/deep-learning/">deep learning</a><a class="post-meta__tags" href="/tags/transformers/">transformers</a><a class="post-meta__tags" href="/tags/attention/">attention</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/02/22/#%20%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2024/06/13/Linux%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/" title="Linux目录结构"><img class="cover" src="https://s3.bmp.ovh/imgs/2024/06/13/e2aa614f45568c02.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Linux目录结构</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s3.bmp.ovh/imgs/2024/06/07/50ea7f3f41ccf524.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">JackZhang</div><div class="author-info__description">人，可以生如蚁而美如神</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">63</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zhangrongxiang" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="/2437603206@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这个blog已经年久失修，关于我的文章，欢迎来CSDN访问 🥰</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention%E7%9A%84%E6%80%9D%E6%83%B3"><span class="toc-number">1.</span> <span class="toc-text">Attention的思想</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-Attention"><span class="toc-number">1.1.</span> <span class="toc-text">Self-Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-head-Attention"><span class="toc-number">1.2.</span> <span class="toc-text">Multi-head Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoder-Decoder"><span class="toc-number">2.</span> <span class="toc-text">Encoder-Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-Encoding"><span class="toc-number">2.1.</span> <span class="toc-text">Positional Encoding</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Rope%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">2.1.1.</span> <span class="toc-text">Rope位置编码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feed-Forward"><span class="toc-number">2.2.</span> <span class="toc-text">Feed Forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Add-amp-Norm"><span class="toc-number">2.3.</span> <span class="toc-text">Add &amp; Norm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E4%B8%AD%E7%9A%84Mask"><span class="toc-number">2.4.</span> <span class="toc-text">Transformer中的Mask</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E5%85%A8%E8%A7%88"><span class="toc-number">2.5.</span> <span class="toc-text">Transformer全览</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E8%84%89%E7%BB%9C/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E8%84%89%E7%BB%9C/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.493Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.491Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/%E6%94%B9%E5%8A%A8/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/%E6%94%B9%E5%8A%A8/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.490Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/reward-to-go/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/reward-to-go/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.484Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="无题"></a><div class="content"><a class="title" href="/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/" title="无题">无题</a><time datetime="2025-02-22T08:34:25.479Z" title="发表于 2025-02-22 16:34:25">2025-02-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By JackZhang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>