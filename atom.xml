<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JackZhang&#39;s Blog</title>
  
  <subtitle>JKzhang</subtitle>
  <link href="https://www.jkzhang.ml/atom.xml" rel="self"/>
  
  <link href="https://www.jkzhang.ml/"/>
  <updated>2025-01-09T10:43:29.856Z</updated>
  <id>https://www.jkzhang.ml/</id>
  
  <author>
    <name>JackZhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://www.jkzhang.ml/2025/02/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E8%84%89%E7%BB%9C/"/>
    <id>https://www.jkzhang.ml/2025/02/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E8%84%89%E7%BB%9C/</id>
    <published>2025-02-22T08:34:25.493Z</published>
    <updated>2025-01-09T10:43:29.856Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习基本脉络"><a href="#强化学习基本脉络" class="headerlink" title="强化学习基本脉络"></a>强化学习基本脉络</h1><p><img src="C:\Users\24376\AppData\Roaming\Typora\typora-user-images\image-20250109094839938.png" alt="image-20250109094839938"></p><p>这张图包括<strong>基本工具</strong>和<strong>算法/方法</strong>两部分，基本工具包括贝尔曼方程等概念。算法包括价值迭代、蒙特卡罗方法等。</p><h2 id="Chapter-2"><a href="#Chapter-2" class="headerlink" title="Chapter 2"></a>Chapter 2</h2><p>两个基本概念：</p><ul><li><p>One Concept: State Value<br>$$<br>V_{\pi}(s) \doteq \mathbb{E}<em>{\pi}\left[G</em>{t} \mid s_{t}=s\right]=\mathbb{E}<em>{\pi}\left[\sum</em>{k=0}^{\infty} \gamma^{k} r_{t+k+1} \mid s_{t}=s\right], \text{对于所有的} s \in S<br>$$<br>state value代表了奖励的平均值。状态值越高，说明策略越好。可以用来评价策略好还是不好。那么如何分析状态值呢，就要用一个工具：贝尔曼公式。</p></li><li><p>One Tool: Bellman Equation<br>$$<br>v_\pi=r_\pi+\gamma P_\pi v_\pi<br>$$<br>用一句话描述，他描述了状态与状态值的关系，可以给定一个策略，求出他的状态值。</p></li><li><p>policy evaluation: 后期广泛使用</p></li></ul><h2 id="Chapter-3"><a href="#Chapter-3" class="headerlink" title="Chapter 3"></a>Chapter 3</h2><p>贝尔曼最优公式：贝尔曼公式的特殊情况。它和最优策略有关。我们知道，强化学习的最终目标就是<strong>求解最优策略</strong>。所以第三章非常重要。我们要把握</p><ul><li><p>Two concepts</p><p>最优策略$\pi^*$和最优状态值</p></li><li><p>一个工具</p><p>贝尔曼最优公式<br>$$<br>v=\max_\pi{r_\pi+\gamma P_\pi v}=f(v)<br>$$<br>应用不动点原理分析，论证了一系列基础问题，例如最优策略的存在性，最优策略不一定是惟一的，但最优状态值是唯一的。</p></li></ul><h2 id="Chapter-4"><a href="#Chapter-4" class="headerlink" title="Chapter 4"></a>Chapter 4</h2><p>至此进入Algorithm/methods模块。</p><ul><li>Three Algorithms<ol><li>Value Iteration</li><li>Policy Iteration</li><li>Truncated Policy Iteration。他是前两种算法的统一描述。</li></ol></li></ul><p>这三个算法都有共同点：就是都是迭代算法。</p><p>分别用Policy Update和Value Update这两个步骤进行迭代。后面的蒙特卡洛算法等也都是基于这些步骤。</p><h2 id="Chapter-5"><a href="#Chapter-5" class="headerlink" title="Chapter 5"></a>Chapter 5</h2><p>**Gap: How to do model-free learning?**没有模型，该怎么学习呢？又要学习什么呢？我们要学习随机变量的期望值：<br>$$<br>E(X)=\bar{x}<br>$$</p><p>我们就学习了第一个不需要模型的强化学习算法。</p><ul><li>Algorithms<ol><li>MC Basic</li><li>MC Exploring Starts</li><li>MC $\epsilon$-greedy</li></ol></li></ul><p>第一个算法效率很低，但他是最核心的东西，学习这个算法，是为了把复杂的东西与简单的东西分离开，明白一些复杂的算法，核心是很简单的。<br>我们也能感受到强化学习一环扣一环，要学蒙特卡洛方法，就要先学policy iteration，要学policy iteration就要学value iteration</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;强化学习基本脉络&quot;&gt;&lt;a href=&quot;#强化学习基本脉络&quot; class=&quot;headerlink&quot; title=&quot;强化学习基本脉络&quot;&gt;&lt;/a&gt;强化学习基本脉络&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;C:\Users\24376\AppData\Roaming\Typor</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://www.jkzhang.ml/2025/02/22/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/"/>
    <id>https://www.jkzhang.ml/2025/02/22/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/</id>
    <published>2025-02-22T08:34:25.491Z</published>
    <updated>2025-01-15T10:24:30.149Z</updated>
    
    <content type="html"><![CDATA[<h1 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h1><p>在上一节中，我们介绍了策略迭代，它依赖于明确的环境模型（model-based）来进行策略评估和改进。然而，在许多现实问题中，环境模型不可用，或者我们无法轻易获得完整的转移概率 T 和奖励函数 R。这时，<strong>蒙特卡洛学习算法</strong>（Monte Carlo Learning）作为一种<strong>无模型</strong>（model-free）方法，提供了一种通过样本进行策略优化的途径。</p><p>蒙特卡洛方法的核心思想是：通过对环境的采样，基于多次模拟的经验回报，<strong>估计</strong>状态值函数或状态-动作值函数（Monte Carlo Estimation）。其依据就是经典的<strong>大数定律</strong>。</p><p>蒙特卡洛学习主要依赖以下几个关键特性：</p><ol><li><strong>基于样本</strong>：直接使用采样的经验（例如状态序列、奖励序列）进行学习。</li><li><strong>延迟更新</strong>：直到采样结束后，才对策略或值函数进行更新。</li><li><strong>无需环境模型</strong>：完全基于与环境的交互，适用于复杂环境。</li></ol><h2 id="蒙特卡罗算法"><a href="#蒙特卡罗算法" class="headerlink" title="蒙特卡罗算法"></a>蒙特卡罗算法</h2><p>与策略迭代类似，蒙特卡洛方法需要基于一个固定策略$\pi$。我们知道策略迭代包含两个主要步骤：</p><ul><li>策略评估 (PE): $v_{\pi^{(k)}}=r_{\pi^{(k)}}+\gamma P_{\pi^{(k)}}v_{\pi^{(k)}}$</li><li>策略改进(Pl):$\pi^{(k+1)}=\arg\max_{\pi}\left(r_{\pi}+\gamma P_{\pi}v_{\pi^{(k)}}\right)$</li></ul><p>这里面最核心的地方就是状态值$v_{\pi^{(k)}}$的求解，此时有两种办法：</p><ol><li>基于模型$T$和$R$,使用迭代法求解：</li></ol><p>$$<br>v_{\pi^{(k)}}(s)=\sum_a\pi^{(k)}(a\mid s)\left[\sum_rp(r\mid s,a)r+\gamma\sum_{s’}p(s’\mid s,a)v_{\pi^{(k)}}(s’)\right]<br>$$</p><ol start="2"><li>免模型求解，回归状态值的原始定义：</li></ol><p>$$<br>v_\pi(s)=\mathbb{E}_\pi\left[G_t\mid S_t=s\right]<br>$$</p><p>换句话说，我们可以从任意状态$s$出发，随机采样若干轨迹${\tau}$,使用这些轨迹的累计回报${g_t}$的平均<br>值来估计$s$的期望累计回报：<br>$$<br>v_\pi(s)=\mathbb{E}<em>\pi\left[G_t\mid S_t=s\right]\approx\frac{1}{N}\sum</em>{i=1}^Ng^{(i)}(s)<br>$$<br>同理，也可以估计动作$a$的期望累计回报：</p><p>$$<br>q_\pi(s,a)=\mathbb{E}_\pi\left[G_t\mid S_t=s,A_t=a\right]\approx\frac{1}{N}\sum_{i=1}^Ng^{(i)}(s,a)<br>$$<br>总之就是一句话：<strong>没有模型时，就得有数据！</strong>这里采样到的轨迹在统计学或概率学上会称为样本<br>(Sample),而在强化学习中则称为经验(Experience)。</p><p>具体步骤如下：</p><p>1.采样：从待求解状态$s_t$和动作$a_t$出发，基于策略$\pi$生成多条完整的轨迹，每条轨迹由状态、动作，<br>奖励序列组成，例如：</p><p>$$<br>\tau={s_t,a_t,r_{t+1},s_{t+1},a_{t+1},r_{t+2},\ldots,s_T}<br>$$</p><p>其中$T$ 是终止时刻。</p><p>2.回报计算：对于每个轨迹，计算累计回报(Return):<br>$$<br>g_t=\sum_{k=t}^T\gamma^{k-t}r_{k+1}<br>$$</p><p>3.更新：通过多次采样得到的回报${g_t}$,对动作价值函数进行更新：</p><p>$$<br>q_\pi(s_t,a_t)\leftarrow\frac{\text{累计回报和}}{\text{访问次数}}<br>$$</p><p>之后在策略改进 (PI)步骤中，依旧是贪心选择最大动作价值来更新策略。</p><p>有趣的现象——回合（Episode）的长度：</p><ul><li>为了 Gt 估计的准确性，我们希望每次采样到的 gt 都尽量精确，因此 Episode 也是<strong>越长越好</strong>。</li><li>如果 Episode 长度很短，例如 1，即模型只会探索一个步骤，最终只有目标周围一步的状态值会得到准确估计，其他地方均为 0，因为无法得到最终奖励。</li><li>推广开来，只有当 Episode 长度超过一个状态距离目标状态的最短路径时，该状态值才能得到更新；并且<strong>超过得越多，这个估计状态值就离真实值越接近</strong>。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;蒙特卡洛方法&quot;&gt;&lt;a href=&quot;#蒙特卡洛方法&quot; class=&quot;headerlink&quot; title=&quot;蒙特卡洛方法&quot;&gt;&lt;/a&gt;蒙特卡洛方法&lt;/h1&gt;&lt;p&gt;在上一节中，我们介绍了策略迭代，它依赖于明确的环境模型（model-based）来进行策略评估和改进。然而，</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://www.jkzhang.ml/2025/02/22/%E6%94%B9%E5%8A%A8/"/>
    <id>https://www.jkzhang.ml/2025/02/22/%E6%94%B9%E5%8A%A8/</id>
    <published>2025-02-22T08:34:25.490Z</published>
    <updated>2025-02-12T01:20:45.690Z</updated>
    
    <content type="html"><![CDATA[<p>基于网站《动手学强化学习》，其中的代码是几年前的，而名为 Gym Lib 的 Lib 已经过时了。因此，需要更改的地方，这是 DQN 和其他变体的完整新代码。 如果您只想实现 DQN，则只需下载它。<strong>但是，在 hands_on_RL 中使用 gym 的其他算法上需要更改的部分是相同的。因此，在这里我们展示需要更改的部分，以帮助您更改其他算法</strong></p><h2 id="需要更改"><a href="#需要更改" class="headerlink" title="需要更改"></a>需要更改</h2><h3 id="1-环境-‘env-name-v0’-已过期。您应该考虑升级到版本v1"><a href="#1-环境-‘env-name-v0’-已过期。您应该考虑升级到版本v1" class="headerlink" title="1. 环境 ‘env_name-v0’ 已过期。您应该考虑升级到版本v1"></a>1. 环境 ‘env_name-v0’ 已过期。您应该考虑升级到版本<code>v1</code></h3><pre><code>env_name = &#39;CartPole-v1&#39;env = gym.make(env_name, render_mode=&#39;human&#39;)</code></pre><h3 id="2-env-seed（0）-没有被使用，你应该改到那里，并在后面加上-‘-0-’"><a href="#2-env-seed（0）-没有被使用，你应该改到那里，并在后面加上-‘-0-’" class="headerlink" title="2. env.seed（0） 没有被使用，你应该改到那里，并在后面加上 ‘[0]’"></a>2. env.seed（0） 没有被使用，你应该改到那里，并在后面加上 ‘[0]’</h3><pre><code>state = env.reset(seed = 0)[0]</code></pre><h3 id="3-需要解包的值太多（预期为-4）：next-state、reward、done、-env-step（action）"><a href="#3-需要解包的值太多（预期为-4）：next-state、reward、done、-env-step（action）" class="headerlink" title="3. 需要解包的值太多（预期为 4）：next_state、reward、done、_ = env.step（action）"></a>3. 需要解包的值太多（预期为 4）：next_state、reward、done、_ = env.step（action）</h3><p>添加返回值</p><pre><code>next_state, reward, done, _, __ = env.step(action)</code></pre><h3 id="4-从-numpy-ndarrays-列表创建张量非常慢。请考虑使用-numpy-array（）-将列表转换为单个-numpy-ndarray"><a href="#4-从-numpy-ndarrays-列表创建张量非常慢。请考虑使用-numpy-array（）-将列表转换为单个-numpy-ndarray" class="headerlink" title="4. 从 numpy.ndarrays 列表创建张量非常慢。请考虑使用 numpy.array（） 将列表转换为单个 numpy.ndarray"></a>4. 从 numpy.ndarrays 列表创建张量非常慢。请考虑使用 numpy.array（） 将列表转换为单个 numpy.ndarray</h3><pre><code>def take_action(self. state)    ....    &#39;state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)&#39;    ....</code></pre><h3 id="5-新版本-env-无法自动停止，因此我们需要添加一个计数器"><a href="#5-新版本-env-无法自动停止，因此我们需要添加一个计数器" class="headerlink" title="5. 新版本 env 无法自动停止，因此我们需要添加一个计数器"></a>5. 新版本 env 无法自动停止，因此我们需要添加一个计数器</h3><pre><code>....    for i_episode in range(int(num_episodes / 10)):        episode_return = 0        state = env.reset(seed=0)[0]        done = False        &#39;steps = 0&#39;        while not done &#39;and steps &lt; 200&#39;:            &#39;steps += 1&#39;            ....</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;基于网站《动手学强化学习》，其中的代码是几年前的，而名为 Gym Lib 的 Lib 已经过时了。因此，需要更改的地方，这是 DQN 和其他变体的完整新代码。 如果您只想实现 DQN，则只需下载它。&lt;strong&gt;但是，在 hands_on_RL 中使用 gym 的其他算法</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://www.jkzhang.ml/2025/02/22/reward-to-go/"/>
    <id>https://www.jkzhang.ml/2025/02/22/reward-to-go/</id>
    <published>2025-02-22T08:34:25.484Z</published>
    <updated>2025-02-22T08:35:47.474Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title:Reward-to-go 数学推导<br>date: 2025-02-19 17:29:01<br>tags:</p><ul><li>强化学习</li><li>人工智能</li><li>技术</li><li>计算机</li><li>目录结构<br>categories: 技术<br>cover: <a href="https://s3.bmp.ovh/imgs/2024/06/13/e2aa614f45568c02.jpg">https://s3.bmp.ovh/imgs/2024/06/13/e2aa614f45568c02.jpg</a></li></ul><hr><h3 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h3><p>回顾一下原始的期望回报：</p><p><img src="https://spinningup.openai.com/en/latest/_images/math/e8b721fa0eb7fa2aa4b088106518b3ee88ff7707.svg"></p><p>我们的agent应该只根据某一action的<strong>后果来强化行动</strong>，因此<strong>采取行动之前获得的奖励与该行动的好坏无关：只与之后的奖励有关。</strong></p><p>从这一直觉出发，可以有：<br>$$<br>\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = \sum_{t=1}^T \mathbb{E}<em>{\tau \sim p</em>{\boldsymbol{\theta}}(\tau)} \left[ \nabla_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}} (\mathbf{a}<em>t \mid \mathbf{s}<em>t) ,\sum</em>{t’=t}^T  r(\mathbf{s}</em>{t’}, \mathbf{a}_{t’}) \right]<br>$$</p><p>下面我们用数学语言严格证明一下。</p><h3 id="EGLP引理"><a href="#EGLP引理" class="headerlink" title="EGLP引理"></a>EGLP引理</h3><p>先介绍一个结论，即Expected Grad-Log-Prob (EGLP) lemma。<br>$$<br>\mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right] = 0.<br>$$<br><strong>证明</strong>：</p><p>由于概率积分为1：</p><p><img src="https://spinningup.openai.com/en/latest/_images/math/cc70eeb5c704222ca73cf6d2a7b28b8ae2f2e2aa.svg"></p><p>两边取梯度：</p><p><img src="https://spinningup.openai.com/en/latest/_images/math/acde4d18e221ccd91e93e6ca4e25ae84d05f29d8.svg" alt="https://spinningup.openai.com/en/latest/_images/math/acde4d18e221ccd91e93e6ca4e25ae84d05f29d8.svg"></p><p>接着运用很常见的技巧（对数梯度）：</p><p><img src="https://spinningup.openai.com/en/latest/_images/math/479e34ecc40103079f87e46e47837f3c066d30bd.svg"></p><h2 id="reward-to-go证明"><a href="#reward-to-go证明" class="headerlink" title="reward-to-go证明"></a>reward-to-go证明</h2><p>展开表达式：<br>$$<br>\begin{aligned}<br>\nabla_{\theta} J(\theta) &amp;= \mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \left[ \left( \sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \right) \left( \sum_{t=1}^T r(s_t, a_t) \right) \right] \<br>%<br>&amp;= \sum_{t=1}^T \mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t’=1}^T r(s_{t’}, a_{t’}) \right] \<br>%<br>&amp;= \sum_{t=1}^T \mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t’=1}^{t-1} r(s_{t’}, a_{t’}) + \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t’=t}^T r(s_{t’}, a_{t’}) \right] \<br>%<br>&amp;= \sum_{t=1}^T \left( \mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t’=1}^{t-1} r(s_{t’}, a_{t’}) \right] \</p><ul><li>\mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t’=t}^T r(s_{t’}, a_{t’}) \right] \right) \<br>\end{aligned}<br>$$<br>在第$t$轮中，由于Markov假设，1~t-1时刻的奖励和$\nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t)$是独立的。因为action只和当前状态有关。</li></ul><p>于是就可以分离开：<br>$$<br>\nabla_{\theta} J(\theta) = \sum_{t=1}^T \left( \mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \left[ \sum_{t’=1}^{t-1} r(s_{t’}, a_{t’}) \right] \mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \right] \</p><ul><li>\mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t’=t}^T r(s_{t’}, a_{t’}) \right] \right)<br>$$</li></ul><p>第一项根据EGLP引理可以变为0.从而只留下最后一项。<br>$$<br>\begin{aligned}<br>\nabla_{\theta} J(\theta)<br>%<br>&amp;= \sum_{t=1}^T \mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t’=t}^T r(s_{t’}, a_{t’}) \right]  \<br>%<br>&amp;= \mathbb{E}<em>{\tau \sim \pi</em>{\theta}(\tau)} \sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \left( \sum_{t’=t}^T r(s_{t’}, a_{t’}) \right). \<br>\end{aligned}<br>$$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;p&gt;title:Reward-to-go 数学推导&lt;br&gt;date: 2025-02-19 17:29:01&lt;br&gt;tags:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;强化学习&lt;/li&gt;
&lt;li&gt;人工智能&lt;/li&gt;
&lt;li&gt;技术&lt;/li&gt;
&lt;li&gt;计算机&lt;/li&gt;
&lt;li&gt;目录结构</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://www.jkzhang.ml/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/"/>
    <id>https://www.jkzhang.ml/2025/02/22/##%20%E5%80%BC%E8%BF%AD%E4%BB%A3%20%20Value%20Iteration/</id>
    <published>2025-02-22T08:34:25.479Z</published>
    <updated>2025-01-15T13:52:43.346Z</updated>
    
    <content type="html"><![CDATA[<h2 id="值迭代-Value-Iteration"><a href="#值迭代-Value-Iteration" class="headerlink" title="值迭代 | Value Iteration"></a>值迭代 | Value Iteration</h2><p><strong>值迭代</strong>（Value Iteration）直接利用递归性，通过不断迭代逼近最优状态值函数 $v^{*}(s)$。整个算法可以概括为：<br>$$<br>v^{(k+1)}=f\left(v^{(k)}\right)=\max _\pi\left(r_\pi+\gamma P_\pi v^{(k)}\right), \quad k=1,2,3 \ldots<br>$$<br>因此这个算法理论上需要包含两个主要步骤：</p><ol><li><p><strong>策略更新（PU）</strong>：$\pi^{(k+1)} = \arg\max _\pi\left(r_\pi+\gamma P_\pi v^{(k)}\right)$</p><p>现在我求出了$\pi^{(k+1)}$，我就可以再代入下面的式子，再去求解$v^{(k+1)}$，他们有一个前后顺序！</p></li><li><p><strong>状态更新（VU）</strong>：$v^{(k+1)}=r_{\pi^{(k+1)}}+\gamma P_{\pi^{(k+1)}} v^{(k)}$</p></li></ol><p><strong>注意！此处的$v_k$并不是state value!因为step 2这个方程不是贝尔曼方程，是一个迭代式。对比一下贝尔曼方程的matrix-vector形式，你会看见方程左右应该是同一个$v_k$</strong></p><p>而策略 π 更新的时候由于使用了贪心选择，这两个步骤可以合并简化，<strong>直接使用最优的动作价值去更新状态价值</strong>：<br>$$<br>v^{(k+1)}(s) = \max_a q^{(k+1)}(s,a)<br>$$</p><blockquote><p>值得一提的是，贪心更新策略 π 也带来了一个有趣的现象：<strong>越靠近目标区域的状态越先变好</strong>。直观上就是因为 π 依赖于其他状态，而当其他状态都不好的时候无从更新，只有接近目标区域的状态有明确的优化方向。</p></blockquote><h3 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h3><ol><li><p><strong>初始化值函数</strong>：设定初始值 $v^{(0)}(s)$，通常初始化为全零或任意常数。</p></li><li><p><strong>迭代更新值函数</strong>： 对于每个状态 s，根据贝尔曼最优公式更新<br>$$<br>v^{(k+1)}(s) = \max_a \left[ \sum_r p(r \mid s, a) r + \gamma \sum_{s’} p(s’ \mid s, a) v^{(k)}(s’) \right]<br>$$</p></li><li><p><strong>判断收敛</strong>： 检查值函数更新是否足够小（例如$\lVert v^{(k+1)} - v^{(k)} \rVert_\infty &lt; \epsilon$），若收敛则停止迭代并输出 v(k+1)。</p></li><li><p><strong>推导策略</strong>： 一旦得到收敛的值函数 v∗(s)，通过以下方式获得对应的最优策略 π∗：</p></li></ol><p>$$<br>\pi^*(s) = \arg\max_a \left[ \sum_r p(r \mid s, a) r + \gamma \sum_{s’} p(s’ \mid s, a) v_*(s’) \right]<br>$$<br>假设我们有一个3 x 3的棋盘：</p><ul><li>有一个单元格是超级玛丽，每回合可以往上、下、左、右四个方向移动</li><li>有一个单元格是宝藏，<a href="https://zhida.zhihu.com/search?content_id=5792339&content_type=Article&match_order=2&q=%E8%B6%85%E7%BA%A7%E7%8E%9B%E4%B8%BD&zhida_source=entity">超级玛丽</a>找到宝藏则游戏结束，目标是让超级玛丽以最快的速度找到宝藏</li><li>假设游戏开始时，宝藏的位置一定是(1, 2)</li></ul><p><img src="https://pic2.zhimg.com/v2-9f91c909194ebcc7a62e0e2d2e2e8837_1440w.jpg"></p><p>这个一个标准的马尔科夫决策过程(MDP)：</p><ul><li><strong><a href="https://zhida.zhihu.com/search?content_id=5792339&content_type=Article&match_order=1&q=%E7%8A%B6%E6%80%81%E7%A9%BA%E9%97%B4&zhida_source=entity">状态空间</a>State</strong>：超级玛丽当前的坐标</li><li><strong>决策空间Action</strong>: 上、下、左、右四个动作</li><li>**Action对State的影响和回报 P(State’, Reward | State, Action)**：本文认为该关系是已知的<ul><li>超级玛丽每移动一步，reward = -1</li><li>超级玛丽得到宝箱，reward = 0并且游戏结束</li></ul></li></ul><p>结合上图可以非常简单的理解价值迭代：</p><ul><li><p><strong>初始化</strong>：所有state的价值V(s) = 0</p></li><li><p>第一轮迭代：对于每个state，逐一尝试上、下、左、右四个Action</p><ul><li>记录Action带来的Reward、以及新状态 V(s’)</li><li>选择最优的Action，更新V(s) = Reward + V(s’) = -1 + 0</li><li>第一轮结束后，所有状态都有V(s) = -1，即从当前位置出发走一步获得Reward=-1</li></ul></li><li><p>第二轮迭代：对于每个state，逐一尝试上、下、左、右四个Action</p><ul><li>记录Action带来的Reward、以及新状态 V(s’)</li><li>选择最优的Action，更新V(s) = Reward + V(s’)<ul><li>对于宝箱周围的State，最优的Action是一步到达宝箱，V(s) = Reward + V(s’) = -1 + 0</li><li>对于其他State，所有的Action都是一样的，V(s) = Reward + V(s’) = -1 + -1</li></ul></li><li>第二轮结束后，宝箱周围的State的价值保持不变 V(s) = -1，其他State的价值 V(s) = -2</li></ul></li><li><p>第三轮迭代：对于每个state，逐一尝试上、下、左、右四个Action</p><ul><li>记录Action带来的Reward、以及新状态 V(s’)</li><li>选择最优的Action，更新V(s) = Reward + V(s’)<ul><li>对于宝箱周围的State，最优的Action是一步到达宝箱，V(s) = Reward + V(s’) = -1 + 0</li><li>对于宝箱两步距离的State，最优的Action是先一步到达宝箱周边的State，V(s) = Reward + V(s’) = -1 + -1</li><li>对于宝箱三步距离的State，所有Action都是一样的，V(s) = Reward + V(s’) = -1 + -2</li></ul></li></ul></li><li><p>第四轮迭代：对于每个state，逐一尝试上、下、左、右四个Action</p><ul><li><p>记录Action带来的Reward、以及新状态 V(s’)</p></li><li><p>选择最优的Action，更新V(s) = Reward + V(s’)</p><ul><li>对于宝箱周围的State，最优的Action是一步到达宝箱，V(s) = Reward + V(s’) = -1 + 0</li><li>对于宝箱两步距离的State，最优的Action是先一步到达宝箱周边的State，V(s) = Reward + V(s’) = -1 + -1</li><li>对于宝箱三步距离的State，最优的Action是所有Action都是一样的，V(s) = Reward + V(s’) = -1 + -2</li></ul></li><li><p>在第四轮迭代中，所有V(s)更新前后都没有任何变化，价值迭代已经找到了最优策略。、</p><p><img src="https://picx.zhimg.com/v2-3308c12abd8f521cfd6b30d0eaaf0b81_1440w.jpg"></p></li></ul></li></ul><h2 id="策略迭代-Policy-Iteration"><a href="#策略迭代-Policy-Iteration" class="headerlink" title="策略迭代 | Policy Iteration"></a>策略迭代 | Policy Iteration</h2><p>策略迭代是另一种求解 BOE 的方法，核心思想是交替优化策略和状态值函数，直到收敛到最优策略 π∗。</p><p>与值迭代不同的是，策略迭代先初始化一个<strong>随机策略</strong>，再交替进行以下两个主要步骤：</p><ul><li>策略评估（PE）：$v_{\pi^{(k)}}=r_{\pi^{(k)}}+\gamma P_{\pi^{(k)}} v_{\pi^{(k)}}$</li><li>策略改进（PI）：$\pi^{(k+1)} = \arg\max <em>\pi\left(r_\pi+\gamma P_\pi v</em>{\pi^{(k)}}\right)$</li></ul><p>这个算法按照如下顺序</p><p>$$<br>{\pi_0}\xrightarrow{PE}{v_{\pi_0}}\xrightarrow{PI}\pi_1\xrightarrow{PE}{v_{\pi_1}}\xrightarrow{PI}\pi_2\xrightarrow{PE}v_{\pi_2}\xrightarrow{PI}\ldots<br>$$</p><p>PE=policy evaluation, PI=policy improvement</p><ol><li><p>初始化策略：随机初始化一个初始策略$\pi^{(0)}$。</p></li><li><p>策略评估：在当前策略$\pi^{(k)}$下，通过求解以下线性方程组来获得状态</p></li></ol><p>值函数$v_{\pi^{(k)}}(s):$</p><p>$$<br>v_{\pi^{(k)}}(s)=\sum_a\pi^{(k)}(a\mid s)\left[\sum_rp(r\mid s,a)r+\gamma\sum_{s^{\prime}}p(s^{\prime}\mid s,a)v_{\pi^{(k)}}(s^{\prime})\right]<br>$$</p><p>​    $\circ$当状态空间较小时，可以直接解线性方程组，</p><p>​    $\circ$对于较大的问题，可以使用 Bootstrap 迭代逼近的方法。这个算法过程如下：首先把所有的$V_0(s)$都初始化成0，然后根据V0计算V1，…，一直继续下去直到收敛。根据$V_k(s)$计算$V_{k+1}(s)$的公式如下：<br>$$<br>V_{k+1}(s)=\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)[r+\gamma V_k(s’)]<br>$$<br>我们可以看一下收敛的时候，Vk+1=Vk，Δ=0，那么上面的迭代公式正好就是贝尔曼方程！有了上面的迭代算法，任何给定的策略ππ，我们都可以计算出它的价值函数。</p><ol start="3"><li>策略改进：基于更新后的值函数$v_{\pi^{(k)}}$,贪心更新策略：</li></ol><p>$$<br>\pi^{(k+1)}(s)=\arg\max_a\left[\sum_rp(r\mid s,a)r+\gamma\sum_{s’}p(s’\mid s,a)v_{\pi^{(k)}}(s’)\right]<br>$$</p><ol start="4"><li><p>检查收敛：如果策略不再变化，即$\pi^{(k+1)}=\pi^{(k)}$,则算法结束，$\pi^{(k)}$<br>即为最优策略；否则返回步骤 2。</p><p><img src="C:\Users\24376\AppData\Roaming\Typora\typora-user-images\image-20250115181005916.png" alt="image-20250115181005916"></p></li></ol><p><img src="C:\Users\24376\AppData\Roaming\Typora\typora-user-images\image-20250115181032097.png" alt="image-20250115181032097"></p><h2 id="策略提升定理"><a href="#策略提升定理" class="headerlink" title="策略提升定理"></a>策略提升定理</h2><p>假设$\pi$和$\pi^\prime$是两个确定的策略(一个状态s下只有一个行为a的概率是1，其余是0),如果对于所有的<br>$s\in\mathcal{S}_\text{都有：}$</p><p>$$<br>q_\pi(s,\pi^{\prime}(s))\geq v_\pi(s)<br>$$</p><p>那么策略$\pi^{\prime}$比$\pi$要“好”,也就是对于所有的$s\in\mathcal{S}:$</p><p>$$v_{\pi^{\prime}}(s)\geq v_\pi(s)$$</p><p>并且如果第一个不等式是严格大于，那么第二个不等式也是严格大于，也就是$\pi^{\prime}$一定比$\pi$更好(而不<br>是一样)。</p><p>如果这个定理成立，那就证明了我们前面结论，因为我们新的策略在s的时候采取的满足<br>$q_\pi(s,a)\geq v_\pi(s)$,而其它的状态s’都是一样的，因此新的策略会更好。当然上面的新策略是随便选择一个满足条件的行为a，但更好的办法是从所有可能的a里选择$q_\pi(s,a)$最大的那个a，此外我们可以改变所有s的策略，而不是一个s，这个改进版本就是我们最终用到的策略提升算法。如果所有的状态，当前的a已经是$q_\pi(s,a)$中最大的那个呢？那说明当前策略已经是最优的策略了。下面我们来证明这个定理：<br>$$<br>\begin{aligned}&amp;v_{\pi}(s)\leq q_{\pi}(s,\pi^{\prime}(s))\&amp;\text{t时刻使用策略}\pi^{\prime},\text{t+1时刻之后还是用策略}\pi\&amp;=\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s]\&amp;\text{用前面的假设}\&amp;\leq\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma q_{\pi}(S_{t+1},\pi^{\prime}(S_{t+1}))|S_{t}=s]\&amp;=\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+2}+\gamma v_{\pi}(S_{t+2})]|S_{t}=s]\&amp;=\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma R_{t+2}+\gamma^{2}v_{\pi}(S_{t+2})|S_{t}=s]\&amp;\leq\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\gamma^{3}v_{\pi}(S_{t+3})|S_{t}=s]\&amp;\ldots\&amp;\leq\mathbb{E}<em>{\pi^{\prime}}[R</em>{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\gamma^{3}R_{t+4}\ldots+|S_{t}=s]\&amp;=v_{\pi^{\prime}}(s)\end{aligned}<br>$$</p><p>上面的证明就是不停的应用假设条件，里面有一点就是$E_\pi E_\pi X=E_\pi X$,因为求一次期望之后就和</p><p>$\pi$无关了，可以认为是常量了。</p><p>我们最终使用的策略是“贪心”的策略，对于所有的s，我们都选择qπ(s,a)最大的那个a。</p><table><thead><tr><th><strong>算法</strong></th><th><strong>优点</strong></th><th><strong>缺点</strong></th><th><strong>适用场景</strong></th></tr></thead><tbody><tr><td>值迭代</td><td>通过单个过程即可直接逼近最优值函数，简单、直接，适用于小规模问题</td><td>收敛速度较慢，尤其是当状态空间较大或 γ 接近 1 时，每次迭代的更新量可能变得很小</td><td>状态空间较小、对精度要求较高</td></tr><tr><td>策略迭代</td><td>收敛速度快，通常只需几轮策略更新即可收敛</td><td>每轮策略评估计算量大，尤其是在状态空间很大时，可能成为瓶颈</td><td>中小规模问题、对速度要求较高</td></tr><tr><td>截断策略迭代</td><td>权衡了值迭代和策略迭代的优缺点，减少了策略评估的计算开销，相较于策略迭代更高效</td><td>收敛速度可能略低于完整的策略迭代</td><td>大规模问题、需要减少计算开销</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;值迭代-Value-Iteration&quot;&gt;&lt;a href=&quot;#值迭代-Value-Iteration&quot; class=&quot;headerlink&quot; title=&quot;值迭代 | Value Iteration&quot;&gt;&lt;/a&gt;值迭代 | Value Iteration&lt;/h2&gt;&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://www.jkzhang.ml/2025/02/22/#%20%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/"/>
    <id>https://www.jkzhang.ml/2025/02/22/#%20%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/</id>
    <published>2025-02-22T08:34:25.475Z</published>
    <updated>2025-01-15T13:36:32.350Z</updated>
    
    <content type="html"><![CDATA[<h1 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h1><p>在一个马尔可夫奖励过程中，从第t时刻状态开始，直到终止状态T时，所有奖励的衰减之和称为<strong>回报</strong>（Return），公式如下：<br>$$<br>G_{t}=r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\gamma^{3} r_{t+4}+\ldots+\gamma^{T-t-1} r_{T}<br>$$<br>其中，$T$是最终时刻，$\gamma$ 是折扣因子，越往后得到的奖励，折扣越多。关于折扣因子的必要性，以前的笔记已经介绍过。</p><p>如果提取一个$\gamma$，还可以得到一个递推式：<br>$$<br>G_{t}=r_{t+1}+\gamma G_{t+1}<br>$$<br>这个意思是，此时的计算只需要关注这一步的奖励，加折扣因子乘以下一步的回报即可。这个式子推导贝尔曼方程时极其有用。</p><p>接下来定义价值函数：<br>$$<br>\begin{aligned}<br>    V_{\pi}(s) &amp;=\mathbb{E}\left[G_{t} \mid s_{t}=s\right] \<br>    &amp;=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\ldots+\gamma^{T-t-1} r_{T} \mid s_{t}=s\right]<br>\end{aligned}<br>$$<br>其中，$G_t$ 是之前定义的<strong>折扣回报（discounted return）</strong>。我们对$G_t$取了一个期望，期望就是从这个状态开始，我们可能获得多大的价值。所以期望也可以看成未来可能获得奖励的当前价值的表现，就是当我们进入某一个状态后，我们现在有多大的价值。</p><p>可以看到价值函数有一个下标$\pi$，这是因为价值函数除了与s有关，也与策略$\pi$有关，不同的策略价值函数自然也不一样。并且价值函数越大，说明这个策略越好。因此价值函数可以评估策略的优劣。</p><p>我们利用价值函数表示出<strong>贝尔曼方程（Bellman equation）</strong><br>$$<br>V(s)=\underbrace{R(s)}<em>{\text {即时奖励}}+\underbrace{\gamma \sum</em>{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)}_{\text {未来奖励的折扣总和}}<br>$$<br>其中，</p><ul><li>$s’$ 可以看成未来的所有状态，</li><li>$p(s’|s)$  是指从当前状态转移到未来状态的概率。</li><li>$V(s’)$ 代表的是未来某一个状态的价值。我们从当前状态开始，有一定的概率去到未来的所有状态，所以我们要把 $p\left(s^{\prime} \mid s\right)$ 写上去。我们得到了未来状态后，乘一个 $\gamma$，这样就可以把未来的奖励打折扣。</li><li>$\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)$ 可以看成未来奖励的折扣总和（discounted sum of future reward）。</li></ul><p>贝尔曼方程的推导过程如下：</p><p>由于$V_{\pi}(s) =\mathbb{E}\left[G_{t} \mid s_{t}=s\right]$，再把$G_{t}=r_{t+1}+\gamma G_{t+1}$代入，其中<br>$$<br>\begin{aligned}\mathbb{E}[G_{t+1}|S_{t}=s]&amp;\begin{aligned} =\sum_{s^{\prime}\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_t=s,S_{t+1}=s^{\prime}]p(s^{\prime}|s)\end{aligned}\&amp;\begin{aligned}&amp;=\sum_{s^{\prime}\in\mathcal{S}}\mathbb{E}[G_{t+1}|S_{t+1}=s^{\prime}]p(s^{\prime}|s)&amp;&amp;\text{(due to the Markov property)}\end{aligned}\&amp;\begin{aligned}=\sum_{s^{\prime}\in\mathcal{S}}v_\pi(s^{\prime})p(s^{\prime}|s)\end{aligned}\&amp;\begin{aligned}&amp;=\sum_{s^{\prime}\in\mathcal{S}}v_{\pi}(s^{\prime})\sum_{a\in\mathcal{A}}p(s^{\prime}|s,a)\pi(a|s).\end{aligned}\end{aligned}<br>$$</p><p>接下来我们可以求期望了：</p><p>$$<br>\begin{aligned}<br>    V(s)&amp;=\mathbb{E}\left[G_{t} \mid s_{t}=s\right]\<br>    &amp;=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\ldots \mid s_{t}=s\right]  \</p><pre><code>&amp;=R(s)+\gamma \mathbb&#123;E&#125;[G_&#123;t+1&#125;|s_t=s] \\&amp;=R(s)+\gamma \sum_&#123;s^&#123;\prime&#125; \in S&#125; p\left(s^&#123;\prime&#125; \mid s\right) V\left(s^&#123;\prime&#125;\right)\end&#123;aligned&#125;</code></pre><p>$$<br>再进一步探究：<br>$$<br>\begin{aligned}v_{\pi}(s)&amp;=\mathbb{E}[R_{t+1}|S_{t}=s]+\gamma\mathbb{E}[G_{t+1}|S_{t}=s],\&amp;=\underbrace{\sum_a\pi(a|s)\sum_rp(r|s,a)r}<em>{\text{mean of immediate rewards}}+\underbrace{\gamma\sum_a\pi(a|s)\sum_{s^{\prime}}p(s^{\prime}|s,a)v_\pi(s^{\prime})}_{\text{mean of future rewards}},\&amp;=\sum</em>{a}\underbrace{\pi(a|s)}\left[\sum_{r}\underbrace{p(r|s,a)r}<em>{}+\gamma\sum</em>{s^{\prime}}\underbrace{p(s^{\prime}|s,a)}<em>{}\underbrace{v</em>{\pi}(s^{\prime})}_{}\right],\quad\forall s\in\mathcal{S}.\end{aligned}<br>$$<br>得到了关于$\pi$的表达式，这就说明了策略$\pi$与状态函数的关系，这为policy evaluation提供了工具！</p><p>我们可以把贝尔曼方程写成矩阵的形式：<br>$$<br>  \left(\begin{array}{c}<br>    V\left(s_{1}\right) \<br>    V\left(s_{2}\right) \<br>    \vdots \<br>    V\left(s_{N}\right)<br>    \end{array}\right)=\left(\begin{array}{c}<br>    R\left(s_{1}\right) \<br>    R\left(s_{2}\right) \<br>    \vdots \<br>    R\left(s_{N}\right)<br>    \end{array}\right)+\gamma\left(\begin{array}{cccc}<br>    p\left(s_{1} \mid s_{1}\right) &amp; p\left(s_{2} \mid s_{1}\right) &amp; \ldots &amp; p\left(s_{N} \mid s_{1}\right) \<br>    p\left(s_{1} \mid s_{2}\right) &amp; p\left(s_{2} \mid s_{2}\right) &amp; \ldots &amp; p\left(s_{N} \mid s_{2}\right) \<br>    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>    p\left(s_{1} \mid s_{N}\right) &amp; p\left(s_{2} \mid s_{N}\right) &amp; \ldots &amp; p\left(s_{N} \mid s_{N}\right)<br>    \end{array}\right)\left(\begin{array}{c}<br>    V\left(s_{1}\right) \<br>    V\left(s_{2}\right) \<br>    \vdots \<br>    V\left(s_{N}\right)<br>    \end{array}\right)<br>$$</p><p>我们当前的状态是向量$[V(s_1),V(s_2),\cdots,V(s_N)]^\mathrm{T}$。每一行来看，向量$\boldsymbol{V}$乘状态转移矩阵里面的某一行，再加上它当前可以得到的奖励，就会得到它当前的价值。</p><p>当我们把贝尔曼方程写成矩阵形式后，可以直接求解：<br>$$<br>\begin{aligned}<br>    \boldsymbol{V} &amp;= \boldsymbol{\boldsymbol{R}}+ \gamma \boldsymbol{P}\boldsymbol{V} \<br>    \boldsymbol{I}\boldsymbol{V} &amp;= \boldsymbol{R}+ \gamma \boldsymbol{P}\boldsymbol{V} \<br>    (\boldsymbol{I}-\gamma \boldsymbol{P})\boldsymbol{V}&amp;=\boldsymbol{R} \<br>    \boldsymbol{V}&amp;=(\boldsymbol{I}-\gamma \boldsymbol{P})^{-1}\boldsymbol{R}<br>    \end{aligned}<br>$$</p><p>我们可以直接得到<strong>解析解（analytic solution）</strong>：<br>$$<br>  \boldsymbol{V}=(\boldsymbol{I}-\gamma \boldsymbol{P})^{-1} \boldsymbol{R}<br>$$</p><p>我们可以通过矩阵求逆把 $\boldsymbol{V}$ 的价值直接求出来。但是一个问题是这个矩阵求逆的过程的复杂度是 $O(N^3)$。所以当状态非常多的时候，比如从10个状态到1000个状态，或者到100万个状态，当我们有100万个状态的时候，状态转移矩阵就会是一个100万乘100万的矩阵，对这样一个大矩阵求逆是非常困难的。所以这种通过解析解去求解的方法只适用于很小量的马尔可夫奖励过程。</p><p><img src="C:\Users\24376\AppData\Roaming\Typora\typora-user-images\image-20250115181203944.png" alt="image-20250115181203944"></p><p>这个解法利用了<strong>不动点思想：</strong>假设代数方程</p><p><img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/ad27e68aee3a027d0a5bace0c55dd79250239c47" alt="{\displaystyle f(x)=0}"></p><p>可以等价写成</p><p><img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/c1944e08d22cd6b68dcb809cdc1804d0df3b5458" alt="{\displaystyle x=\varphi (x).}"></p><p>我们称满足<img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/b598d12079d102bda91773a160b7c99540c36d9c" alt="{\displaystyle x^{*}=\varphi (x^{*})}">的<img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/0c3d92911742787872be53f047783d16d9737ea0" alt="{\displaystyle x^{*}}">为<img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/87792d8065e427aa737caaf609bbdf9da8cd6a3e" alt="{\displaystyle \varphi (x)}">的<strong>不动点</strong>，设计如下迭代格式</p><p><img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/1300b775de9924f6d1d0cdda71d12b8d29ec7541" alt="{\displaystyle x_{k+1}=\varphi (x_{k}),\quad k=0,1,\cdots .}"></p><p>如果该格式收敛，那么它可用于求解<img src="https://services.fandom.com/mathoid-facade/v1/media/math/render/svg/ad27e68aee3a027d0a5bace0c55dd79250239c47" alt="{\displaystyle f(x)=0}">的问题，这种方法就是不动点迭代法。 至于这个迭代格式是否收敛，则需要利用不动点定理作进一步分析。</p><p><img src="C:\Users\24376\AppData\Roaming\Typora\typora-user-images\image-20250115181634889.png" alt="image-20250115181634889"></p><p>对于MDP，我们在第一节里已经讲到了它的价值函数$v_\pi(s)$的表达式。但是这个表达式没有考虑到所采用的动作<em>a</em>带来的价值影响，因此我们除了这个$v_\pi(s)$状态价值函数外，还有一个动作价值函数$q_\pi(s,a)$，即：<br>$$<br>q_{\pi}(s, a)=\mathbb{E}<em>{\pi}\left[G</em>{t} \mid s_{t}=s, a_{t}=a\right] \tag{2.4}<br>$$<br>使用$q_\pi(s)$可以表示状态价值函数，即：<br>$$<br>v_{\pi}(s)=\sum_{a \in A} \pi(a \mid s)q_{\pi}(s, a)<br>\tag{2.5}<br>$$<br>另外，我们给出 Q 函数的贝尔曼方程</p><p>$$<br>Q(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V\left(s^{\prime}\right) \tag{2.21}<br>$$<br>根据和的关系，我们可以得到贝尔曼最优方程（Bellman optimality equation）：<br>$$<br>V^*(s)=\max_{a\in\mathcal{A}}{r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)V^*(s^{\prime})}<br>$$</p><p>$$<br>Q^*(s,a)=r(s,a)+\gamma\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}|s,a)\max_{a^{\prime}\in\mathcal{A}}Q^*(s^{\prime},a^{\prime})<br>$$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;贝尔曼方程&quot;&gt;&lt;a href=&quot;#贝尔曼方程&quot; class=&quot;headerlink&quot; title=&quot;贝尔曼方程&quot;&gt;&lt;/a&gt;贝尔曼方程&lt;/h1&gt;&lt;p&gt;在一个马尔可夫奖励过程中，从第t时刻状态开始，直到终止状态T时，所有奖励的衰减之和称为&lt;strong&gt;回报&lt;/str</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Attention is all I need：Transformer的原理和代码详解</title>
    <link href="https://www.jkzhang.ml/2025/02/20/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/"/>
    <id>https://www.jkzhang.ml/2025/02/20/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/</id>
    <published>2025-02-20T09:00:00.000Z</published>
    <updated>2025-02-22T08:38:37.553Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Transformer可运行的代码发布在<a href="https://github.com/JinHanLei/Transformers_tutorial">GitHub</a><br>{: .prompt-tip }</p></blockquote><p>提到ChatGPT的原理，就绕不开Transformer，Transformer中的核心思想之一便是<strong>Attention</strong>，Attention机制彻底击败了在此之前的绝对王者RNN模式，并统治各大NLP任务直到现在。正因如此，Transformer的论文不叫Transformer，而是叫做<a href="https://arxiv.org/abs/1706.03762">《Attention is all you need》</a>。本文是以我的理解，阐述Transformer是怎么想出来的，为什么这么设计。</p><h2 id="Attention的思想"><a href="#Attention的思想" class="headerlink" title="Attention的思想"></a>Attention的思想</h2><p>Attention的关键在于理解$$QKV$$，即Query、Key和Value。可以将Attention机制看作一种寻址操作：存储器中存有键Key和值Value，当前产生了一个Query的查询，要查询出Value，那么首先需要匹配Query和Key的相似度。举个也许不恰当，但直观的例子，有以下Key和Value：</p><table><thead><tr><th align="center">Key</th><th align="center">Value</th></tr></thead><tbody><tr><td align="center">段誉的招牌武功</td><td align="center">六脉神剑</td></tr><tr><td align="center">段誉的生父</td><td align="center">段延庆</td></tr><tr><td align="center">段誉的结拜兄弟</td><td align="center">乔峰和虚竹</td></tr><tr><td align="center">乔峰的招牌武功</td><td align="center">降龙十八章</td></tr></tbody></table><p>寻址流程如下：</p><ol><li>发起Query：“段誉的生父是谁？”</li><li>与Key相似度匹配到“段誉的生父”</li><li>返回Value“段延庆”</li></ol><p>这里的关键是相似度计算方法，通常是Query和Key的矩阵乘法，或加个缩放$$\sqrt{d_k}$$，或乘个$$W$$，如下：</p><ul><li>矩阵相乘：$$sim(Q,K)=QK^T$$ </li><li>相乘加缩放：$$sim(Q,K)=\frac{QK^T}{\sqrt{d_k}}$$（Transformer使用，缩放使得训练可以收敛）</li><li>权重+激活：$$sim(Q,K)=tanh(WQ+UK)$$</li><li>权重+相乘：$$sim(Q,K)=QWK^T$$</li></ul><p>取出一个或者部分Value的方法叫Hard Attention，如上例只输出“段延庆”。但是，如果我问“段誉结拜兄弟的招牌武功是什么？”，Hard Attention可能匹配到“段誉的结拜兄弟”，输出“乔峰和虚竹”，这就不对了。替代方案就是Soft Attention，提供所有Value和对应的Attention权重，当$$Q=$$“段誉结拜兄弟的招牌武功是什么？”，结果可能如下表：</p><table><thead><tr><th align="center">Key</th><th align="center">Attention权重</th><th align="center">Value</th></tr></thead><tbody><tr><td align="center">段誉的招牌武功</td><td align="center">7</td><td align="center">六脉神剑</td></tr><tr><td align="center">段誉的生父</td><td align="center">1</td><td align="center">段延庆</td></tr><tr><td align="center">段誉的结拜兄弟</td><td align="center">9</td><td align="center">乔峰和虚竹</td></tr><tr><td align="center">乔峰的招牌武功</td><td align="center">5</td><td align="center">降龙十八章</td></tr></tbody></table><p>这样，就可以把高分答案结合，得到正确答案。当问题更宏大，需要的信息就更多，于是干脆每次都输出整张表，虽然有可能冗余，但这样得到的答案是既有重点、又完整的。</p><p>以往设备限制导致计算和输出全部非常困难，但现在设备的发展使得超大规模、超长文本输入的LLM得以出现，而Transformer的self-attention保证了LLM的效率和学习能力。</p><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>Self-Attention说来很简单，就是$$Q=K=V$$。</p><p>为什么要这么做？个人理解是让一句话先找出自己内部的关键词，再去适配下游的任务。例如$$Q=K=V=青花瓷$$，用Pytorch简单计算如下：</p><pre><code class="python">from torch import nn as nnimport torch# &#123;青:0, 花:1, 瓷:2&#125;tokens = torch.LongTensor([0, 1, 2])# 将3个字转换成向量，向量维度为10token_embedding = nn.Embedding(3, 10)emb = token_embedding(tokens)# 相似度计算QK = torch.mm(emb, emb.T) / torch.sqrt(torch.FloatTensor([10]))print(QK)</code></pre><p>由于nn.Embedding随机初始化，所以结果会不一样，我的结果表述如下：</p><p>$$<br>sim(Q,K)=<br>\begin{bmatrix}<br> 青 &amp; 花 &amp; 瓷<br>\end{bmatrix}<br>\times<br>\begin{bmatrix}<br> 青\<br> 花\<br> 瓷<br>\end{bmatrix}<br>/10<br>=<br>\begin{bmatrix}<br> 3.2897 &amp; 0.7432 &amp; -1.1652 \<br> 0.7432 &amp; 1.3647 &amp; -1.1707\<br> -1.1652 &amp; -1.1707 &amp; 5.6380<br>\end{bmatrix}<br>$$</p><p>矩阵对角线表示自身的相似度，比如3.2897就表示“青”和“青”的相似度，就很大。每行代表每个字的权重。由于点积可以产生任意大的数字，这会破坏训练过程的稳定性，因此需要 $$Softmax$$。Attention的公式表示为：</p><p>$$<br>\text{Attention}(Q, K, V) = \text{Softmax} \big( \frac{QK^T}{\sqrt{d_k}} \big)V<br>$$</p><p>代码只须再加上：</p><pre><code class="python">Attention = torch.mm(torch.softmax(QK, dim=-1), emb)</code></pre><p>这样得到矩阵的每行就表示[青, 花, 瓷]这三个字的Attention。在训练过程中会更新这些参数，从而根据上下文和标签得到更好的向量表示。</p><h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h3><p>为了关注到更多信息，Transformer采用Multi-head Attention机制，也就是重复n次Attention操作然后拼接，得到和原来的Attention维度相同的MultiHead，公式为：</p><p>$$<br>\begin{gather}head_i = \text{Attention}(\boldsymbol{Q}\boldsymbol{W}_i^Q,\boldsymbol{K}\boldsymbol{W}_i^K,\boldsymbol{V}\boldsymbol{W}_i^V)\\text{MultiHead}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = \text{Concat}(head_1,…,head_h)\boldsymbol{W}^O\end{gather}<br>$$</p><p>其中 $$\boldsymbol{W}<em>i^Q\in\mathbb{R}^{d</em>{model}\times d_k}, \boldsymbol{W}<em>i^K\in\mathbb{R}^{d</em>{model}\times d_k}, \boldsymbol{W}<em>i^V\in\mathbb{R}^{d</em>{model}\times d_v},\boldsymbol{W}^O\in\mathbb{R}^{hd_{v}\times d_{model}}$$ 。</p><p>原文模型的维度$$d_{model}$$是512，我们设置$$h=8$$个注意力头，那么$$d_k=d_v=d_{model}/h=64$$。每个注意力头负责关注某一方面的语义相似性，多个头就可以让模型同时关注多个方面。不怎么严谨的代码如下，便于理解：</p><pre><code class="python">from torch import nnimport torch.nn.functional as Ffrom math import sqrtimport torchclass AttentionHead(nn.Module):    def __init__(self, embed_dim, head_dim):        super().__init__()        self.WQ = nn.Linear(embed_dim, head_dim)        self.WK = nn.Linear(embed_dim, head_dim)        self.WV = nn.Linear(embed_dim, head_dim)    def forward(self, query, key, value):        QK = torch.mm(WQ(query), WK(key).T) / torch.sqrt(query.size(-1))        Attention = torch.mm(torch.softmax(QK, dim=-1), WV(value))        return Attention    class MultiHeadAttention(nn.Module):    def __init__(self, config):        super().__init__()        embed_dim = config.hidden_size        num_heads = config.num_attention_heads        head_dim = embed_dim // num_heads        self.heads = nn.ModuleList(            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]        )        self.output_linear = nn.Linear(embed_dim, embed_dim)    def forward(self, query, key, value):        MultiHead = torch.cat([h(query, key, value) for h in self.heads], dim=-1)        x = self.output_linear(MultiHead)        return x</code></pre><p>更多时候，为了并行效率，多头操作是先乘上$$\boldsymbol{W}\in\mathbb{R}^{d_{model}\times d_{model}}$$的权重矩阵，再将QKV切块相乘，同一个结果但是抛弃了<code>for</code>循环，<a href="https://github.com/JinHanLei/Transformers_tutorial">我的仓库</a>中就是这种做法。Transformer最核心的就是上文所述的Attention，下面介绍其他部分。</p><h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>之前的博客介绍了Encoder-Decoder结构，Transformer也遵从这种结构。而Transformer“浑身都是宝”，每个部分都被开发出了作用：</p><ul><li><strong>Transformer的Encoder</strong>（如<a href="https://arxiv.org/abs/1810.04805">BERT</a>），又称自编码 (auto-encoding) Transformer 模型</li><li><strong>Transformer的Decoder</strong>（如<a href="https://openai.com/blog/language-unsupervised/">GPT系列</a>），又称自回归 (auto-regressive) Transformer 模型</li><li><strong>完整的Encoder-Decoder</strong>（例如<a href="https://arxiv.org/pdf/1910.13461">BART</a>、<a href="https://arxiv.org/pdf/1910.10683.pdf">T5</a>等）</li></ul><p>理解了Transformer，以上模型的上手难度会小很多，我们之后再了解。Transformer整体结构如图：</p><p><img src="/imgs/transformer.jpeg" alt="transformer"></p><p>左边是Encoder，右边是Decoder，可以看到两边都有：</p><ul><li>Positional Encoding</li><li>Multi-head Attention</li><li>Feed Forward</li><li>Add &amp; Norm。</li></ul><p>Multi-head Attention已经在上文介绍了，介绍下其他几位。</p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>在Attention中其实可以看出，并没有任何有关位置的特征，这样”一步两步三步四步望着天“的每一个”步“向量都是一样的，甚至把这句话变成”天着望步四步三步两步”一“，都是一样的，这显然不合理。因为每一步情绪都是递进的，而Attention无法解决前后顺序和句子内的一词多义。</p><p>RNN通过记忆使得每一步不一样，而Transformer采用了<em>Positional Encoding</em>，即位置编码，其公式如下：</p><p>$$<br>\begin{gather}<br>PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\<br>PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})<br>\end{gather}<br>$$</p><p>意思就是向量的偶数位置填$$sin$$，奇数位置填$$cos$$，$$pos$$指相对位置，如”青花瓷“的”青“的$$pos$$就是0。对”青花瓷“这三个10维的向量进行位置编码，简单实现代码如下：</p><pre><code class="python">import mathimport torchseq_len = 3d_model = 10pe = torch.zeros(seq_len, d_model)position = torch.arange(0, seq_len).unsqueeze(1)div_term = torch.exp(torch.arange(0, d_model, 2) *                     -math.log(10000.0) / d_model)pe[:, 0::2] = torch.sin(position * div_term)pe[:, 1::2] = torch.cos(position * div_term)print(pe)</code></pre><p>由于次方比较难算，利用$e^{lnx}=x$的性质，代码中进行了如下转换：</p><p>$$<br>\frac{1}{10000^{2i/d}}<br>=e^{ln\frac{1}{10000^{2i/d}}}<br>=e^{-\frac{2i}{d}ln10000}<br>$$</p><p>得到的PE矩阵用来加上原始的词向量。位置编码相当于根据位置给权重。为什么这么做？我们从0开始思考，让我设计位置编码，怎么设计？</p><p>$$<br>一步两步三步四步望着天:[0,1,2,3,4,5,6,7,8,9,10]<br>$$</p><p>问题在哪？跟embedding向量的数字相比，这个数字太大了，那归一化一下：</p><p>$$<br>一步两步三步四步望着天:[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]<br>$$</p><p>这样的问题在于无法体现位置关系，因为注意力是相乘，下面采取相乘：$0.1 \times 1=0.2 \times 0.5$，不同距离相乘居然一样，反之，相同距离相乘居然不一样。大概有头绪了，位置编码起码需要满足这些条件：SSS</p><ol><li>相同距离相乘一样；</li><li>不同距离相乘不一样；</li><li>数量不限：序列无论多长都能找到相应的位置编码表示。</li></ol><p>Transformer本身是加入额外的位置，词向量加上位置的<strong>绝对位置编码</strong>。另外，还有修改Attention结构的<strong>相对位置编码</strong>。而下面介绍苏神的<a href="https://arxiv.org/abs/2104.09864">Rope</a>结合了两者。</p><h4 id="Rope位置编码"><a href="#Rope位置编码" class="headerlink" title="Rope位置编码"></a>Rope位置编码</h4><p>根据Transformer的位置编码公式，$pos+k$位置的编码如下：</p><p>$$<br>\begin{gather}<br>PE_{(pos+k,2i)}=sin((pos+k)/10000^{2i/d_{model}})\<br>PE_{(pos+k,2i+1)}=cos((pos+k)/10000^{2i/d_{model}})<br>\end{gather}<br>$$</p><p>先令$w_i=1/10000^{2i/d_{model}}$，根据：</p><p>$$<br>\begin{gather}<br>sin(\alpha+\beta)=sin\alpha \cdot cos\beta+cos\alpha \cdot sin\beta\<br>cos(\alpha+\beta)=cos\alpha \cdot cos\beta-sin\alpha \cdot sin\beta<br>\end{gather}<br>$$</p><p>推导出：</p><p>$$<br>\begin{gather}<br>PE_{(pos+k,2i)}=sin(w_i(pos+k))=sin(w_ipos) \cdot cos(w_ik)+cos(w_ipos) \cdot sin(w_ik)\<br>PE_{(pos+k,2i+1)}=cos(w_i(pos+k))=cos(w_ipos) \cdot cos(w_ik)-sin(w_ipos) \cdot sin(w_ik)<br>\end{gather}<br>$$</p><p>就是多了$k$的部分，可以表示为矩阵：</p><p>$$<br>\begin{bmatrix}<br> PE_{(pos+k,2i)} \<br> PE_{(pos+k,2i+1)}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br> cos(w_ik) &amp; sin(w_ik) \<br> -sin(w_ik) &amp; cos(w_ik)<br>\end{bmatrix}<br>\times<br>\begin{bmatrix}<br> PE_{(pos,2i)} \<br> PE_{(pos,2i+1)}<br>\end{bmatrix}<br>$$</p><p>令:</p><p>$$<br>R_k=<br>\begin{bmatrix}<br> cos(w_ik) &amp; sin(w_ik) \<br> -sin(w_ik) &amp; cos(w_ik)<br>\end{bmatrix}^T<br>$$</p><p>根据：</p><p>$$<br>\begin{gather}<br>-sinx=sin-x \<br>cosx=cos-x<br>\end{gather}<br>$$</p><p>易得：</p><p>$$<br>R_k = R_{-k}^T<br>$$</p><p>并有如下性质，从而可以表示相对位置：</p><p>$$<br>R_{k_2-k_1}=R_{k_1}^TR_{k_2}<br>$$</p><p>对位置为$m$的词向量$A$和位置为$n$的$B$，对他们乘上$R_m$和$R_n$，就给Attention加上了绝对位置信息，并且具有$m-n$的相对位置信息：</p><p>$$<br>AR_m(BR_n)^T=AR_mR_n^TB=AR_{n-m}B<br>$$</p><p>Rope被许多大模型如<a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">LLaMA</a>、<a href="https://huggingface.co/tiiuae/falcon-40b-instruct">falcon</a>等采用。以往较多模型采用直接embedding+学习的方式，但是这样最开始就定死了长度，遇到长文本只能截断，而Rope改良了这一点，使得大模型具有处理超长文本的能力。更多位置编码方式，可以参考苏神的博客<a href="https://kexue.fm/archives/8130">《让研究人员绞尽脑汁的Transformer位置编码》</a>。</p><h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h3><p>Feed Forward简称FFN，在Encoder和Decoder的压轴处都各有一层，由两个全连接和<a href="https://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">ReLU</a>激活函数组成，如下式：</p><p>$$<br>FFN(X)=max(0,xW_1+b_1)W_2+b_2<br>$$</p><p>这个$$max$$就是ReLU。各个激活函数如图：</p><p><img src="/imgs/Activation_Functions.gif" alt="Activation"></p><p>ReLU的导数只有0和1，使得计算成本很低。</p><p>两层全连接把模型维度从512扩展到了2048又回到512。我找遍了原文和各个教程，都没有详细解释这一步的作用。</p><p>个人理解可能是扩展到更高维度以储存更多信息，但之前这么多参数还不够？</p><p>也可能是加个激活函数，但是之前也有Softmax。</p><p>有加入类似层进行调参的工作：<a href="https://arxiv.org/abs/1902.00751">adapter</a>，但其真实设计意图是什么，也不得而知了。</p><h3 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add &amp; Norm"></a>Add &amp; Norm</h3><p>Add &amp; Norm由Add和Norm两部分组成。</p><p>Add是将箭头指过来的两者相加，包括Attention和原embedding这两个矩阵相加、以及过了FFN和没过之前的矩阵相加，这个很简单，就不细说了。</p><p>Norm指标准化，Transformer中使用<a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">LayerNorm</a>，在图像领域常使用BatchNorm，两者都是拿均值方差做标准化处理，都是下式：</p><p>$$<br>y=\frac{x-E[x]}{\sqrt{Var[x]+\epsilon }}<br>$$</p><p>其中$$E[x]$$为均值，$$Var[x]$$为方差，$$\epsilon$$是一个很小的常数，用于避免分母为零。区别就在于两者拿来算均值方差的数不一样，如下：</p><ul><li>LayerNorm的均值方差，是对单个样本的不同特征做操作，即每个词向量内部；</li><li>BatchNorm是对不同样本的同一特征做操作。</li></ul><p>拿“青花瓷”的相似矩阵为例，手推LayerNorm：</p><pre><code class="python">import torch.nn as nnimport mathmatrix = torch.Tensor([[3.2897, 0.7432, -1.1652],                       [0.7432, 1.3647, -1.1707],                       [-1.1652, -1.1707, 5.6380]])# torch官方的layer_normlayer_norm = nn.LayerNorm(matrix.shape[-1])y_torch = layer_norm(matrix)print(y_torch)# 初始化一个与matrix大小相同的全0矩阵y_ours = torch.zeros_like(matrix)# 手推均值方差和LayerNormfor row in range(matrix.shape[0]):    E = sum(matrix[row]) / len(matrix[row])    Var = 0    for col in range(matrix.shape[1]):        Var += (matrix[row, col] - E) ** 2 / len(matrix[row])    for col in range(matrix.shape[1]):        y_ours[row, col] = (matrix[row, col] - E) / math.sqrt(Var + 1e-5)print(y_ours)</code></pre><p>求得的结果是一样的。可以看到结果中，有小于0也有大于1的，因此我认为有些教程称之为“归一化”是不合理的，归一化是通过MinMax将所有数据转换至0-1范围内。而LayerNorm，明显是均值0方差1的<strong>标准化</strong>。</p><p>BatchNorm的代码如下：</p><pre><code class="python">import torch.nn as nnimport mathmatrix = torch.Tensor([[3.2897, 0.7432, -1.1652],                       [0.7432, 1.3647, -1.1707],                       [-1.1652, -1.1707, 5.6380]])layer_norm = nn.BatchNorm1d(matrix.shape[-1])y_batch = layer_norm(matrix)print(y_batch)y_ours = torch.zeros_like(matrix)# 手动求均值方差和normfor col in range(matrix.shape[1]):    E = sum(matrix[:, col]) / len(matrix[:, col])    Var = 0    for row in range(matrix.shape[0]):        Var += (matrix[row, col] - E) ** 2 / len(matrix[row])    for row in range(matrix.shape[1]):        y_ours[row, col] = (matrix[row, col] - E) / math.sqrt(Var + 1e-5)print(y_ours)</code></pre><p>差别就在于LayerNorm在行，BatchNorm在列。</p><p>Transformer 为什么使用 Layer？这个问题还没有啥定论，包括LN和BN为啥能work也众说纷纭，感兴趣的话可以参考<a href="https://arxiv.org/abs/1607.06450">原文</a>和以下论文：</p><ul><li>PowerNorm: Rethinking Batch Normalization in Transformers <a href="https://arxiv.org/abs/2003.07845">[1]</a></li><li>Understanding and Improving Layer Normalization <a href="https://arxiv.org/abs/1911.07013">[2]</a></li></ul><p>了解Transformer这些基本组件后，还有值得探讨的是Mask。</p><h3 id="Transformer中的Mask"><a href="#Transformer中的Mask" class="headerlink" title="Transformer中的Mask"></a>Transformer中的Mask</h3><p>此Mask非彼BERT的的那个掩码Mask。这里的Mask指Pad Mask和Attention Mask。</p><p>在<strong>Encoder</strong>中，Pad Mask需要去掉$$<pad>$$的计算。什么是$$<pad>$$呢？为了并行计算提高训练速度，通常把数据打包成batch，一批批训练，例如一个batch：</p><ul><li>[“青花瓷”, “爱在西元前”, “星晴”]</li></ul><p>但是模型只能处理长度相同的句，于是用&lt;pad&gt;填充到相同长度</p><ul><li>[“青花瓷$$<pad><pad>$$”, “爱在西元前”, “星晴$$<pad><pad><pad>$$”]</li></ul><p>计算时把$$<pad>$$的位置设置成负无穷，softmax的值就趋于0，从而忽略。</p><p>在<strong>Decoder</strong>中同样要Pad Mask，除此之外还需要Attention Mask来遮住后面的词。例如训练时文本是“星晴”，标签是“乘着风”，虽然知道标签全句“乘着风”，但是推理时是一个词一个词预测的，Decoder预测出“乘”时，并不知道后面是“着风”。为了在训练时适配推理，预测“着”时需要把“着风”给Mask，也就是去掉“乘”与“着”、“风”的相似度。</p><p>”乘着风$$<pad><pad>$$“的Attention Mask矩阵如图：</p><p>$$<br>\begin{matrix}<br>1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\<br>1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\<br>1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\<br>1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\<br>1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\<br>\end{matrix}<br>$$</p><p>对应相似度矩阵0的位置会被替换成负无穷，softmax后值就趋于0，从而使得Attention矩阵第一行就只有第一个字的权重，第二行有一二两个字的权重，以此类推。</p><h3 id="Transformer全览"><a href="#Transformer全览" class="headerlink" title="Transformer全览"></a>Transformer全览</h3><p>结合上述所有组件，对照着模型图，对Transformer做个全览。</p><p><img src="/imgs/transformer.jpeg" alt="transformer"></p><p>Encoder的流程如下：</p><ol><li><p>输入是<strong>“星晴”</strong>，先根据词表转化为2个向量的矩阵</p></li><li><p>加上位置信息</p></li><li><p>过Self-Attention</p></li><li><p>和没过Self-Attention的矩阵相加，然后LayerNorm标准化</p></li><li><p>过FFN，再和没过FFN的矩阵相加，然后LayerNorm标准化</p></li><li><p>得到跟输入向量维度一样的Encoder矩阵</p></li></ol><p>为了告诉Decoder从哪开始到哪结束，需要添加开始符和结束符，例如$$<sos>$$（start of sentence）和$$<eos>$$（end of sentence）。Decoder训练的流程如下：</p><ol><li>标签是$$\text{trg} = [sos, x_1, x_2, x_3, eos]$$，输入<code>trg[:-1]</code>，如“$$<sos>$$乘着风”，根据词表转化为4个向量的矩阵</li><li>加上位置信息</li><li>过<strong>Attention Mask了</strong>的Self-Attention</li><li>和没过Self-Attention的矩阵相加，然后LayerNorm标准化</li><li><strong>得到的矩阵作为Q，Encoder矩阵作为KV，做Cross Attention</strong></li><li>和没过<strong>Cross Attention</strong>的矩阵相加，然后LayerNorm标准化</li><li>过FFN，再和没过FFN的矩阵相加，然后LayerNorm标准化</li><li>**过一层全连接和Softmax，得到$$\text{output} = [y_0, y_1, y_2, y_3]$$**，例如得到“乘着车$$<eos>$$”</li><li>多分类问题用交叉熵计算<code>trg[1:]</code>和<code>output</code>间的损失、更新参数</li></ol><p>Decoder推理时，这个流程变成：</p><ol><li>向Decoder输入$$<sos>$$，输出$$[y_0]$$</li><li>合并得$$[<sos>, y_0]$$，继续向Decoder输入，得$$[y_0^{‘}, y_1]$$</li><li>合并得$$[<sos>, y_0, y_1]$$，以此类推</li><li>当预测到$$<eos>$$，或者达到设置的最大长度，停止</li></ol><p>通过本章，相信你已经对 Transformer 模型的定义和发展有了大概的了解。幸运的是，<a href="https://huggingface.co/">Hugging Face</a> 专门为使用 Transformer 模型编写了一个 <a href="https://huggingface.co/docs/transformers/index">Transformers 库</a>，并且在<a href="https://huggingface.co/models">Hugging Face Hub</a>中提供了训练好的模型以供快速使用。</p><p>在后面的章节中我会手把手地带你编写并训练自己的 Transformer 模型。</p>]]></content>
    
    
    <summary type="html">attention机制，attention原理，Transformer原理，Transformer代码</summary>
    
    
    
    <category term="Deep Learning" scheme="https://www.jkzhang.ml/categories/Deep-Learning/"/>
    
    <category term="Transformers" scheme="https://www.jkzhang.ml/categories/Deep-Learning/Transformers/"/>
    
    
    <category term="deep learning" scheme="https://www.jkzhang.ml/tags/deep-learning/"/>
    
    <category term="transformers" scheme="https://www.jkzhang.ml/tags/transformers/"/>
    
    <category term="attention" scheme="https://www.jkzhang.ml/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>Linux目录结构</title>
    <link href="https://www.jkzhang.ml/2024/06/13/Linux%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/"/>
    <id>https://www.jkzhang.ml/2024/06/13/Linux%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/</id>
    <published>2024-06-13T09:29:01.000Z</published>
    <updated>2024-06-13T09:36:43.100Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linux目录结构"><a href="#Linux目录结构" class="headerlink" title="Linux目录结构"></a>Linux目录结构</h1><p>登录系统后，在当前命令窗口下输入命令：</p><pre><code class="docker"> ls /</code></pre><p>树状图结构：<br><img src="https://s3.bmp.ovh/imgs/2024/06/13/f7eb55e2cc51da69.jpg"></p><h3 id="系统启动必须："><a href="#系统启动必须：" class="headerlink" title="系统启动必须："></a><strong>系统启动必须：</strong></h3><ul><li><strong>/boot：</strong>存放的启动Linux 时使用的内核文件，包括连接文件以及镜像文件。</li><li><strong>/etc：</strong>存放<strong>所有</strong>的系统需要的<strong>配置文件</strong>和<strong>子目录列表，</strong>更改目录下的文件可能会导致系统不能启动。</li><li><strong>/lib</strong>：存放基本代码库（比如c++库），其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。</li><li><strong>/sys</strong>： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。该文件系统是内核设备树的一个直观反映。当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中</li></ul><h3 id="指令集合："><a href="#指令集合：" class="headerlink" title="指令集合："></a><strong>指令集合：</strong></h3><ul><li><strong>/bin：</strong>存放着最常用的程序和指令</li><li><strong>/sbin：</strong>只有系统管理员能使用的程序和指令。</li></ul><h3 id="外部文件管理："><a href="#外部文件管理：" class="headerlink" title="外部文件管理："></a><strong>外部文件管理：</strong></h3><ul><li><strong>/dev ：</strong>Device(设备)的缩写, 存放的是Linux的外部设备。<strong>注意：</strong>在Linux中访问设备和访问文件的方式是相同的。</li><li><strong>/media</strong>：类windows的<strong>其他设备，</strong>例如U盘、光驱等等，识别后linux会把设备放到这个目录下。</li><li><strong>/mnt</strong>：临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。</li></ul><h3 id="临时文件："><a href="#临时文件：" class="headerlink" title="临时文件："></a><strong>临时文件：</strong></h3><ul><li><strong>/run</strong>：是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。</li><li><strong>/lost+found</strong>：一般情况下为空的，系统非法关机后，这里就存放一些文件。</li><li><strong>/tmp</strong>：这个目录是用来存放一些临时文件的。</li></ul><h3 id="账户："><a href="#账户：" class="headerlink" title="账户："></a><strong>账户：</strong></h3><ul><li><strong>/root</strong>：系统管理员的用户主目录。</li><li><strong>/home</strong>：用户的主目录，以用户的账号命名的。</li><li><strong>/usr</strong>：用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。</li><li><strong>/usr/bin：</strong>系统用户使用的应用程序与指令。</li><li><strong>/usr/sbin：</strong>超级用户使用的比较高级的管理程序和系统守护程序。</li><li><strong>/usr/src：</strong>内核源代码默认的放置目录。</li></ul><h3 id="运行过程中要用："><a href="#运行过程中要用：" class="headerlink" title="运行过程中要用："></a><strong>运行过程中要用：</strong></h3><ul><li><strong>/var</strong>：存放经常修改的数据，比如程序运行的日志文件（/var/log 目录下）。</li><li><strong>/proc</strong>：管理<strong>内存空间！</strong>虚拟的目录，是系统内存的映射，我们可以直接访问这个目录来，获取系统信息。这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件来做修改。</li></ul><h3 id="扩展用的："><a href="#扩展用的：" class="headerlink" title="扩展用的："></a><strong>扩展用的：</strong></h3><ul><li><strong>/opt</strong>：默认是空的，我们安装额外软件可以放在这个里面。</li><li><strong>/srv</strong>：存放服务启动后需要提取的数据<strong>（不用服务器就是空）</strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Linux目录结构&quot;&gt;&lt;a href=&quot;#Linux目录结构&quot; class=&quot;headerlink&quot; title=&quot;Linux目录结构&quot;&gt;&lt;/a&gt;Linux目录结构&lt;/h1&gt;&lt;p&gt;登录系统后，在当前命令窗口下输入命令：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;</summary>
      
    
    
    
    <category term="技术" scheme="https://www.jkzhang.ml/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="技术" scheme="https://www.jkzhang.ml/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="Linux" scheme="https://www.jkzhang.ml/tags/Linux/"/>
    
    <category term="操作系统" scheme="https://www.jkzhang.ml/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="计算机" scheme="https://www.jkzhang.ml/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="目录结构" scheme="https://www.jkzhang.ml/tags/%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>git入门使用完整流程</title>
    <link href="https://www.jkzhang.ml/2024/06/07/git%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B/"/>
    <id>https://www.jkzhang.ml/2024/06/07/git%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B/</id>
    <published>2024-06-07T14:16:53.000Z</published>
    <updated>2024-06-07T14:28:07.342Z</updated>
    
    <content type="html"><![CDATA[<h1 id="git入门-完整流程"><a href="#git入门-完整流程" class="headerlink" title="git入门-完整流程"></a>git入门-完整流程</h1><h2 id="创建新仓库"><a href="#创建新仓库" class="headerlink" title="创建新仓库"></a>创建新仓库</h2><p>创建新文件夹，打开，然后执行<code>git init</code>，以创建新的 git 仓库。</p><h2 id="克隆仓库"><a href="#克隆仓库" class="headerlink" title="克隆仓库"></a>克隆仓库</h2><p>执行<code>git clone</code>以创建仓库的克隆版本。</p><h2 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h2><p>本地仓库由 git 维护的三棵“树”组成。第一个是你的 <strong>工作目录</strong>，它持有实际文件；第二个是 <strong>暂存区（Index）</strong>，它像个缓存区域，临时保存你的改动；最后是 <strong>HEAD</strong>，它指向你最后一次提交的结果。</p><p><img src="https://www.runoob.com/manual/git-guide/img/trees.png" alt="https://www.runoob.com/manual/git-guide/img/trees.png"></p><h2 id="添加和提交"><a href="#添加和提交" class="headerlink" title="添加和提交"></a>添加和提交</h2><p>你可以提出更改（把它们添加到暂存区），使用如下命令： </p><pre><code class="python">git add &lt;filename&gt; </code></pre><p>或者添加目录下面所有文件</p><pre><code class="python">git add .</code></pre><p> 这是 git 基本工作流程的第一步；使用如下命令以实际提交改动：</p><pre><code class="python"> git commit -m &quot;代码提交信息&quot; </code></pre><p>现在，你的改动已经提交到了 HEAD，但是还没到你的远端仓库。</p><h2 id="推送改动"><a href="#推送改动" class="headerlink" title="推送改动"></a>推送改动</h2><p>你的改动现在已经在本地仓库的 HEAD 中了。执行如下命令以将这些改动提交到远端仓库：</p><pre><code class="python"> git push origin master </code></pre><p>可以把 master 换成你想要推送的任何分支。 </p><p>如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加： </p><pre><code class="python">git remote add origin &lt;server&gt;</code></pre><p> 如此你就能够将你的改动推送到所添加的服务器上去了。</p><h2 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h2><p>分支是用来将特性开发绝缘开来的。在你创建仓库的时候，<em>master</em> 是“默认的”分支。在其他分支上进行开发，完成后再将它们合并到主分支上。</p><p><img src="https://www.runoob.com/manual/git-guide/img/branches.png" alt="https://www.runoob.com/manual/git-guide/img/branches.png"></p><p>创建一个叫做“feature_x”的分支，并切换过去：</p><pre><code class="python">git checkout -b feature_x</code></pre><p>切换回主分支：</p><pre><code class="python">git checkout master</code></pre><p>再把新建的分支删掉：</p><pre><code class="python">git branch -d feature_x</code></pre><p>除非你将分支推送到远端仓库，不然该分支就是 <strong><em>不为他人所见的</em>：</strong></p><pre><code class="python">git push origin &lt;branch&gt;</code></pre><h2 id="更新与合并"><a href="#更新与合并" class="headerlink" title="更新与合并"></a>更新与合并</h2><p>要更新你的本地仓库至最新改动，执行：</p><pre><code class="python">git pull</code></pre><p>以在你的工作目录中 <em>获取（fetch）</em> 并 <em>合并（merge）</em> 远端的改动。</p><p>要合并其他分支到你的当前分支（例如 master），执行：</p><pre><code class="python">git merge &lt;branch&gt;</code></pre><p>在这两种情况下，git 都会尝试去自动合并改动。遗憾的是，这可能并非每次都成功，并可能出现<em>冲突（conflicts）</em>。 这时候就需要你修改这些文件来手动合并这些<em>冲突（conflicts）</em>。改完之后，你需要执行如下命令以将它们标记为合并成功：</p><pre><code class="python">git add &lt;filename&gt;</code></pre><p>在合并改动之前，你可以使用如下命令预览差异：</p><pre><code class="python">git diff &lt;source_branch&gt; &lt;target_branch&gt;</code></pre><h2 id="实践——向gitea上传仓库"><a href="#实践——向gitea上传仓库" class="headerlink" title="实践——向gitea上传仓库"></a>实践——向gitea上传仓库</h2><p><strong>基本流程:add-&gt;commit-&gt;push。</strong></p><h3 id="一、init"><a href="#一、init" class="headerlink" title="一、init"></a>一、init</h3><pre><code class="python">git init</code></pre><h3 id="二、add"><a href="#二、add" class="headerlink" title="二、add"></a>二、add</h3><p>注意:add有多种形式，可以add某个文件，某个文件夹，或直接add当前仓库下所有文件</p><pre><code class="python">git add 单个文件git add 文件夹1/ 文件夹2/ ……多个文件夹之间空格隔开git add .</code></pre><h3 id="三、commit"><a href="#三、commit" class="headerlink" title="三、commit"></a><strong>三、commit</strong></h3><pre><code>git commit -m “注释”</code></pre><h3 id="四、push"><a href="#四、push" class="headerlink" title="四、push"></a><strong>四、push</strong></h3><pre><code>git push -u origin master</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;git入门-完整流程&quot;&gt;&lt;a href=&quot;#git入门-完整流程&quot; class=&quot;headerlink&quot; title=&quot;git入门-完整流程&quot;&gt;&lt;/a&gt;git入门-完整流程&lt;/h1&gt;&lt;h2 id=&quot;创建新仓库&quot;&gt;&lt;a href=&quot;#创建新仓库&quot; class=&quot;he</summary>
      
    
    
    
    <category term="技术" scheme="https://www.jkzhang.ml/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="git" scheme="https://www.jkzhang.ml/tags/git/"/>
    
    <category term="学习笔记" scheme="https://www.jkzhang.ml/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>在糟糕的环境中保持心灵的宁静</title>
    <link href="https://www.jkzhang.ml/2024/05/25/%E5%9C%A8%E7%B3%9F%E7%B3%95%E7%9A%84%E7%8E%AF%E5%A2%83%E4%B8%AD%E4%BF%9D%E6%8C%81%E5%BF%83%E7%81%B5%E7%9A%84%E5%AE%81%E9%9D%99/"/>
    <id>https://www.jkzhang.ml/2024/05/25/%E5%9C%A8%E7%B3%9F%E7%B3%95%E7%9A%84%E7%8E%AF%E5%A2%83%E4%B8%AD%E4%BF%9D%E6%8C%81%E5%BF%83%E7%81%B5%E7%9A%84%E5%AE%81%E9%9D%99/</id>
    <published>2024-05-25T15:08:38.000Z</published>
    <updated>2024-06-07T14:32:50.056Z</updated>
    
    <content type="html"><![CDATA[<p>爱比克泰德有一个重要观点：自由意志不可剥夺。他说的自由意志，是指人在自己可支配的领域里进行选择、做决定和采取行动的能力。所以，必须分清什么是可支配的，什么是不可支配的。可支配的是道德和心灵生活，在这个领域里，人有自由意志，要负起自己的责任。不可支配的是外部事物、外在遭遇以及一切其他人，在这个领域里，人没有自由意志，应该抱顺其自然的态度。<strong>明确了这个区分，在内在方面努力，在外在方面超脱，就能够保持心灵的宁静。</strong></p><h2 id="1-分清可支配的和不可支配的"><a href="#1-分清可支配的和不可支配的" class="headerlink" title="1. 分清可支配的和不可支配的"></a>1. 分清可支配的和不可支配的</h2><p>人可以支配自己的道德和心灵生活。你怎样做人，是行善还是作恶，做好人还是坏人；你有怎样的心灵品质，是优秀还是平庸，这是你自己可以做主的，任何外部的力量都不能剥夺你的主权。也只有在这个范围内，才存在善和恶、好和坏的区别与选择。  </p><p>超出这个范围的事物，既然是你不能支配的，对你来说也就无所谓善和恶、好和坏，你不要去做善恶好坏的判断，顺其自然就是了。</p><p>在你可支配的范围内，好好地运用你的自由意志，读书、写作、学习，提高自己，让自己成为心灵丰富、品德高尚的人。这个范围之外的事情，你都把它们看作与己无关，不去关心和追求。  </p><p>如果这样，你的心怎么会不宁静呢？专注于自由意志范围之内的事情，这是获得宁静的唯一途径。相反，“请记住，如果你看重自由意志范围之外的东西，你就会毁了你的自由意志”。<br>爱比克泰德反复地讲这个道理。凡是看重自己不能支配的东西的人，就会失去自由，成为这些东西——以及能够控制这些东西的人——的奴隶，受它们奴役，被它们抛来抛去。你想要得到那些你以为的好东西，你就会渴望、嫉妒、奉承、心神不安。你想要躲避那些你以为的坏事情，你就会恐惧、焦虑、屈服、悲伤。<br>神规定了可支配和不可支配的界限，你却不服从，这些精神的痛苦都是对你不服从神意的惩罚。</p><p>一个人处在焦虑不安之中，就一定是出了问题。如果他没有想要得到不在他可支配范围内的东西，他是不会焦虑不安的。琴师独自演奏的时候，总是不慌不忙，可是到了舞台上，尽管琴艺高超，他仍然会慌张，因为这时他想要的不只是演奏得好，还想博得掌声，而这就不是他能够支配的了。</p><p>人活在世上，感官会从外界接受许多表象，你不可以受它们控制，而应该要用理性加以分析。对那些按照自己的每一种感官表象行事的人应该怎么称呼呢？爱比克泰德回答说：“一群疯子。</p><p><strong>外部世界很强大，但是一个能够正确运用自由意志的人比外部世界更强大。”知道自己可以支配什么东西，热爱这些东西，并且知道你的这些东西是任何别人无权支配的，你就会无所畏惧，获得宁静和自由。</strong></p><p><img src="https://s3.bmp.ovh/imgs/2024/06/07/98f88df09b005296.jpg"></p><h2 id="2-错误的判断导致痛苦"><a href="#2-错误的判断导致痛苦" class="headerlink" title="2.错误的判断导致痛苦"></a>2.错误的判断导致痛苦</h2><p>你要把自己可支配领域里的事情做好，同时正确对待自己不可支配领域里的事情。这个正确对待，按照爱比克泰德的说法，</p><p>就是不要做好坏的判断。因为那些事情不是你可以支配的，你不必对它们负责，对于你就不存在好坏的区别，而只要你做了好坏的判断，就一定是错误判断。</p><p>正是由于对不可支配的事情做好坏的判断，造成了一切痛苦。“哭泣与叹息是什么？一个判断。不幸是什么？一个判断。冲突、争执、吹毛求疵、非难、不敬、愚蠢，这些是什么？它们都是判断，是对在自由意志领域之外的事物所下的判断，认为它们是好的或坏的。”破坏我们灵魂安宁的东西，不是事情本身，而是我们对它们做的判断。</p><p>然而，这种做好坏判断的习惯是根深蒂固的，在我们幼年时代就开始形成了。一个小孩撞上了一块石头，正打算号啕大哭，这个时候，看护者不是批评小孩，反而是去打那块石头。爱比克泰德问道：“怎么，石头做了什么？难道因为你幼稚无知，石头就必须离开它所在的位置吗？”成年人遇到不顺心的事情，因此而怪罪事情本身，遵循的是同样的逻辑。</p><p>什么东西属于不可支配的领域？大致有三类：一是身外之物；二是外在遭遇；三是他人的品性和行为。在这三个方面，你都不要做好坏的判断，都要淡然处之。</p><h2 id="3-不受制于身外之物"><a href="#3-不受制于身外之物" class="headerlink" title="3. 不受制于身外之物"></a>3. 不受制于身外之物</h2><p>凡是拥有或不拥有都不在你的支配之下的事物，和你只能在某种程度上或某种条件下拥有的事物，都不是真正属于你的，都是身外之物。你要控制自己的欲望，不要对它们产生羡慕和迷恋，  </p><p>否则你就会受它们奴役，被它们控制。对任何身外之物的拥有都受命运的支配，今天得到，明天就可能失去。更为根本的是，一切身外之物都有生有灭，会被时间夺走。所以，你要按照事物当初被给予你时附带的条件来持有它们，只保留它们到所给予你的时限为止。神既给予，也拿回，你为什么要反抗呢？  </p><p>那么，哪些东西属于身外之物呢？一切在我们的心灵之外和我们发生联系的事物，包括财产、地位、名声，包括亲人和各种社会关系，也包括你的身体和生命，因为所有这些都不是你的自由意志能够支配的。  </p><p>财产显然是身外之物。但是，受制于财产是人世间最普遍的现象。爱比克泰德说：“我以神的名义命令你，终止对物的崇拜，<br>因为这种崇拜会使你成为物的奴隶，并且成为那些既能让你得到财产也能拿走你的财产的人的奴隶。”   </p><p>父母、兄弟、孩子也是身外之物。各人都有自己的人生之路，也都可能犯错误，那是你不能支配的，神并没有要你为之负责。  </p><p>你爱的人也都终有一死，并非与你不可分离，你要接受这个事实。你的身体及其各个部分也是身外之物，你不能完全控制自己的健康，保证自己不生病。当然，你也终有一死，所以生命也是你不可支配的身外之物。  </p><p>对于一切身外之物，你都不要说：我失去了它。你应该说：我把它归还了。你有丧子之痛吗？孩子被收回去了。你有丧妻之哀吗？妻子被收回去了。你的钱财被抢走了？这也不过是被收回去了。神的法律是，凡是给你的东西，你要当作托付给你的东西好好照料，而一旦被拿走了，你要欣然地归还，而且为你拥有使用权的那段时间心存感激。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;爱比克泰德有一个重要观点：自由意志不可剥夺。他说的自由意志，是指人在自己可支配的领域里进行选择、做决定和采取行动的能力。所以，必须分清什么是可支配的，什么是不可支配的。可支配的是道德和心灵生活，在这个领域里，人有自由意志，要负起自己的责任。不可支配的是外部事物、外在遭遇以及</summary>
      
    
    
    
    <category term="感受" scheme="https://www.jkzhang.ml/categories/%E6%84%9F%E5%8F%97/"/>
    
    
    <category term="感想" scheme="https://www.jkzhang.ml/tags/%E6%84%9F%E6%83%B3/"/>
    
    <category term="哲思" scheme="https://www.jkzhang.ml/tags/%E5%93%B2%E6%80%9D/"/>
    
    <category term="周国平" scheme="https://www.jkzhang.ml/tags/%E5%91%A8%E5%9B%BD%E5%B9%B3/"/>
    
  </entry>
  
  <entry>
    <title>GPT-4多模态模型学习笔记（1）</title>
    <link href="https://www.jkzhang.ml/2024/05/17/GPT-4%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89/"/>
    <id>https://www.jkzhang.ml/2024/05/17/GPT-4%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89/</id>
    <published>2024-05-17T15:18:42.000Z</published>
    <updated>2024-06-07T14:31:31.308Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GPT-4多模态模型学习笔记（1）"><a href="#GPT-4多模态模型学习笔记（1）" class="headerlink" title="GPT-4多模态模型学习笔记（1）"></a>GPT-4多模态模型学习笔记（1）</h1><p>之前我们一直说自然语言处理是人工智能王冠上最大的那颗珍珠，但如今用世俗的珍珠或者王冠形容已经不合适了。多模态大模型带给人类世界的震撼，就如人工智能企业Hugging Face（因提供开源预训练模型库而闻名）的联合创始人Thomas Wolf所述：“在过去的几年里，好的多模态模型一直是许多大型技术实验室的圣杯。”<strong>其中多模态指的是融合文本、图像、视频或音频等多种模态作为输入或输出。</strong> </p><p>GPT-4这个标签代表第4代生成式预训练变换模型(Generative Pre-trained Transformer 4)，是OpenAI在2023年3月14日公开的一种多模态模型，是对前几个月发布的ChatGPT的多模态升级。GPT-4模型可对图文多模态输入生成应答文字，以及对视觉元素的分类、分析和隐含语义提取，并表现出优秀的应答能力。</p><h2 id="GPT-4核心技术有哪些？"><a href="#GPT-4核心技术有哪些？" class="headerlink" title="GPT-4核心技术有哪些？"></a><strong>GPT-4核心技术有哪些？</strong></h2><h3 id="1-1理论基础——多模态涌现能力"><a href="#1-1理论基础——多模态涌现能力" class="headerlink" title="1.1理论基础——多模态涌现能力"></a>1.1理论基础——多模态涌现能力</h3><p>讲到大语言模型的优势，一般首先要提到这类模型的<strong>涌现能力和思维链</strong>。这两者是大语言模型不断接近人类的关键特征。我们之所以认为GPT-4会是具有里程碑意义的一代，正是因为多模态的GPT-4会从视觉角度和视觉-文字语义融合方面涌现出更多的能力。2022-2023年，我们可以认为AI是第一次睁开双眼理解这个世界。</p><p>在大型语言模型（LLM）中，<strong>涌现能力（Emergent Abilities）是指模型具有从原始训练数据中自动学习并发现新的、更高层次的特征和模式的能力。</strong></p><p>就中文释义而言，涌现能力也指大语言模型涌现出来的新能力。这有点类似于去超市遇到买二赠一，赠品的质量居然还出乎意料。</p><p>与大语言模型（LLM）相比，<strong>多模态大语言模型(Multi-modal Large Language Model，MLLM)可实现更好的常识推理性能，</strong> 跨模态迁移更有利于知识获取，产生更多新的能力，加速了能力的涌现。这些独立模态或跨模态新特征、能力或模式通常不是通过目的明确的编程或训练获得的，而是模型在大量多模态数据中自然而然的学习到的。</p><p><img src="https://s3.bmp.ovh/imgs/2024/06/07/16921ff8c137363a.png"></p><p>在语言模型发展的早期，通过在更多数据上训练更大的模型，可获得近似连续的精确度提升。*(可称为缩放定律/Scaling Laws*)到了2015年左右，随着深度学习技术的发展和语料库的增大，模型达到一定的临界规模后，NLP开发者们发现，<strong>大语言模型(包括GPT-3、GLaM、LaMDA和Megatron-Turing NLG等)开始表现出一些开发者最开始未能预测的、更复杂的能力和特性，这些新能力和新特性被认为是涌现能力的体现。</strong></p><p>涌现能力是基于深度学习模型的分层结构和权重学习机制实现的。涌现出来的能力可以是基于文本的，也可以是多模态的。我们可以将GPT-4这类大模型的训练视为解方程，每一层神经元（可视为变量组合）的输出都作为下一层神经元的输入，并且模型的每个权重（Weight）都通过强化学习算法进行学习和更新。这种分层的结构和权重学习机制使得深度学习模型能够自动的学习到从原始数据中提取隐含的特征和模式，从而实现涌现能力。</p><p>涌现能力的另一个重要表现是<strong>模型的泛化能力</strong>。在没有专门训练过的情况，GPT-4也可以泛化到新的、未知的多模态数据样本上。这种泛化能力取决于模型的结构和训练过程，以及数据的数量和多样性。如果模型具有足够的复杂性和泛化能力，就可以从原始数据中发现新的、未知的特征和模式。当然，GPT-4涌现出的新能力可能仍有局限性，例如：模型可能产生错误的回答，对某些问题缺乏理解，容易受到输入干扰等。目前认为GPT-4的幻觉与其涌现能力具有相关性。</p><h2 id="多模态思维链"><a href="#多模态思维链" class="headerlink" title="多模态思维链"></a>多模态思维链</h2><p><strong>思维链(Chain of Thought)可视为大语言模型涌现出来的核心能力之一。</strong>之所以现在各类GPT研究火爆，也与模型训练出的思维链可进入实用有密切关系。思维链形成机制可以解释为模型通过学习大量的语言数据来构建一个关于语言结构和意义的内在表示，通过一系列中间自然语言推理步骤来完成最终输出。思维链是ChatGPT和GPT-4能让大众感觉到语言模型“像人”的关键特性。</p><p>虽然GPT-4这些模型并非具备真正的意识或思考能力，<strong>但用类似于人的推理方式的思维链来提示语言模型，极大的提高了GPT-4在推理任务上的表现，打破了精调（Fine-tune）的平坦曲线。</strong>具备了多模态思维链能力的GPT-4模型<strong>具有一定逻辑分析能力，已经不是传统意义上的词汇概率逼近模型。</strong></p><p>当然思维链的训练可能并不容易。尽管现在有大量团队进入大语言模型训练领域，但若干年内能找到训练诀窍并完成思维链训练的团队可能不多。对创企来说，完成思维链的训练，才算真正拿到了这波大模型AI竞技的入场券。</p><p><img src="https://s3.bmp.ovh/imgs/2024/06/07/3e7dcec6de2aa9ce.png"></p><p>通过多模态思维链技术，GPT-4将一个多步骤的问题（例如图表推理）分解为可以单独解决的中间步骤。在解决多步骤推理问题时，模型生成的思维链会模仿人类思维过程。这意味着额外的计算资源被分配给需要更多推理步骤的问题，可以进一步增强GPT-4的表达和推理能力。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;GPT-4多模态模型学习笔记（1）&quot;&gt;&lt;a href=&quot;#GPT-4多模态模型学习笔记（1）&quot; class=&quot;headerlink&quot; title=&quot;GPT-4多模态模型学习笔记（1）&quot;&gt;&lt;/a&gt;GPT-4多模态模型学习笔记（1）&lt;/h1&gt;&lt;p&gt;之前我们一直说自然语</summary>
      
    
    
    
    <category term="AI学习笔记" scheme="https://www.jkzhang.ml/categories/AI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="GPT-4" scheme="https://www.jkzhang.ml/tags/GPT-4/"/>
    
    <category term="多模态" scheme="https://www.jkzhang.ml/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    <category term="LLM" scheme="https://www.jkzhang.ml/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>逆序输出数字</title>
    <link href="https://www.jkzhang.ml/2023/09/06/%E9%80%86%E5%BA%8F%E8%BE%93%E5%87%BA%E6%95%B0%E5%AD%97/"/>
    <id>https://www.jkzhang.ml/2023/09/06/%E9%80%86%E5%BA%8F%E8%BE%93%E5%87%BA%E6%95%B0%E5%AD%97/</id>
    <published>2023-09-06T14:53:40.000Z</published>
    <updated>2024-05-17T14:48:35.104Z</updated>
    
    <content type="html"><![CDATA[<p><strong>输入一个数字后逆序输出之。</strong><br>用数列吗？也可以。</p><p>更为巧妙的是，利用了取余运算。<br>因为整数取余给整型，仍会强制转换为整型，因此不断除以10并取余。</p><pre><code>#include&lt;stdio.h&gt;int main()&#123;    int inp = 0;    scanf(&quot;%d&quot;,&amp;inp);    while(inp%10!=0)&#123;            printf(&quot;%d&quot;,inp%10);            inp = inp/10;    &#125;    return 0;&#125;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;输入一个数字后逆序输出之。&lt;/strong&gt;&lt;br&gt;用数列吗？也可以。&lt;/p&gt;
&lt;p&gt;更为巧妙的是，利用了取余运算。&lt;br&gt;因为整数取余给整型，仍会强制转换为整型，因此不断除以10并取余。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include&amp;lt;stdio.h</summary>
      
    
    
    
    <category term="C语言" scheme="https://www.jkzhang.ml/categories/C%E8%AF%AD%E8%A8%80/"/>
    
    
    <category term="C语言" scheme="https://www.jkzhang.ml/tags/C%E8%AF%AD%E8%A8%80/"/>
    
    <category term="算法" scheme="https://www.jkzhang.ml/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="基础算法" scheme="https://www.jkzhang.ml/tags/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"/>
    
    <category term="取余" scheme="https://www.jkzhang.ml/tags/%E5%8F%96%E4%BD%99/"/>
    
  </entry>
  
  <entry>
    <title>转置一个矩阵</title>
    <link href="https://www.jkzhang.ml/2023/08/27/%E8%BD%AC%E7%BD%AE%E4%B8%80%E4%B8%AA%E7%9F%A9%E9%98%B5/"/>
    <id>https://www.jkzhang.ml/2023/08/27/%E8%BD%AC%E7%BD%AE%E4%B8%80%E4%B8%AA%E7%9F%A9%E9%98%B5/</id>
    <published>2023-08-27T11:35:51.000Z</published>
    <updated>2024-05-17T14:48:35.104Z</updated>
    
    <content type="html"><![CDATA[<p><strong>将一个三行四列的矩阵转置后变成四行三列的矩阵。</strong></p><p>这个问题我打算用二维数组。</p><p>首先，我打算随机生成一个数组，并定义一个函数绘制它</p><pre><code>void printAnArray(int arr[ROW][COL])&#123;        for(int i=0;i&lt;ROW;i++)&#123;        for(int j=0;j&lt;COL;j++)&#123;            printf(&quot;%4d&quot;,arr[i][j]); //%4d tab制表        &#125;         printf(&quot;\n&quot;);    &#125;&#125;</code></pre><p>怎么实现转置，认清实质：把每一行的元素赋给对应列！</p><pre><code>void convertion(int arr[ROW][COL],int arr2[COL][ROW])&#123;       for(int i=0;i&lt;COL;i++)&#123;        for(int j=0;j&lt;ROW;j++)&#123;            arr2[i][j]=arr[j][i]; //有理解难度！        &#125;    &#125;for(int i=0;i&lt;COL;i++)&#123;        for(int j=0;j&lt;ROW;j++)&#123;             printf(&quot;%4d&quot;,arr2[i][j]);        &#125;          printf(&quot;\n&quot;);    &#125;&#125;</code></pre><p>综上，</p><pre><code># include &lt;stdio.h&gt;# include &lt;stdlib.h&gt;#include &lt;time.h&gt;#define ROW 3#define COL 4void printAnArray(int arr[ROW][COL])&#123;        for(int i=0;i&lt;ROW;i++)&#123;        for(int j=0;j&lt;COL;j++)&#123;            printf(&quot;%4d&quot;,arr[i][j]);        &#125;         printf(&quot;\n&quot;);    &#125;&#125;void convertion(int arr[ROW][COL],int arr2[COL][ROW])&#123;       for(int i=0;i&lt;COL;i++)&#123;        for(int j=0;j&lt;ROW;j++)&#123;            arr2[i][j]=arr[j][i];        &#125;    &#125;for(int i=0;i&lt;COL;i++)&#123;        for(int j=0;j&lt;ROW;j++)&#123;             printf(&quot;%4d&quot;,arr2[i][j]);        &#125;          printf(&quot;\n&quot;);    &#125;&#125;int main()&#123;    int array_a[ROW][COL]=&#123;&#125;;    int array_b[COL][ROW]=&#123;&#125;;    srand(time(0));    for(int i=0;i&lt;ROW;i++)&#123;        for(int j=0;j&lt;COL;j++)&#123;            array_a[i][j]=rand()%100;        &#125;    &#125;    printf(&quot;Before converted:\n&quot;);    printAnArray(array_a);    printf(&quot;After convertion:\n&quot;);    convertion(array_a,array_b);    return 0;&#125;</code></pre><p>书上还提供了一个思考题：如果行数、列数相等，能否只用一个数组实现？</p><p>我觉得，不用操作数组，更改打印的思路：</p><p>原来是横着打印，现在竖着打印就行。即原来按列走，现在按行走。</p><pre><code>int main()&#123;           for(int i=0;i&lt;ROW;i++)&#123;        for(int j=0;j&lt;ROW;j++)&#123;            arr[i][j]=rand()%100;        &#125;    &#125;    printAnArray(arr);    printf(&quot;After operation:\n&quot;);    for(int i =0 ;i&lt;ROW;i++)&#123;        for(int j =0;j&lt;ROW;j++)&#123;            printf(&quot;%4d&quot;,arr[j][i]);        &#125;        printf(&quot;\n&quot;);    &#125;    return 0;&#125;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;将一个三行四列的矩阵转置后变成四行三列的矩阵。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这个问题我打算用二维数组。&lt;/p&gt;
&lt;p&gt;首先，我打算随机生成一个数组，并定义一个函数绘制它&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;void printAnArray(int arr[R</summary>
      
    
    
    
    <category term="C语言学习" scheme="https://www.jkzhang.ml/categories/C%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="C语言" scheme="https://www.jkzhang.ml/tags/C%E8%AF%AD%E8%A8%80/"/>
    
    <category term="二维数组" scheme="https://www.jkzhang.ml/tags/%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84/"/>
    
  </entry>
  
  <entry>
    <title>子数列和问题</title>
    <link href="https://www.jkzhang.ml/2023/07/19/%E5%AD%90%E6%95%B0%E5%88%97%E5%92%8C%E9%97%AE%E9%A2%98/"/>
    <id>https://www.jkzhang.ml/2023/07/19/%E5%AD%90%E6%95%B0%E5%88%97%E5%92%8C%E9%97%AE%E9%A2%98/</id>
    <published>2023-07-19T12:27:33.000Z</published>
    <updated>2024-05-17T14:48:35.109Z</updated>
    
    <content type="html"><![CDATA[<p>一道经典的算法问题<a id="more"></a><br><img src="https://mirror.ghproxy.com/https://raw.githubusercontent.com/zhangrongxiang/store/master/Untitle3d.png"></p><pre><code class="c"># include&lt;stdio.h&gt;int max(int A,int B, int C)&#123;    return (A &gt; B) ? (A &gt; C ? A : C) : (B &gt; C ? B : C);&#125;// 改进版暴力搜索int maxSubArray(int arr[],int n)&#123;       int ultimate_max=0;    for(int i=0;i&lt;n;i++)&#123;         int this_max=0;        for(int j=i;j&lt;n;j++)&#123;            this_max += arr[j]; //直接加即可            if(this_max&gt;ultimate_max)&#123;                ultimate_max=this_max;            &#125;        &#125;    &#125;    return ultimate_max;&#125;/*分治法球List[left]到List[right]的最大子列和，复杂度O(nlog2n)*/int DivideAndConquer ( int List[], int left, int right ) &#123;    int MaxLeftSum, MaxRightSum;    //存放左右子问题的解。    int MaxLeftBorderSum, MaxRightBorderSum;    //存放跨分界线的结果。        int LeftBorderSum, RightBorderSum;    int center, i;        /*递归的终止条件，子列只有1个数字*/    if ( left == right ) &#123;        if ( List[left] &gt; 0 )    return List[left];        else return 0;    &#125;        /* “分”的过程 */    center = ( left + right ) / 2;    //找到中分点。    MaxLeftSum = DivideAndConquer ( List, left, center );    //递归求左子列和。    MaxRightSum = DivideAndConquer ( List, center+1, right );    //递归求右子列和。        /*求跨分界线的最大子列和*/    MaxLeftBorderSum = 0;    LeftBorderSum = 0;    for ( i = center; i &gt;= left; i-- ) &#123;        LeftBorderSum += List[i];        if ( LeftBorderSum &gt; MaxLeftBorderSum )            MaxLeftBorderSum = LeftBorderSum;    &#125;//左边扫描结束。        MaxRightBorderSum = 0;    RightBorderSum = 0;    for ( i = center+1; i &lt;= right; i++ ) &#123;        RightBorderSum += List[i];        if ( RightBorderSum &gt; MaxRightBorderSum )            MaxRightBorderSum = RightBorderSum;    &#125;//右边扫描结束。        /*返回“治”的结果*/    return max ( MaxLeftSum, MaxRightSum, MaxLeftBorderSum + MaxRightBorderSum );&#125;/*此函数用于保持接口相同*/int maxSubArray3 ( int List[], int N ) &#123;    return DivideAndConquer ( List, 0, N-1 );&#125;//在线处理法int maxSubArray2(int arr[],int n)&#123;    int this_max=0;    int ultimate_max=0;    for(int i=0;i&lt;n;i++)&#123;        this_max += arr[i];        if(this_max&gt;ultimate_max)&#123;            ultimate_max=this_max;        &#125;else if(this_max &lt; 0)&#123;            this_max = 0;        &#125;    &#125;    return ultimate_max;&#125;int main()&#123;    int n=0;    int array[1000];    scanf(&quot;%d&quot;,&amp;n);    for(int i=0;i&lt;n;i++)&#123;        scanf(&quot;%d&quot;,&amp;array[i]);    &#125;    printf(&quot;OK.&quot;);    getchar();    printf(&quot;%d&quot;,maxSubArray3(array,n));&#125;</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;一道经典的算法问题</summary>
    
    
    
    <category term="数据结构与算法" scheme="https://www.jkzhang.ml/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="C" scheme="https://www.jkzhang.ml/tags/C/"/>
    
    <category term="数据结构" scheme="https://www.jkzhang.ml/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    <category term="算法" scheme="https://www.jkzhang.ml/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>不需要临时变量的双变量交换数值</title>
    <link href="https://www.jkzhang.ml/2023/07/10/%E4%B8%8D%E9%9C%80%E8%A6%81%E4%B8%B4%E6%97%B6%E5%8F%98%E9%87%8F%E7%9A%84%E5%8F%8C%E5%8F%98%E9%87%8F%E4%BA%A4%E6%8D%A2%E6%95%B0%E5%80%BC/"/>
    <id>https://www.jkzhang.ml/2023/07/10/%E4%B8%8D%E9%9C%80%E8%A6%81%E4%B8%B4%E6%97%B6%E5%8F%98%E9%87%8F%E7%9A%84%E5%8F%8C%E5%8F%98%E9%87%8F%E4%BA%A4%E6%8D%A2%E6%95%B0%E5%80%BC/</id>
    <published>2023-07-10T00:54:41.000Z</published>
    <updated>2024-05-17T14:48:35.100Z</updated>
    
    <content type="html"><![CDATA[<h1 id="不需要第三个变量的变量交换"><a href="#不需要第三个变量的变量交换" class="headerlink" title="不需要第三个变量的变量交换"></a>不需要第三个变量的变量交换</h1><p>要知道的事：</p><p>a^0=a;</p><p>a^a=0.<a id="more"></a></p><pre><code class="c">void inplace_swap(int *x, int *y)&#123;*y =*x ^ *y;  //Step 1*x = *x ^ *y; //Step 2*y =*x ^*y;   //Step 3&#125;</code></pre><p>正如程序名字暗示的，我们认为这个过程交换了指针变量x y位置处存放的值。</p><p>注意，与通常方法不同，该方法不需要第三个临时变量存储另一个值。不过这种方法性能上优势不大，仅仅是个智力游戏。</p><table><thead><tr><th>Step</th><th>*x</th><th>*y</th></tr></thead><tbody><tr><td>1</td><td>a</td><td>a^b</td></tr><tr><td>2</td><td>b</td><td>a^b</td></tr><tr><td>3</td><td>b</td><td>a</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;不需要第三个变量的变量交换&quot;&gt;&lt;a href=&quot;#不需要第三个变量的变量交换&quot; class=&quot;headerlink&quot; title=&quot;不需要第三个变量的变量交换&quot;&gt;&lt;/a&gt;不需要第三个变量的变量交换&lt;/h1&gt;&lt;p&gt;要知道的事：&lt;/p&gt;
&lt;p&gt;a^0=a;&lt;/p&gt;
&lt;p&gt;a^a=0.</summary>
    
    
    
    <category term="程序设计" scheme="https://www.jkzhang.ml/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    
    <category term="C语言" scheme="https://www.jkzhang.ml/tags/C%E8%AF%AD%E8%A8%80/"/>
    
    <category term="位运算" scheme="https://www.jkzhang.ml/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"/>
    
    <category term="深入理解计算机系统" scheme="https://www.jkzhang.ml/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>The End Of Life</title>
    <link href="https://www.jkzhang.ml/2022/01/29/The-Ends-Of-Life/"/>
    <id>https://www.jkzhang.ml/2022/01/29/The-Ends-Of-Life/</id>
    <published>2022-01-29T09:01:08.000Z</published>
    <updated>2024-05-17T15:41:25.742Z</updated>
    
    <content type="html"><![CDATA[<p>THE END OF LIFE, being the first section of the last chapter “Epilogue” of Lin Yutang’s My Country and My People, published in New York by the John Day Publishing Company, 1935.</p><p>Lin Yutang，林语堂（1895-1976）, Chinese philologist and author. His My Country and My People has won for him both in America and England the reputation of being one of the ablest interpreters of China and her civilization.</p><p>In the general survey of Chinese art and Chinese life, the conviction must have been forced upon us that the Chinese are past masters in the art of living. There is a certain whole-hearted concentration on the material life, a certain zest in living, which is mellower, perhaps deeper, anyway just as intense as in the West. In China the spiritual values have not been separated from the material values, but rather help man in a keener enjoyment of life as it falls to our lot. This accounts for our joviality and our incorrigible humor. A heathen can have a heathenish devotion to the life of the present and envelop both spiritual and material values in one outlook, which it is difficult for a Christian to imagine. We live the life of the senses and the life of the spirit at the same moment, and see no necessary conflict. For the human spirit is used to beautify life, to extract its essence, perhaps to help it overcome ugliness and pain inevitable in the world of our senses, but never to escape from it and find its meaning in a life hereafter. When Confucius said in reply to a question by a disciple on death, “Don’t know life—how know death?” he expressed there a somewhat bourgeois, unmetaphysical and practical attitude toward the problems of life and knowledge which has characterized our national life and thinking.<br>在概述了中国的艺术与生活之后，我们不得不承认中国人的确是精通生活艺术的大师。他们全心全意地致力于物质生活，其热忱决不下于西方，并且更为成熟，或许还更为深沉。在中国，精神的价值并未与物质的价值相分离，反而帮助了人们更好地享受自己命定的生活。这就解释了为什么我们具有一种快活的性情和根深蒂固的幽默。一个无宗教信仰的人会对现世的世俗生活抱有一种粗野的热忱，并且融物质与精神两种价值于一身，这在基督徒是难以想象的。我们能够同时生活在感官世界和精神世界之中，而不认为两者一定会有什么冲突。因为人类的精神是被用来美化生活，提炼生活的精华，或许还能帮助生活克服感官世界中不可避免的种种丑恶和痛苦，而不是用来逃避生活，或在来世找寻生活的意义。孔子在回答一位弟子关于死亡的问题时说：“未知生，焉知死？”这句话表达了一种对于生命和知识问题的庸俗、具体而实用的态度，而正是这种态度造就了我们现在国民生活及思维的特征。</p><p>This standpoint establishes for us a certain scale of values. In every aspect of knowledge and of living, the test of life holds. It accounts for our pleasures and our antipathies. The test of life was with us a racial thought, wordless and needing no definition or giving of reasons. It was that test of life which, instinctively I think, guided us to distrust civic civilization and uphold the rural ideal in art, life and letters, to dislike religion in our rational moments, to play with Buddhism but never quite accept its logical conclusions, and to hate mechanical ingenuity. It was that instinctive trust in life that gave us a robust common sense in looking at life’s kaleidoscopic changes and the myriad vexatious problems of the intellect which we rudely ignored. It enabled us to see life steadily and see life whole, with no great distortions of values. It taught us some simple wisdom, like respect for old age and the joys of domestic life, acceptance of life, of sex, and of sorrow. It made us lay emphasis on certain common virtues, like endurance, industry, thrift, moderation, and pacificism. It prevented the development of freakish extreme theories and the enslaving of man by the products of his own intelligence. It gave us a sense of values, and taught us to accept the material as well as the spiritual goods of life. It taught us that, after all is said and done, human happiness is the end of all knowledge. And we arrange ourselves to make our lives happy on this planet, under whatever vicissitudes of fortune.<br>这一立场为我们树立了多层级的价值尺度。这种生活标准适用于知识和人生的方方面面，解释了我们喜好与憎恶某一事物的原因。这种生活标准已经融入我们的民族意识，不需要任何文字上的说明、界定或阐释。我认为也正是这种生活标准促使我们在艺术、人生和文学中本能地怀疑城市文明，而崇尚田园理想；促使我们在理智的时刻厌恶宗教，涉猎佛学但从不完全接受其合乎逻辑的结论；促使我们憎恶机械发明。正是这种对于生活的本能信仰，赋予我们一种坚定的常识，面对生活的万千变化以及智慧的无数棘手问题，可以做到岿然不动。它使我们能够沉着地、完整地看待生活，并维系固有的价值观念。它也教会了我们一些简单的智慧，比如尊敬老人，享受家庭生活的乐趣，接受生活，接受性别差异，接受悲哀。它使我们注重这样几种寻常的美德：忍耐、勤劳、节俭、中庸与和平主义。它使我们不至于发展某些怪异极端的理论，不至于成为自己智慧产品的奴隶。它赋予我们一种价值观，教会我们同时接受生活给予我们的物质和精神财富。它告诉人们：归根结底，只有人类的幸福才是一切知识的最终目标。于是我们得以在命运的浮沉中调整自己，欣欣然生活在这个行星之上。</p><p>We are an old nation. The eyes of an old people see in its past and in this changing modern life much that is superficial and much that is of true meaning to our lives. We are a little cynical about progress, and we are a little bit indolent, as are all old people. We do not want to race about in a field for a ball; we prefer to saunter along willow banks to listen to the bird’s song and the children’s laughter. Life is so precarious that when we know something truly satisfies us, we hold on to it tight, as a mother hugs her baby close to her breast in a dark, stormy night. We have really no desire for exploring the South Pole or scaling the Himalayas. When Westerners do that, we ask, “What do you do that for? Do you have to go to the South Pole to be happy?” We go to the movies and theaters, but in the heart of our hearts we feel that a real child’s laughter gives us as much real joy and happiness as an imaginary child’s laughter on the screen. We compare the two and stay at home. We do not believe that kissing one’s own wife is necessarily insipid, and that other people’s wives are necessarily more beautiful because they are other people’s wives. We do not ache to reach the foot of the mountain when we are in the middle of the lake, and we do not ache to be at the top of the hill when we are at its foot. We drink what wine there is in the pot and enjoy what scenery there is before our eyes.<br>我们是一个古老的民族。在老人看来，我们民族的过去以及变化万端的现代生活，有不少是浅薄的，也有不少确实触及了生活的真谛。同任何一个老人一样，我们对进步有所怀疑，我们也有点懒散。我们不喜欢为一只球在球场上争逐，而喜欢漫步于柳堤之上，听听鸟儿的鸣唱和孩子的笑语。生活是如此动荡不安，因而当我们发现了真正令自己满意的东西，我们就会抓住不放，就像一位母亲在黑暗的暴风雨之夜里紧紧搂住怀中的婴孩。我们对探险南极或者攀登喜马拉雅山实在毫无兴趣，一旦西方人这样做，我们会问：“你做这件事的目的何在？你非得到南极去寻找幸福吗？”我们会光顾影院和剧场，然而内心深处却认为，相比荧幕上的幻象，现实生活中儿童的嬉笑同样能给我们带来欢乐和幸福。如此一来，我们便情愿待在家里。我们不认为亲吻自己的老婆必定寡淡无味，而别人的妻子仅仅因为是别人的妻子就显得更加楚楚动人。我们在身处湖心之时并不渴望走到山脚下去，我们在山脚下时也并不企求登至山顶。我们信奉今朝有酒今朝醉，花开堪折直须折。</p><p>So much of life is merely a farce. It is sometimes just as well to stand by and look at it and smile, perhaps better than to take part in it. Like a dreamer awakened, we see life, not with the romantic color of yesternight’s dream, but with a saner vision. We are more ready to give up the dubious, the glamorous and the unattainable, but at the same time to hold on to the few things that we know will give us happiness. We always go back to nature as an eternal source of beauty and of true and deep and lasting happiness. Deprived of progress and of national power, we yet throw open our windows and listen to cicadas or to falling autumn leaves and inhale the fragrance of chrysanthemums, and over the top there shines the autumn moon, and we are content.<br>人生在很大程度上不过是一场闹剧，有时最好做个超然的旁观者，或许比一味参与要强得多。我们就像一个刚刚醒来的睡梦者一样，看待人生用的是一种清醒的眼光，而不是带着昨夜梦境的浪漫色彩。我们乐于放弃那些捉摸不定、令人向往却又难以达到的东西，同时紧紧抓住不多的几件我们清楚会给自己带来幸福的东西。我们常常喜欢回归自然，以之为美和真正的、深沉的、长久的幸福的永恒源泉。尽管丧失了进步与国力，我们还是能够打开窗子，聆听金蝉的鸣声，欣赏秋天的落叶，呼吸菊花的芬芳。秋月朗照之下，我们感到心满意足。</p><p>For we are now in the autumn of our national life. There comes a time in our lives, as nations and as individuals, when we are pervaded by the spirit of early autumn, in which green is mixed with gold and sadness is mixed with joy, and hope is mixed with reminiscence. There comes a time in our lives when the innocence of spring is a memory and the exuberance of summer a song whose echoes faintly remain in the air, when as we look out on life, the problem is not how to grow but how to live truly, not how to strive and labor but how to enjoy the precious moments we have, not how to squander our energy but how to conserve it in preparation for the coming winter. A sense of having arrived somewhere, of having settled and having found out what we want. A sense of having achieved something also, precious little compared with its past exuberance, but still something, like an autumn forest shorn of its summer glory but retaining such of it as will endure.<br>我们现在身处民族生活的秋天。在我们生命中的某一时刻，无论是民族还是个人，都为新秋精神所渗透：绿色错落着金色、悲伤交织着欢乐、希望混杂着怀旧。在这一时刻，春天的单纯已成记忆，夏日的繁茂已为空气中微弱回荡着的歌吟。我们看待人生，不是在筹谋怎样发展，而是去考虑如何真正地活着；不是怎样奋发劳作，而是如何珍惜当下的宝贵时光尽情享乐；不是如何挥霍自己的精力，而是养精蓄锐应对冬天的到来。我们感到自己已经到达某个地方，安顿了下来，并找到了自己想要的东西。我们还感到已经获得了某种东西，这与过去的荣华相比尽管微不足道，却像是褪去了夏日繁茂的秋林一样，仍然有些余晖在继续放光。</p><p>I like spring, but it is too young. I like summer, but it is too proud. So I like best of all autumn, because its leaves are a little yellow, its tone mellower, its colors richer, and it is tinged a little with sorrow and a premonition of death. Its golden richness speaks not of the innocence of spring, nor of the power of summer, but of the mellowness and kindly wisdom of approaching age. It knows the limitations of life and is content. From a knowledge of those limitations and its richness of experience emerges a symphony of colors, richer than all, its green speaking of life and strength, its orange speaking of golden content, and its purple of resignation and death. And the moon shines over it, and its brow seems white with reflection, but when the setting sun touches it with an evening glow, it can still laugh cheerily. An early mountain breeze brushes by and sends its shivering leaves dancing gaily to the ground, and you do not know whether the song of the falling leaves is the song of laughter or of parting tears. For it is the Song of the Spirit of Early Autumn, the spirit of calm and wisdom and maturity, which smiles at sorrow itself and praises the exhilarating, keen, cool air—the Spirit of Autumn so well expressed by Hsin Ch‘ichi:<br>我喜欢春天，可它过于稚嫩；我喜欢夏天，可它过于骄矜。因而我最喜欢秋天，喜欢它泛黄的树叶、成熟的格调和斑斓的色彩。它带着些许感伤，也带着死亡的预兆。秋天的金碧辉煌所展示的不是春天的单纯，也不是夏天的伟力，而是接近高迈之年的老成和睿智——明白人生有限因而知足，这种“生也有涯”的感知与丰富的人生经验变幻出和谐的秋色：绿色代表生命和力量，橘黄代表金玉的内容，紫色代表屈从与死亡。在月光照耀下，秋天陷入沉思，露出苍白的神情；而当夕阳的余晖抚摸她面容的时候，她仍然能够爽悦地欢笑。山间的晨风拂过，枝杈间片片颤动着的秋叶舞动着飘向大地，你真不知道这落叶的歌吟是欣喜的欢唱还是离别的泪歌，因为它是新秋精神的歌吟：镇定、智慧、成熟。这种歌吟用微笑面对悲伤，赞颂那种令人振奋、敏锐而冷静的神情——这种秋的精神在辛弃疾的笔下表现得最为恰切：</p><pre><code>“In my young days,  I had tasted only gladness,But loved to mount the top floor,But loved to mount the top floor,To write a song pretending sadness.“And now I&#39;ve tastedSorrow&#39;s flavors, bitter and sour,And can&#39;t find a word,And can&#39;t find a word,But merely say, ‘What a golden autumn hour! &#39;”</code></pre><blockquote></blockquote><p>少年不识愁滋味，爱上层楼。爱上层楼，为赋新词强说愁。<br>而今识尽愁滋味，欲说还休。欲说还休，却道天凉好个秋。<br><img src="https://fastly.jsdelivr.net/gh/zhangrongxiang/store/master/photos31251762_p0%20(1).jpg"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;THE END OF LIFE, being the first section of the last chapter “Epilogue” of Lin Yutang’s My Country and My People, published in New York b</summary>
      
    
    
    
    <category term="English" scheme="https://www.jkzhang.ml/categories/English/"/>
    
    
    <category term="essay" scheme="https://www.jkzhang.ml/tags/essay/"/>
    
    <category term="Lin Yutang" scheme="https://www.jkzhang.ml/tags/Lin-Yutang/"/>
    
  </entry>
  
  <entry>
    <title>图表写作词句</title>
    <link href="https://www.jkzhang.ml/2021/08/15/%E5%9B%BE%E8%A1%A8%E5%86%99%E4%BD%9C%E8%AF%8D%E5%8F%A5/"/>
    <id>https://www.jkzhang.ml/2021/08/15/%E5%9B%BE%E8%A1%A8%E5%86%99%E4%BD%9C%E8%AF%8D%E5%8F%A5/</id>
    <published>2021-08-15T03:23:15.000Z</published>
    <updated>2025-02-22T08:37:37.147Z</updated>
    
    <content type="html"><![CDATA[<h1 id="波动"><a href="#波动" class="headerlink" title="波动"></a>波动</h1><p>xxxxxxxxxx # include&lt;stdio.h&gt;​int max(int A,int B, int C){​    return (A &gt; B) ? (A &gt; C ? A : C) : (B &gt; C ? B : C);​}// 改进版暴力搜索int maxSubArray(int arr[],int n){       int ultimate_max=0;    for(int i=0;i&lt;n;i++){         int this_max=0;        for(int j=i;j&lt;n;j++){            this_max += arr[j]; //直接加即可            if(this_max&gt;ultimate_max){                ultimate_max=this_max;            }        }    }    return ultimate_max;}​/<em>分治法球List[left]到List[right]的最大子列和，复杂度O(nlog2n)</em>/int DivideAndConquer ( int List[], int left, int right ) {    int MaxLeftSum, MaxRightSum;    //存放左右子问题的解。    int MaxLeftBorderSum, MaxRightBorderSum;    //存放跨分界线的结果。        int LeftBorderSum, RightBorderSum;    int center, i;        /<em>递归的终止条件，子列只有1个数字</em>/    if ( left == right ) {        if ( List[left] &gt; 0 )   return List[left];        else return 0;    }        /* “分”的过程 */    center = ( left + right ) / 2;    //找到中分点。    MaxLeftSum = DivideAndConquer ( List, left, center );    //递归求左子列和。    MaxRightSum = DivideAndConquer ( List, center+1, right );    //递归求右子列和。        /<em>求跨分界线的最大子列和</em>/    MaxLeftBorderSum = 0;   LeftBorderSum = 0;    for ( i = center; i &gt;= left; i– ) {        LeftBorderSum += List[i];        if ( LeftBorderSum &gt; MaxLeftBorderSum )            MaxLeftBorderSum = LeftBorderSum;    }//左边扫描结束。        MaxRightBorderSum = 0;  RightBorderSum = 0;    for ( i = center+1; i &lt;= right; i++ ) {        RightBorderSum += List[i];        if ( RightBorderSum &gt; MaxRightBorderSum )            MaxRightBorderSum = RightBorderSum;    }//右边扫描结束。        /<em>返回“治”的结果</em>/    return max ( MaxLeftSum, MaxRightSum, MaxLeftBorderSum + MaxRightBorderSum );}/<em>此函数用于保持接口相同</em>/int maxSubArray3 ( int List[], int N ) {    return DivideAndConquer ( List, 0, N-1 );}​//在线处理法int maxSubArray2(int arr[],int n){    int this_max=0;    int ultimate_max=0;    for(int i=0;i&lt;n;i++){        this_max += arr[i];        if(this_max&gt;ultimate_max){            ultimate_max=this_max;        }else if(this_max &lt; 0){            this_max = 0;        }    }    return ultimate_max;}​int main(){    int n=0;    int array[1000];    scanf(“%d”,&amp;n);    for(int i=0;i&lt;n;i++){        scanf(“%d”,&amp;array[i]);    }    printf(“OK.”);    getchar();    printf(“%d”,maxSubArray3(array,n));}c</p><h1 id="保持不变"><a href="#保持不变" class="headerlink" title="保持不变"></a>保持不变</h1><p>remain/keep/stay stable/steady/unchanged</p><h1 id="修饰词"><a href="#修饰词" class="headerlink" title="修饰词"></a>修饰词</h1><h2 id="剧烈"><a href="#剧烈" class="headerlink" title="剧烈"></a>剧烈</h2><h3 id="副词"><a href="#副词" class="headerlink" title="副词"></a>副词</h3><p>dramatically, drastically, sharply, considerably, rapidly, suddenly, greatly, alarmingly, significantly, enormously, steeply, massively, incredibly, hugely, amazingly, substantially, markedly</p><h3 id="词组"><a href="#词组" class="headerlink" title="词组"></a>词组</h3><p>at an alarming rate, by leaps and bounds, in big leaps, by a massive leap, be a wide margin</p><h1 id="相关表达"><a href="#相关表达" class="headerlink" title="相关表达"></a>相关表达</h1><h2 id="1-超过"><a href="#1-超过" class="headerlink" title="1.超过"></a>1.超过</h2><p>more than, no less than, exceed, outrun, outstrip</p><h2 id="2-极值"><a href="#2-极值" class="headerlink" title="2.极值"></a>2.极值</h2><p>(1)高峰，历史最高peak, hit an all-time high, break the record</p><p>(2)低谷，历史最低：trough, hit an all-time low</p><h2 id="3-描述走势相同或相反"><a href="#3-描述走势相同或相反" class="headerlink" title="3.描述走势相同或相反"></a>3.描述走势相同或相反</h2><p>(1)相同：A is in line with B, in accordance with, in conformity with</p><p>(2)相反：A and B tend to move in opposite directions<br><img src="https://fastly.jsdelivr.net/gh/zhangrongxiang/store/master/photosphoto-1526628953301-3e589a6a8b74.webp"></p><h2 id="4-大约、左右、接近于"><a href="#4-大约、左右、接近于" class="headerlink" title="4.大约、左右、接近于"></a>4.大约、左右、接近于</h2><p>(1)大约：about, approximately, roughly, around, some</p><p>(2)左右：or so, plus or minus</p><p>(3)接近于：close to, nearly</p><h2 id="5-展现，表明，意味着"><a href="#5-展现，表明，意味着" class="headerlink" title="5.展现，表明，意味着"></a>5.展现，表明，意味着</h2><p>illustrate, demonstrate, imply, show, reflect, reveal</p><h2 id="6-根据…"><a href="#6-根据…" class="headerlink" title="6.根据…"></a>6.根据…</h2><p>according to, based on(upon), on the basis of</p><h2 id="7-同期"><a href="#7-同期" class="headerlink" title="7.同期"></a>7.同期</h2><p>year-on-year</p><h2 id="8-预期，预计"><a href="#8-预期，预计" class="headerlink" title="8.预期，预计"></a>8.预期，预计</h2><p>expect, estimate, predict</p><h2 id="9-由…组成"><a href="#9-由…组成" class="headerlink" title="9.由…组成"></a>9.由…组成</h2><p>consist of, be made up of, be composed of</p><h2 id="10-占多大比例"><a href="#10-占多大比例" class="headerlink" title="10.占多大比例"></a>10.占多大比例</h2><p>account for, make/take up</p><h2 id="11-不同图表类型"><a href="#11-不同图表类型" class="headerlink" title="11.不同图表类型"></a>11.不同图表类型</h2><p>柱状图：column chart</p><p>折线图：line graph/line chart</p><p>饼图：pie chart</p><p>表格：table</p><h2 id="12-相比而言"><a href="#12-相比而言" class="headerlink" title="12.相比而言"></a>12.相比而言</h2><p>by contrast, in contrast to, in comparison to/with, by comparison with</p><h1 id="例句"><a href="#例句" class="headerlink" title="例句"></a>例句</h1><p>As is shown/demonstrated/implied/illustrated/reflected in the column chart/line graph/pie chart/table, ××(主题词) 【increase/decrease…我们重点分享的[上升、下降]的表达】+time</p><p>(1)The line graph clearly illustrates that the number of museums in China and their number of visitors both increased markedly between 2013 and 2015. 这幅线状图清晰地显示，从2013年到2015年，中国博物馆的数量和参观人数均急剧上升。</p><p>(2)According to the above bar chart, the percentages of graduates’ whereabouts in certain university underwent an enormous change from 2013 to 2018. 根据上面柱状图所示，某高校毕业生去向的百分比在2013年至2018年间发生了巨大的变化。</p><h2 id="引原因"><a href="#引原因" class="headerlink" title="引原因"></a>引原因</h2><p>1)Reasons abound for sth…, but one of the unusual aspects of the… is / one of which is…</p><p>造成这一现象的原因有很多，但是最重要的一方面是… / 一方面在于…</p><p>(2)A many of factors are contributing to sth, but the most important appears to be the…造成某事的原因有很多，但最重要的似乎是…</p><p>(3)Multiple factors may have attributed to sth…造成某事的因素可能有很多…</p><p>(4)There are likely a number contributing factors here. The first is…. Second…造成某事的因素可能有很多…首先是…，其次是…</p><p>(5)I can probably rattle off a number of the myriad reasons that lead to this problem我可能会不假思索的说出导致这个问题的很多原因</p><p>(6)There are myriad reasons for sth. The prime/chief cause of…is…造成某事的原因有很多，最主要 / 最首要的的一个原因是…</p><p>(7)Myriad reasons are put forward to explain sth很多原因能够用来解释…</p><p>(8)Several factors conspired to sth很多因素可以共同导致…</p><p>(9)A huge array of factors can conspire to sth很多因素可以共同导致…</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;波动&quot;&gt;&lt;a href=&quot;#波动&quot; class=&quot;headerlink&quot; title=&quot;波动&quot;&gt;&lt;/a&gt;波动&lt;/h1&gt;&lt;p&gt;xxxxxxxxxx # include&amp;lt;stdio.h&amp;gt;​int max(int A,int B, int C){​    r</summary>
      
    
    
    
    <category term="English" scheme="https://www.jkzhang.ml/categories/English/"/>
    
    
    <category term="English" scheme="https://www.jkzhang.ml/tags/English/"/>
    
    <category term="writing" scheme="https://www.jkzhang.ml/tags/writing/"/>
    
  </entry>
  
  <entry>
    <title>福尔摩斯精读:花斑带案</title>
    <link href="https://www.jkzhang.ml/2021/08/05/%E7%A6%8F%E5%B0%94%E6%91%A9%E6%96%AF%E7%B2%BE%E8%AF%BB-%E8%8A%B1%E6%96%91%E5%B8%A6%E6%A1%88/"/>
    <id>https://www.jkzhang.ml/2021/08/05/%E7%A6%8F%E5%B0%94%E6%91%A9%E6%96%AF%E7%B2%BE%E8%AF%BB-%E8%8A%B1%E6%96%91%E5%B8%A6%E6%A1%88/</id>
    <published>2021-08-05T09:59:19.000Z</published>
    <updated>2024-05-17T15:41:25.717Z</updated>
    
    <content type="html"><![CDATA[<p>故事为冒险史中的小说<a id="more"></a><br>海伦·史东纳小姐在家中晚上听到口哨声，而她的姐姐茱莉亚在家中死前亦曾表示听到口哨声，还声称自己看到带有花斑的带子，所以海伦小姐感到十分害怕，故向福尔摩斯求助。福尔摩斯连同华生到海伦小姐居住的地方调查，福尔摩斯发现海伦小姐所住的地方有很多奇怪的装修，例如不通风的通风口、没有连接到仆人的房铃等，他已经大约推理到事件的情况，于是晚上躲在海伦小姐的房间等疑凶出现。最后终于发现海伦小姐的继父詹姆斯·罗伊洛特博士为了谋取家产，而设计以毒蛇杀害海伦两姊妹。</p><h2 id="环境描写"><a href="#环境描写" class="headerlink" title="环境描写"></a>环境描写</h2><p>这部小说中为了渲染恐怖、惊险的氛围，大片段运用环境描写来衬托人物心理，渲染气氛，并推动情节进一步发展。<br>例如，在海伦叙述她姐姐暴死的夜晚时：  </p><blockquote><p>It was a wild night. <strong>The wind was howling outside, and the rain was beating and splashing against the windows.</strong> Suddenly, amid all the hubbub of the gale, there burst forth the wild scream of a terrified woman. </p></blockquote><ul><li>howl<br>因惊恐而喊叫，等同于roar。</li><li>splash<br>指雨水飞溅。<blockquote><p>splash against the windows  </p></blockquote></li></ul><p>雨水在窗户上飞溅。<br><img src="https://fastly.jsdelivr.net/gh/zhangrongxiang/store/master/photosphoto-1532294351631-a205083f392c.jpg"></p><h3 id="各种“喊”"><a href="#各种“喊”" class="headerlink" title="各种“喊”"></a>各种“喊”</h3><h4 id="roar"><a href="#roar" class="headerlink" title="roar"></a>roar</h4><p>怒吼。</p><blockquote><p>the wind was roaring.  </p></blockquote><p>狂风大作。</p><h4 id="scream"><a href="#scream" class="headerlink" title="scream"></a>scream</h4><p>惊喊</p><blockquote><p>Suddenly,  there burst forth the wild scream of a terrified woman.  </p></blockquote><p>突然间，爆出一个女人惊恐的叫喊。</p><h4 id="cry"><a href="#cry" class="headerlink" title="cry"></a>cry</h4><p>哭喊</p><blockquote><p>He looked inside the envelope. ‘So it is,’ he cried.   </p></blockquote><p>他看了看信封里边，“看来是了！”他哭喊着。</p><h4 id="yell"><a href="#yell" class="headerlink" title="yell"></a>yell</h4><p>狂喊</p><blockquote><p>“You see it, Watson?” he yelled. “You see it?”</p></blockquote><p>”看见了没，华生？“他狂喊。</p><h2 id="短语"><a href="#短语" class="headerlink" title="短语"></a>短语</h2><p>His smile broadened.他笑得更开心了。<br>the sweet promise of spring春日甜美的景色。<br><img src="https://fastly.jsdelivr.net/gh/zhangrongxiang/store/master/photos20210805195058.png"></p><h2 id="神态描写"><a href="#神态描写" class="headerlink" title="神态描写"></a>神态描写</h2><blockquote><p>My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, <strong>and his chin sunk upon his breast, buried in the deepest thought.</strong></p></blockquote><p>我的朋友坐在车前，叉着胳膊，帽子遮在眼前，而头埋在胸前，陷入沉思。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;故事为冒险史中的小说</summary>
    
    
    
    <category term="笔记" scheme="https://www.jkzhang.ml/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="English" scheme="https://www.jkzhang.ml/tags/English/"/>
    
    <category term="积累" scheme="https://www.jkzhang.ml/tags/%E7%A7%AF%E7%B4%AF/"/>
    
    <category term="Holmes" scheme="https://www.jkzhang.ml/tags/Holmes/"/>
    
  </entry>
  
  <entry>
    <title>苏联政治笑话浅谈</title>
    <link href="https://www.jkzhang.ml/2021/02/15/%E8%8B%8F%E8%81%94%E6%94%BF%E6%B2%BB%E7%AC%91%E8%AF%9D%E9%80%89%E8%B0%88/"/>
    <id>https://www.jkzhang.ml/2021/02/15/%E8%8B%8F%E8%81%94%E6%94%BF%E6%B2%BB%E7%AC%91%E8%AF%9D%E9%80%89%E8%B0%88/</id>
    <published>2021-02-15T03:23:44.000Z</published>
    <updated>2024-05-17T15:41:25.717Z</updated>
    
    <content type="html"><![CDATA[<p>苏联政治笑话（或称苏东政治笑话），即流行于前苏联和前东欧社会主义国家的政治笑话。<a id="more"></a>这种笑话的来源很多，除了来自于在这些国家生活的人之手外，也有一些来自美国、西德及其他国家的报社杂志，此类笑话以讽刺苏联-东欧诸国的领导人、政治、经济和生活状态为主，数量极大，讽刺辛辣，广为流传，前美国总统罗纳德·里根还在公众集会上讲过几个这样的笑话，据他所说他还“讲给了戈尔巴乔夫听，然后他笑了”。[1]近年美国政府解密档案揭露，当年美国驻莫斯科大使馆一直持续收集并整理流传的苏联政治笑话，再用密电传回华盛顿给中情局分析，因“这些笑话非常准确的反映苏联社会公众的情绪”。   </p><p>在苏联的这种政治笑话的起源时间不详，但在50年代斯大林执政末期就已经广泛出现了。这些笑话最初来自于沙俄时期的政治笑话。在苏联和东欧，政治笑话的泛滥是在60-80年代（尤其是在勃列日涅夫执政后期），和地下出版物一起成为了社会主义阵营中政治异见的一种象征。在“公开化”之前当局一直试图限制政治笑话的传播流动，但效果不佳。   </p><p>从体裁上，政治笑话一般是段子式或者问答式，问答式也会非常简单，适合一个人为别人讲。著名的“亚美尼亚广播电台”系列就是典型的问答式政治笑话。<br><a href="http://www.talkreason.org/marperak/jokes/armenrad.htm">亚美尼亚广播电台笑话链接</a></p><h1 id="1-嘲笑领导人的笑话"><a href="#1-嘲笑领导人的笑话" class="headerlink" title="1.嘲笑领导人的笑话"></a>1.嘲笑领导人的笑话</h1><p>苏联、东欧的重要领导人基本都进入了笑话：斯大林、赫鲁晓夫、勃列日涅夫、安德罗波夫、戈尔巴乔夫、昂纳克、齐奥塞斯库等人都是很经常出现的角色。列宁被讽刺的几率倒很低，因为他的历史形象一直比较好。不过，在1970年列宁诞辰100周年之际，由于官方铺天盖地的政治宣传，相应也出现了不少关于列宁和列宁纪念活动的笑话。   </p><h2 id="列宁"><a href="#列宁" class="headerlink" title="列宁"></a>列宁</h2><p>有人通过走关系，把列宁同志送到了天堂，接纳了这个无神论者。<br>过了几天，这人给上帝打电话表示感谢，电话接通了，他说：你是上帝吗？<br>上帝回答：首先，没有上帝。其次，我们这里每个人都是同志。第三，有事请直说，我还要参加党代表会议。   </p><h2 id="斯大林"><a href="#斯大林" class="headerlink" title="斯大林"></a>斯大林</h2><p>约瑟夫·斯大林在政治笑话中的形象一般是残暴、猜忌，他和农民生活困苦经常挂钩（考虑到1929年的农业集体化和30年代末的大清洗）。</p><p>斯大林去看一场苏联喜剧电影的首映礼。在影片播放时他一直快活的大笑，不过在电影结束之时他突然问道：<br>“好吧，我喜欢这电影。可为什么那个丑角的小胡子和我的一样？”<br>所有人都噤若寒蝉，只有一人怯怯地提议道：“斯大林同志，要不要让演员把胡子剃了？”<br>斯大林答道：“好主意，枪毙前先把胡子剃了。”<br><img src="https://fastly.jsdelivr.net/gh/zhangrongxiang/store/master/photos20210216120244.png"></p><p>集体农庄庄员伊万[注 1]在河里捉到一条大鱼，高兴的回到家里和老婆说：“看，我们有炸鱼吃了！”<br>“没有油啊。”<br>“那就煮！”<br>“没锅。”<br>“烤鱼！”<br>“没柴。”<br>伊万气死了，走到河边把鱼扔了回去。那鱼在水里划了一个半圆，上身出水，举起右鳍激动地高呼：“斯大林万岁！”  </p><p>一位美国历史学家和一位俄罗斯历史学家，讨论谁领导二十世纪的前半世纪。<br>“我投票赞成胡佛先生，”美国人说，“他尝试着教导我们美国人停止酗酒！”<br>“那个没什么了不起！”俄国人接着说，“我选择斯大林，他尝试教我们俄国人不要吃饭。”</p><h2 id="赫鲁晓夫"><a href="#赫鲁晓夫" class="headerlink" title="赫鲁晓夫"></a>赫鲁晓夫</h2><p>尼基塔·赫鲁晓夫在政治笑话中的形象是无知、粗鲁、愚蠢、秃顶、爱胡扯、爱吹牛皮的。这和他本人的以上作风分不开。也因为赫鲁晓夫当初是逼宫斯大林，出现不少揶揄赫鲁晓夫忌讳斯大林的笑话。</p><p>赫鲁晓夫前来参观前卫派的美术展览。<br>“这对该死的绿点和黄点是什么？”<br>“这幅画，赫鲁晓夫同志，是表现我们英勇的农民在努力完成生产两亿吨谷物的计划。”<br>“啊……哦……那这堆黑三角和红条条呢？”<br>“这幅画描绘了工厂中我们英雄般的产业工人。”<br>“那这个长耳朵的肥屁股呢？”<br>“赫鲁晓夫同志，这不是画，是镜子。”</p><p>共产党餐叙，赫鲁晓夫直接用手抓肉吃得非常狼狈，在一旁的斯大林看不过去对他说：“尼基塔，把刀子拿出来”，赫鲁晓夫迅速的回答说：“是的，领导同志！下一个目标是谁？”<br>有个人向赫鲁晓夫汇报说：“现在大剧院正上演一个剧，里面有您出现，每当您一出场，下面就热烈鼓掌。”赫鲁晓夫听了以后非常得意。有一天他买了一张票去看这个剧，他陷入了沉思，忘记了鼓掌，这时旁边有人推了他一把，紧张地说：“哎！你为什么不鼓掌？不要命啦？”<br>科学家联名要求赫鲁晓夫去领导一个癌症治疗所。赫鲁晓夫推托道：“谢谢同志们的支持，但我对癌症一窍不通啊。”<br>科学家们纷纷说道：“赫鲁晓夫同志，您只要应用一下您在农业问题上的经验就好了！您一负责，粮食就全不见了！”</p><p>斯大林死后， 因为被赫鲁晓夫视为禁忌， 因此希望把斯大林的遗体葬在海外，于是苏联向各国征求意愿：<br>英： 我们这边已经有丘吉尔，大战的英雄有一个就很够了。<br>德： 我们这边已经有希特勒，独裁者有一个就太多了。<br>这时， 以色列表明同意的意见。但是赫鲁晓夫马上脸色发青，<br>“靠！ 那里以前有人复活过啊！”</p><p>在苏共20大上，赫鲁晓夫在做秘密报告前突然消失了。但是过了几分钟后他又回来了，并登上了主席台。<br>人们问他：“赫鲁晓夫同志，您刚才去哪了？”<br>赫鲁晓夫说：“我去列宁墓里看了看，摸摸斯大林还有没有脉搏，以防万一。”</p><p>赫鲁晓夫和肯尼迪交谈，各自吹嘘，肯尼迪说：“美国医学发达，有种药死人吃了可以复活。”<br>赫鲁晓夫说：“苏联体育发达，有人十分钟可以从莫斯科跑到美国。”肯尼迪要求兑现，赫鲁晓夫慌了手脚，召集文武大臣商量对策，<br>有人出了个好计策说：“这很好办，你先让肯尼迪把药拿来，让斯大林吃了，那斯大林一定复活，那么你用不了五分钟就可以从苏联跑到美国。”</p><h2 id="勃涅日列夫"><a href="#勃涅日列夫" class="headerlink" title="勃涅日列夫"></a>勃涅日列夫</h2><p>列昂尼德·勃列日涅夫在政治笑话中的形象一般是爱慕虚荣、权力欲强、喜欢享乐、痴呆、健忘、病入膏肓的。苏联当局努力培养对他的个人崇拜、他本人的“勋章收藏爱好”和他请人捉刀而获得了列宁文学奖的回忆录三部曲也经常是被嘲笑的对象。关于他的笑话是所有苏联领导人中最多的。</p><p>在政治局会议上，勃列日涅夫兴致勃勃地说：“同志们都说我那三本回忆录写得好，哪天给我弄一套，我也读一读。”<br>在勃列日涅夫访问英国时，首相撒切尔夫人问：“您怎么看丘吉尔？”<br>“丘吉尔是谁？”勃列日涅夫反问。<br>回到苏联驻英大使馆，大使评价道：“祝贺您，勃列日涅夫同志，你让撒切尔夫人老实了。她以后再也不敢问什么愚蠢的问题了。”<br>“撒切尔是谁？”勃列日涅夫反问。</p><p><img src="https://fastly.jsdelivr.net/gh/zhangrongxiang/store/master/photos20210216120026.png"><br>一只鳄鱼吃掉勃列日涅夫，结果它拉了一个星期的勋章。</p><p>“你听说了吗？勃列日涅夫做手术了。”<br>“做什么手术？”<br>“开胸手术。因为勋章摆不下了。”</p><p>英国广播公司报道说： “昨天莫斯科发生了地震。”但是学者们对此表示怀疑，因为莫斯科处于非地震带。<br>不过，我们的驻莫斯科记者报告说该市确有震感。最后经过核实，原来并非是地震，而是苏共中央总书记勃列日涅夫佩戴勋章的衣服掉到了地上。</p><p>一个苏联人在公众场合对着勃列日涅夫同志的肖像骂了句“白痴”，被克格勃逮捕，判了5年徒刑。他的罪名是：侮辱党和国家领导人判刑1年，泄露党和国家机密判刑4年。<br>不过听说他很快就获释了，因为自从勃列日涅夫同志在联合国发表演说之后，那就不再是党和国家机密了。</p><p>话说勃列日涅夫同志当上苏共中央总书记之后，将在乡下的老母亲接到了莫斯科。老太太来了以后，勃列日涅夫得意洋洋地向老妈展示了一番自己的豪华别墅、高级汽车、名贵家具等等，展示完了后，勃列日涅夫问老太太这一切如何？老太太说：“儿子啊，这一切都很好，但是，共产党来了你怎么办？”</p><p>在美国人登陆月球成功后，勃列日涅夫当天就打电话给苏联太空人：“鉴于美国人已经在月球上登陆，现在苏联决定，马上派你们去太阳登陆。”<br>太空人大惊，哽咽道：“您不知道吗，勃列日涅夫同志，我们会被烧死的。”<br>勃列日涅夫生气地说：“你以为政治局没有考虑过吗？我们已经决定，派你们在晚上在太阳登陆！”</p><p>勃列日涅夫即将访问波兰，波兰当局命令一位著名画家创作一幅名为《勃列日涅夫在波兰》的大型油画作为献礼。很不情愿的画家在威逼下接受了工作。画完成后，波兰一高官前来验收，结果让他大吃一惊：画面上是一男一女在豪华的大床上极尽缠绵，窗外的风景是克里姆林宫。<br>“这是什么？这女的是谁？！”高官愤怒的问。<br>“勃列日涅夫的夫人。”画家答道。<br>“男的呢？！”“勃列日涅夫的秘书。”<br>“可勃列日涅夫同志在哪里？”<br>“勃列日涅夫在波兰。”画家答道。</p><h2 id="安德罗波夫"><a href="#安德罗波夫" class="headerlink" title="安德罗波夫"></a>安德罗波夫</h2><p>短暂担任过苏联最高领导人的尤里·安德罗波夫除了克格勃主席的身份经常遭到讽刺之外，他糟糕的健康状况也是老人政治的缩影，故也经常在笑话中出场。</p><p>问：为什么勃列日涅夫能出国，安德罗波夫却不能？<br>答：因为前者安的是电池，后者接的是电线。（勃列日涅夫用心脏起搏器，安德罗波夫用透析机）</p><h2 id="昂纳克"><a href="#昂纳克" class="headerlink" title="昂纳克"></a>昂纳克</h2><p>关于埃里希·昂纳克的政治笑话相当丰富，因为德意志民主共和国国内的知识分子政治异见势力较强，而昂纳克又执政太久。这些笑话大多是表达对昂内克的各种不满和不支持。</p><p>埃里希·昂纳克想知道人民是如何看待他的，所以他化妆微服私访。他在大街上问一个人：“打搅一下，请问您觉得昂纳克怎么样？”<br>这个人把他引到一条暗巷，确认四下无人，没有其他人能听见他说话之后，他贴着埃里希的耳朵小声说：“我支持昂纳克！”  </p><p>一个老妇人在东德询问站岗的警察：“请问那家名字叫‘原则’的商店在哪里呢？”<br>警察想了一会说：“这里没有叫这个名字的商店啊。”<br>老夫人说：“肯定应该有的啊，不然我们的昂纳克同志怎么会说‘一切东西在原则上都是可以买到的’呢？”  </p><p>昂纳克想知道自己的民意支持度如何，于是走近一家高端住宅并敲开了门。<br>一个小女孩打开门：“叔叔，你是谁呀？”<br>“小朋友，我是那个让你们能够生活得如此幸福的人，是我为你们带来了食物和住所……”<br>“妈妈，妈妈，快点过来，慕尼黑的彼得叔叔来看我们啦！”（慕尼黑在西德）  </p><h1 id="2-亚美尼亚广播电台系列"><a href="#2-亚美尼亚广播电台系列" class="headerlink" title="2.亚美尼亚广播电台系列"></a>2.亚美尼亚广播电台系列</h1><p>亚美尼亚广播电台（或称埃里温广播电台）是苏联境内（远不限于亚美尼亚）甚至整个东欧地区都广为流传的笑话，笑话的格式相对固定，为“亚美尼亚广播电台收到提问……，亚美尼亚广播电台对此回答……”，通常简写为“问答”（Q/A）式笑话。这种笑话的题材其实不仅仅限于政治笑话（还有不少生活笑话），但政治笑话占绝大多数。这样的笑话经常会有很多小系列。在德国电影《窃听风暴》里，斯塔西的头头古毕兹中校就讲了一个关于昂纳克的这种笑话。</p><p>问：鸡和蛋哪个先有？<br>答：从前两个都有。<br>问：什么是最短的笑话？<br>答：共产主义。<br>问：什么是最长的笑话？<br>答：赫鲁晓夫在党代表大会上的讲话。<br>问：猪会秃头吗？<br>答：我们不回答政治问题。<br>问：改革的前景是什么？<br>答：有两种可能的情况。现实的可能是火星人会降临地球帮我们打理一切，科幻的可能是我们能自己完成改革目标。<br>问：什么是混乱？<br>答：我们对国民经济不做评论。<br>问：您能描述一下东德的地理特征吗？<br>答：一个充满各种瓶颈的坦途。<br>问：民主与社会主义民主的区别是什么？<br>答：大概相当于椅子和电椅的区别。<br>问：我要去西方国家开会，可是旅费不够。怎么办？<br>答：在原则上这不是问题。别买返程票就行。<br><a href="http://www.talkreason.org/marperak/jokes/armenrad.htm">更多亚美尼亚广播电台笑话链接</a></p><h1 id="3-犹太人笑话"><a href="#3-犹太人笑话" class="headerlink" title="3.犹太人笑话"></a>3.犹太人笑话</h1><p>由于历史原因，在苏联曾经对犹太人有着严重的偏见和歧视，尤其是在50年代斯大林执政后期和70年代移民以色列风潮期间。犹太人对歧视不满于是编出了不少笑话。犹太人的奸猾形象也经常出现在笑话之中。前一类笑话中的犹太人一般叫拉宾诺维奇（Rabinovich，因此这一系列经常称为拉宾诺维奇笑话），后一种笑话中的犹太人则经常叫埃布拉姆。这一类笑话通常和敖德萨（沙俄时代起的犹太人聚居区）、移民以色列（在一段时间中苏联人能够自由移民的国家只有以色列，在勃列日涅夫时代有1/4的苏联犹太人移民以色列）有关。</p><p>俄罗斯移民在好莱坞开了家俄式餐馆叫“俄罗斯怀旧”。菜单：<br>第一道菜——罗宋汤；<br>第二道菜——油炸包；<br>甜点——猛敲一下顾客的脖子并怒吼：“滚出去犹太猪！”</p><p>在五一劳动节的游行上，拉宾诺维奇举着这样一个牌子走过会场：感谢你，斯大林同志，是你给了我幸福的童年。<br>党代表找到他：“你在侮辱我们的常识吗？谁都知道当你在童年的时候我们的斯大林同志还没出生呢！”<br>拉宾诺维奇答道：“这就是我感谢他的原因。”</p><p>苏联反犹太主义者：“外国报纸全都是犹太人办的！”<br>苏联犹太人：“没错！所以外国报纸寄到这里时都给施了割礼。”</p><p>说：曾经有法老和犹太人，法老灭绝了，犹太人活下来了；曾经有宗教裁判官和犹太人，前者都死绝了，后者活了下来；曾经有纳粹和犹太人，前者灭绝了，后者存活了下来；现在有共产主义者和犹太人。<br>问：你到底想说什么？<br>答：没什么，只是说犹太人可能活到最后。 </p><h1 id="4-苏美关系的笑话"><a href="#4-苏美关系的笑话" class="headerlink" title="4.苏美关系的笑话"></a>4.苏美关系的笑话</h1><p>苏美关系是苏联笑话中特别常见的话题，讽刺的内容不定，不过经常是针对苏联领导人或苏联体制的。</p><p>一个美国人和一个苏联人互相吹牛，夸耀自己的国家。美国人说道：“我的国家实在自由。你可以径直走进白宫，对总统说：‘总统先生，我不同意你的现行对内政策！’”苏联人便答道：“嘿，这有什么！我的国家也很自由！你可以径直走进克里姆林宫，对总书记说：‘总书记同志，我不同意美国总统的现行对内政策！’”</p><h1 id="5-关于政治"><a href="#5-关于政治" class="headerlink" title="5.关于政治"></a>5.关于政治</h1><p>作为政治笑话，讽刺苏联、东欧政治生活的内容是相当多的，涉及到党组织生活、等额选举、路线摇摆、老人政治、言论控制等等等等。政治笑话经常在重大政治事件之后飞速传播，克格勃的调查报告称，一个政治笑话可以在一天以内传遍一个莫斯科大小的城市。</p><p>有人给克里姆林宫打电话：<br>“你们现在在找苏共的新总书记吗？”<br>“不！你是谁？傻瓜吗？”<br>“对，病入膏肓的老傻瓜！”</p><p>问：美国和苏联宪法有什么区别？不是都保证言论自由吗？<br>答：当然，不过美国宪法也保证言论后的自由。</p><p>美国人：“我们有言论自由，就是在白宫前面说美国总统是傻子也没事。”<br>苏联人：“那又怎么样？我们在克林姆林宫前面说美国总统是傻子也没事。”</p><p>在调查表上有这样一个问题：在执行总路线时你动摇过吗？<br>拉宾诺维奇回答道：“我和总路线一起动摇。”</p><p>监狱里两个囚犯正交流经验。<br>“你是因为政治犯罪被捕的么？”<br>“当然。我是个管子工，被党委员会叫去修下水管。我看了看，说，‘整个体系都该换换了’，于是我就被判了7年。”</p><p>一个英国人、一个法国人、一个苏联人谈论什么是世界上最幸福的事。<br>英国人：“最幸福的事情就是冬天晚上回家，穿着羊毛裤坐在壁炉前面。”<br>法国人：“你们英国人就是古板，最幸福的事情是和一个金发女郎一起去地中海度假，然后我们好聚好散。”<br>苏联人：“最幸福的事情就是半夜有警察敲门，开门后：‘伊万，你被捕了。’”<br>…：“你弄错了，伊万在隔壁。”</p><h1 id="6-关于内务部-克格勃-斯塔西等秘密警察机构"><a href="#6-关于内务部-克格勃-斯塔西等秘密警察机构" class="headerlink" title="6.关于内务部-克格勃-斯塔西等秘密警察机构"></a>6.关于内务部-克格勃-斯塔西等秘密警察机构</h1><p>讽刺秘密警察（内务人民委员部、克格勃、斯塔西）的段子多以调侃这些组织的严刑逼供和无孔不入为主。也有一些笑话是在嘲笑他们的愚蠢。</p><p>来了一个格鲁吉亚代表团，得到斯大林接见，谈话，然后离开，斯大林开始找他的烟斗，找不到。<br>他叫贝利亚来：“拉甫连季·巴甫洛维奇，去追代表团，找找谁拿了我的烟斗。”<br>贝利亚赶忙去追代表团。<br>五分钟后，斯大林在一堆纸下（一说为椅子下）找到了他的烟斗。<br>他叫贝利亚——“瞧，拉甫连季·巴甫洛维奇，我找到我的烟斗了。”<br>“太晚了。”贝利亚说，“代表团中的半数已经承认他们拿了你的烟斗，并且加入了‘利用偷烟斗进行暗杀活动的托洛茨基阴谋组织’，而另外一半则在审讯中死掉了。”   </p><p>地狱里有条规矩：所有生前杀过人的都要被受害者的血淹没。一个人下地狱参观，发现贝利亚的血只淹没到了他的膝盖，不解问他为什么。<br>贝利亚说：“因为我站在斯大林同志的肩膀上。”  </p><p>一个东德的居民被掐掉了电话线，他跑去申诉并询问原因，有关部门告诉他：<br>“因为您诬蔑了国家安全部。”<br>“我怎么诬蔑的？”<br>“我们有记录：您曾多次在电话中声称，安全部窃听了您的通话。”</p><p>一位内务人民委员部审判员结束一天的审判工作，回到办公室，突然独自大笑起来。<br>对面办公桌的同事奇怪的问道：“有什么好笑的事吗？”<br>“是啊，”审判员用手帕擦著笑出来的眼泪：“一个很好笑的笑话……”<br>“哦？说来听听？”<br>“你疯了吗？我刚判了说这笑话的家伙五年苦役！”</p><p>“假设你在酒吧里，而一个陌生人坐到你的身边并开始唉声叹气，你该怎么做？”<br>“立即去阻止这种反苏宣传。”  </p><p>一位莫斯科市民在公车上：“您好同志，请问您是克格勃吗？”<br>“不是。”<br>“那您的家人或直系亲属有在克格勃工作吗？”<br>“没有。”<br>“那您有朋友在克格勃吗？”<br>“没有。”<br>“那您把脚拿开好吗，您踩到我的脚了。”   </p><p>一位年轻人在工作时抱怨道：“这种政府真差劲。”<br>结果被一位秘密警察听到而遭逮捕。<br>年轻人辩解说：“我根本没讲是哪个政府，你怎么可以随便逮捕我呢？”<br>“你少骗人！”秘密警察咆哮道，“我在这里工作二十多年了，哪一个政府差劲我不会知道吗？”</p><h1 id="7-关于政治笑话"><a href="#7-关于政治笑话" class="headerlink" title="7.关于政治笑话"></a>7.关于政治笑话</h1><p>在公开化之前的年代讲政治笑话并不是没有风险的。在斯大林时代，如果讲政治笑话遭到检举就会因违反苏联刑法第58号条款第10款（恶意散布煽动推翻、分裂或削弱苏维埃联盟的言论）处一般3-5年的徒刑，根据历史学家、持不同政见者罗伊·麦德维杰夫的说法，在1953年斯大林死后，当局决定释放因讲政治笑话而入狱的人，而因此获释的人数达20万[6]。赫鲁晓夫时代之后这一惩罚措施被取消，但当局针对不同政见的压制仍然明显存在。克格勃和斯塔西都拥有监控居民言论和行为的部门，政治笑话也是考虑点之一。因此就有了关于政治笑话的政治笑话。</p><p>Q：白海-波罗的海运河是什么人修建的？<br>A：左边是讲政治笑话的人修的，右边是听政治笑话的人修的。</p><p>有人喜欢说笑话；有人喜欢收集笑话去说笑话；有人喜欢收集说笑话的人。</p><p>“勃列日涅夫同志，听说您收集政治笑话，是真的么？”<br>“是的。”<br>“那么您现在收集了多少了呢？”<br>“三座半劳改营。”</p><p>向最佳政治笑话大赛获奖者颁奖：一等奖，二十五年；二等奖，二十年；还有两个三等奖，每个十五年。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;苏联政治笑话（或称苏东政治笑话），即流行于前苏联和前东欧社会主义国家的政治笑话。</summary>
    
    
    
    <category term="笑话" scheme="https://www.jkzhang.ml/categories/%E7%AC%91%E8%AF%9D/"/>
    
    
    <category term="笑话" scheme="https://www.jkzhang.ml/tags/%E7%AC%91%E8%AF%9D/"/>
    
    <category term="苏联" scheme="https://www.jkzhang.ml/tags/%E8%8B%8F%E8%81%94/"/>
    
    <category term="玉米大王" scheme="https://www.jkzhang.ml/tags/%E7%8E%89%E7%B1%B3%E5%A4%A7%E7%8E%8B/"/>
    
  </entry>
  
  <entry>
    <title>一键批量删除QQ动态</title>
    <link href="https://www.jkzhang.ml/2021/02/14/%E4%B8%80%E9%94%AE%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4QQ%E5%8A%A8%E6%80%81/"/>
    <id>https://www.jkzhang.ml/2021/02/14/%E4%B8%80%E9%94%AE%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4QQ%E5%8A%A8%E6%80%81/</id>
    <published>2021-02-14T07:51:59.000Z</published>
    <updated>2024-05-17T15:41:25.742Z</updated>
    
    <content type="html"><![CDATA[<p>当你看到你以前的QQ动态（说说）时，有何感觉？</p><a id="more"></a><p><img src="https://fastly.jsdelivr.net/gh/zhangrongxiang/store/master/photosphotos%E8%B6%A3%E5%9B%BE%EF%BC%9A%E5%81%B6%E7%84%B6%E9%97%B4%E7%9C%8B%E5%88%B0%E8%87%AA%E5%B7%B1%E5%A4%9A%E5%B9%B4%E5%89%8D%E5%86%99%E7%9A%84%E4%BB%A3%E7%A0%81_java%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AF%95-CSDN%E5%8D%9A%E5%AE%A2_2.gif"><br>简直就是自己的一场黑历史。那么我们就来看一个能一键删除QQ动态的脚本。</p><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="进入DevTools"><a href="#进入DevTools" class="headerlink" title="进入DevTools"></a>进入DevTools</h2><p>浏览器打开QQ空间<a href="https://user.qzone.qq.com/">链接</a>，进入后按<code>F12</code>,浏览器弹出<code>DevTools</code>，或者右键选择“检查”。 </p><h2 id="调出控制台"><a href="#调出控制台" class="headerlink" title="调出控制台"></a>调出控制台</h2><p>选择<code>console</code><br><img src="https://fastly.jsdelivr.net/gh/zhangrongxiang/store/master/photosconsole.webp"> </p><h2 id="复制代码"><a href="#复制代码" class="headerlink" title="复制代码"></a>复制代码</h2><p>复制下面代码到控制台，回车即可。</p><pre><code>var temp = true;function clickDel() &#123;    try &#123;        document.querySelector(&#39;.app_canvas_frame&#39;).contentDocument.querySelector(&#39;.del_btn&#39;).click();    &#125; catch&#123;        var a = document.querySelector(&#39;.app_canvas_frame&#39;).contentDocument.querySelector(&#39;.mod_pagenav_main&#39;).querySelectorAll(&#39;.c_tx&#39;);        a[a.length - 1].click();        temp = false;        setTimeout(clickDel, 2000);    &#125;    setTimeout(clickYes, temp ? 2000 : 5000);    temp = true;&#125;function clickYes() &#123;    document.querySelector(&#39;.qz_dialog_layer_btn&#39;).click();    setTimeout(clickDel, 2000);&#125;if (confirm(&quot;是不是要删除说说&quot;)) &#123;    setTimeout(function () &#123;        document.querySelectorAll(&quot;a[tabindex]&quot;)[8].click();        setTimeout(clickDel, 2000);    &#125;, 3000);&#125;</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;当你看到你以前的QQ动态（说说）时，有何感觉？&lt;/p&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.jkzhang.ml/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
    <category term="技术" scheme="https://www.jkzhang.ml/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="javascript" scheme="https://www.jkzhang.ml/tags/javascript/"/>
    
    <category term="说说" scheme="https://www.jkzhang.ml/tags/%E8%AF%B4%E8%AF%B4/"/>
    
    <category term="动态" scheme="https://www.jkzhang.ml/tags/%E5%8A%A8%E6%80%81/"/>
    
  </entry>
  
</feed>
